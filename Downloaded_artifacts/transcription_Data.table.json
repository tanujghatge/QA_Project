{"_type": "table", "column_types": {"params": {"type_map": {"Playlist_Title": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "Transcription": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "Video_Date": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "Video_Length": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "number"}]}, "wb_type": "union"}, "Video_Link": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}, "Video_Title": {"params": {"allowed_types": [{"wb_type": "none"}, {"wb_type": "string"}]}, "wb_type": "union"}}}, "wb_type": "typedDict"}, "columns": ["Playlist_Title", "Video_Title", "Video_Link", "Transcription", "Video_Length", "Video_Date"], "data": [["MIT 6.S191: Introduction to Deep Learning", "MIT Introduction to Deep Learning | 6.S191", "https://www.youtube.com/watch?v=QDX-1M5Nj7s", " Good afternoon, everyone. Thank you all for joining today. My name is Alexander Amini, and I'll be one of your course organizers this year, along with Ava. And together, we're super excited to introduce you all to Introduction to Deep Learning. Now, MIT Intro to Deep Learning is a really, really fun, exciting, and fast-paced program here at MIT. And let me start by just, first of all, giving you a little bit of background into what we do and what you're going to learn about this year. So this week of Intro to Deep Learning, we're going to cover a ton of material in just one week. You'll learn the foundations of this really, really fascinating and exciting field of deep learning and artificial intelligence. And more importantly, you're going to get hands-on experience actually reinforcing what you learn in the lectures as part of hands-on software labs. Now, over the past decade, AI and deep learning have really had a huge resurgence and had incredible successes and a lot of problems that even just a decade ago we thought were not really even solvable in the near future. Now we're solving with deep learning with incredible ease. Now this past year in particular of 2022 has been an incredible year for deep learning progress and I'd like to say that actually this past year in particular has been the year of generative deep learning, using deep learning to generate brand new types of data that have never been seen before and never existed in reality. In fact, I want to start this class by actually showing you how we started this class several years ago, which was by playing this video that I'll play in a second. Now this video actually was an introductory video for the class. It was and kind of exemplifies this idea that I'm talking about. So let me just stop there and play this video first of all. Hi everybody, and welcome to MIT Fit Pass 191, the official introductory course on deep learning taught here at MIT. Deep learning is revolutionizing so many fields, from robotics to medicine and everything in between. You'll learn the fundamentals of this field and how you can build some of these incredible algorithms. In fact, this entire speech and video are not real, and were created using deep learning and artificial intelligence. And in this class, you'll learn how. It has been an honor to speak with you today, and I hope you enjoy the course. Thank you. Thank you. Thank you. Thank you. So, in case you couldn't tell, this video and its entire audio was actually not real. It was synthetically generated by a deep learning algorithm. And when we introduced this class a few years ago, this video was created several years ago. But even several years ago, when we introduced this and put it on YouTube, it went somewhat viral. People really loved this video. They were intrigued by how real the video and audio felt and looked, entirely generated by an algorithm, by a computer. And people were shocked with the power and the realism of these types of approaches. And this was a few years ago. Now fast forward to today, and the state of deep learning today, we have seen deep learning accelerating at a rate faster than we've ever seen before. In fact, we can use deep learning now to generate not just images of faces, but generate full synthetic environments where we can train autonomous vehicles entirely in simulation and deploy them on full-scale vehicles in the real world seamlessly. The videos here you see are actually from a data-driven simulator from neural networks generated called Vista that we actually built here at MIT and have open-sourced to the public so all of you can actually train and build the future of autonomy and self-driving cars. And of course it goes far beyond this as well. Deep learning can be used to generate content directly from how we speak and the language that we convey to it from prompts that we say. Deep learning can reason about the prompts in natural language, in English, for example, and then guide and control what is generated according to what we specify. We've seen examples of where we can generate, for example, things that, again, have never existed in reality. We can ask a neural network to generate a photo of a astronaut riding a horse and it actually can imagine, hallucinate what this might look like even though of course this photo, not only this photo has never occurred before, but I don't think any photo of an astronaut riding a horse has ever occurred before. So there's not really even training data that you could go off in this case. And my personal favorite is actually how we can not only build software that can generate images and videos, but build software that can generate software as well. We can also have algorithms that can take language prompts, for example, a prompt like this, write code in TensorFlow to train a neural network. And not only will it write the code and create that neural network, but it'll have the ability to reason about the code that it's generating and walk you through, step by step, explaining the process and procedure all the way from the ground up to you so that you can actually learn how to do this process as well. Now, I think some of these examples really just highlight how far deep learning and these methods have come in the past six years since we started this course and you saw that example just a few years ago from that introductory video, but now we're seeing such incredible advances and the most amazing part of this course in my opinion is actually that within this one week we're going to take you through from the ground up, starting from today, all of the foundational building blocks that will allow you to understand and make all of this amazing advances possible. So with that, hopefully now you're all super excited about what this class will teach. And I want to basically now just start by taking a step back and introducing some of these terminologies that I've kind of been throwing around so far, with deep learning, artificial intelligence, what do these things actually mean. So first of all I want to maybe just take a second to speak a little bit about intelligence and what intelligence means at its core. So to me, intelligence is simply the ability to process information such that we can use it to inform some future decision or action that we take. Now the field of artificial intelligence is simply the ability for us to build algorithms, artificial algorithms, that can do exactly this. Process information to inform some future decision. Now machine learning is simply a subset of AI which focuses specifically on how we can build a machine to, or teach a machine, how to do this from some experiences or data, for example. Now, deep learning goes one step beyond this and is a subset of machine learning which focuses explicitly on what are called neural networks and how we can build neural networks that can extract features in the data. These are basically what you can think of as patterns that occur within the data so that it can learn to complete these tasks as well. Now that's exactly what this class is really all about at its core. We're going to try and teach you and give you the foundational understanding and how we can build and teach computers to learn tasks, many different type of tasks, directly from raw data. And that's really what this class boils down to at its most simple form. And we'll provide a very solid foundation for you, both on the technical side, through the lectures, which will happen in two parts throughout the class, the first lecture and the second lecture, each one about one hour long, followed by a software lab, which will immediately follow the lectures, which will try to reinforce a lot of what we cover in the technical part of the class and, you know, give you hands-on experience implementing those ideas. So this program is split between these two pieces, the technical lectures and the software labs. We have several new updates this year in specific, especially in many of the later lectures. The first lecture will cover the foundations of deep learning, which is going to be right now. And finally, we'll conclude the course with some very exciting guest lectures from both academia and industry who are really leading and driving forward the state of AI and deep learning. And of course, we have many awesome prizes that go with all of the software labs and the project competition at the end of the course. So maybe quickly to go through these. Each day, like I said, we'll have dedicated software labs that couple with the lectures. Starting today with lab one, you'll actually build a neural network keeping with the steam of generative AI. you'll build a neural network that can learn, listen to a lot of music, and actually learn how to generate brand new songs in that genre of music. At the end, at the next level of the class on Friday, we'll host a project pitch competition where either you individually or as part of a group can participate and present an idea, a novel deep learning idea to all of us. It'll be roughly three minutes in length and we will focus not as much because this is a one-week program. We are not going to focus so much on the results of your pitch but rather the innovation and the idea and the novelty of what you're trying to propose. The prizes here are quite significant already. The first prize is going to get an NVIDIA GPU which is really a key piece of hardware that is instrumental if you want to actually build a deep learning project and train these neural networks which can be very large and require a lot of compute. These prizes will give you the compute to do so. And finally this year we'll be awarding a grand prize for labs two and three combined which will occur on Tuesday and Wednesday, focused on what I believe is actually solving some of the most exciting problems in this field of deep learning, and how specifically how we can build models that can be robust, not only accurate, but robust and trustworthy and safe when they're deployed as well. You'll actually get experience developing those types of solutions that can actually advance the state of the art in AI. Now all of these labs that I mentioned and competitions here are going to be due on Thursday night at 11 p.m., right before the last day of class, and we'll be helping you all along the way. This prize, or this competition in particular, has very significant prizes, so I encourage all of you to really enter this prize and try to get a chance to win the prize. And of course, like I said, we're going to be helping you all along the way. There are many available resources throughout this class to help you achieve this. Please post to Piazza if you have any questions. And of course, this program has an incredible team that you can reach out to at any point in case you have any issues or questions on the materials. Myself and Ava will be your two main lecturers for the first part of the class. We'll also be hearing, like I said, in the later part of the class from some guest lecturers who will share some really cutting edge state-of-the-art developments in deep learning. And of course, I want to give a huge shout out and thanks to all of our sponsors who without their support this program wouldn't have been possible for yet again another year. So thank you all. Okay so now with that let's really dive into the really fun stuff of today's lecture which is you know the technical part and I think I want to start this part by asking all of you and having you ask yourselves this question of, why are all of you here, first of all? Why do you care about this topic in the first place? Now, I think to answer this question, we have to take a step back and think about the history of machine learning and what machine learning is and what deep learning brings to the table on top of machine learning. Now traditional machine learning algorithms typically define what are called these set of features in the data. You can think of these as certain patterns in the data and then usually these features are hand engineered. So probably a human will come into the data set and with a lot of domain knowledge and experience can try to uncover what these features might be. Now the key idea of deep learning, and this is really central to this class, is that instead of having a human define these features, what if we could have a machine look at all of this data and actually try to extract and uncover what are the core patterns in the data so that it can use those when it sees new data to make some decisions. So for example, if we wanted to detect faces in an image, a deep neural network algorithm might actually learn that in order to detect a face, it first has to detect things like edges in the image, lines and edges. And when you combine those lines and edges, you can actually create compositions of features like corners and curves, which when you create those, when you combine those, you can create more high-level features, for example, eyes and noses and ears. And then those are the features that allow you to ultimately detect what you care about detecting, which is the face. But all of these come from what are called kind of a hierarchical learning of features. And you can actually see some examples of these. These are real features learned by a neural network, and how they're combined defines this progression of information. But in fact, what I just described, this underlying and fundamental building block of neural networks and deep learning, have actually existed for decades. Now, why are we studying all of this now and today in this class with all this great enthusiasm to learn this, right? Well, for one, there have been several key advances that have occurred in the past decade. Number one is that data is so much more pervasive than it has ever been before in our lifetimes. These models are hungry for more data and we're living in the age of big data. More data is available to these models than ever before, and they thrive off of that. Secondly, these algorithms are massively parallelizable. They require a lot of compute, and we're also at a unique time in history where we have the ability to train these extremely large-scale algorithms and techniques that have existed for a very long time, but we can now train them due to the hardware advances that have been made. And finally, due to open source toolboxes and software platforms like TensorFlow, for example, which all of you will get a lot of experience on in this class, training and building the code for these neural networks has never been easier. So that from the software point of view as well, there have been incredible advances to open source, you know, the underlying fundamentals of what you're going to learn. So let me start now with just building up from the ground up the fundamental building block of every single neural network that you're going to learn in this class, and that's going to be just a single neuron, right, and in neural network language a single neuron is called a perceptron. So what is a perceptron? A perceptron is, like I said, a single neuron. And it's actually, I'm going to say it's a very, very simple idea. So I want to make sure that everyone in the audience understands exactly what a perceptron is and how it works. So let's start by first defining a perceptron as taking as input a set of inputs, right. So on the left-hand side you can see this perceptron takes m different inputs, 1 to m, right. These are the blue circles. We're denoting these inputs as X's. Each of these numbers, each of these inputs, is then multiplied by a corresponding weight, which we can call W, right? So, X1 will be multiplied by W1, and we'll add the result of all of these multiplications together. Now, we take that single number after the addition, and we pass it through this nonlinear, what we call a nonlinear activation function, and that produces our final output of the perceptron, which we can call Y. Now, this is actually not entirely accurate of the picture of a perceptron. There's one step that I forgot to mention here. So, in addition to multiplying all of these inputs with their corresponding weights, we're also now going to add what's called a bias term, here denoted as this w0, which is just a scalar weight, and you can think of it coming with a input of just one. So that's going to allow the network to basically shift its non-linear activation function, you know, non-linearly, right, as it sees its inputs. Now, on the right-hand side, you can see this diagram mathematically formulated, right. As a single equation, we can now rewrite this linear, this equation with linear algebra, terms of vectors and dot products, right? So, for example, we can define our entire inputs, x1 to xm, as a large vector x, right? That large vector x can be multiplied by, or take a dot, excuse me, matrix multiplied with our weights, w, that's again another vector of our weights, w1 to wm, taking their dot product, not only multiplies them, but it also adds the resulting terms together, adding a bias, like we said before, and applying this non-linearity. Now, you might be wondering what is this nonlinear function? I've mentioned it a few times already. Now, you might be wondering what is this nonlinear function. I've mentioned it a few times already. Well, I said it is a function, right, that's passed, that we pass the outputs of the neural network through before we return it, you know, to the next neuron in the in the pipeline, right. So, one common example of a nonlinear function that's very popular in deep neural networks is called the sigmoid function. You can think of this as kind of a continuous version of a threshold function, right? It goes from 0 to 1, and it's having, it can take as input any real number on the real number line, and you can see an example of it illustrated on the bottom right hand. Now, in fact, there are many types of nonlinear activation functions that are popular in deep neural networks, and here are some common ones, and throughout this presentation, you'll actually see some examples of these code snippets on the bottom of the slides, where we'll try and actually tie in some of what you're learning in the lectures to actual software and how you can implement these pieces, which will help you a lot for your software labs explicitly. So the sigmoid activation on the left is very popular, since it's a function that outputs, you know, between 0 and 1. So, especially when you want to deal with probability distributions, for example, this is very important because probabilities live between 0 and 1. In modern deep neural networks, though, the ReLU function, which you can see on the far right hand, is a very popular activation function because it's piecewise linear. It's extremely efficient to compute, especially when computing its derivatives, right? Its derivatives are constants, except for one nonlinearity at zero. Now, I hope actually all of you are probably asking this question to yourself of, why do we even need this nonlinear activation function? It seems like it kind of just complicates this whole picture when we didn't really need it in the first place, and I want to just spend a moment on answering this, because the point of a nonlinear activation function is, of course, number one, is to introduce nonlinearities to our data, right? If we think about our data, almost all data that we care about, all real-world data is highly nonlinear. Now this is important because if we want to be able to deal with those types of data sets, we need models that are also nonlinear so they can capture those same types of patterns. So imagine I told you to separate, for example, I gave you this data set, red points from green points, and I asked you to try and separate those two types of data points. Now, you might think that this is easy, but what if I could only, if I told you that you could only use a single line to do so? Well, now it becomes a very complicated problem. In fact, you can't really solve it effectively with a single line. And in fact, if you introduce nonlinear activation functions to your solution, that's exactly what allows you to, you know, deal with these types of problems. Nonlinear activation functions allow you to deal with nonlinear types of data. Now, and that's what exactly makes neural networks so powerful at their core. So let's understand this maybe with a very simple example, walking through this diagram of a perceptron one more time. Imagine I give you this trained neural network with weights now not w1, w2. I'm going to actually give you numbers at these locations, right? So the trained weights w0 will be 1 and w will be a vector of 3 and negative 2. So this neural network has two inputs, like we said before it has input x1 and has input x2. If we want to get the output of it, this is also the main thing I want all of you to take away from this lecture today, is that to get the output of a perceptron there are three steps we need to take, right. From this stage, we first compute the multiplication of our inputs with our weights, sorry, yeah, multiply them together, add their result, and compute a non-linearity. It's these three steps that define the forward propagation of information through a perceptron. So let's take a look at how that exactly works, right. So if we plug in these numbers to those equations, we can see that everything inside of our non-linearity, here the non-linearity is G, right, that function G, which could be a sigmoid, we saw a previous slide. That component inside of our non-linearity is in fact just a two-dimensional line. It has two inputs and if we consider the space of all of the possible inputs that this neural network could see, we can actually plot this on a decision boundary, right? We can plot this two-dimensional line as a decision boundary, as a plane separating these two components of our space. In fact, not only is it a single plane, there's a directionality component, depending on which side of the plane that we live on. If we see an input, for example here, negative 1, 2, we actually know that it lives on one side of the plane, and it will have a certain type of output. In this case, that output is going to be positive, right, because in this case, when we plug those components into our equation, we'll get a positive number that passes through the nonlinear component and that gets propagated through as well. Of course, if you're on the other side of the space, you're going to have the opposite result, right, and that thresholding function is going to essentially live at this decision boundary. So depending on which side of the space you live on, that thresholding function, that sigmoid function, is going to then control how you move to one side or the other. Now in this particular example, this is very convenient, right, because we can actually visualize, and I can draw this exact full space for you on this slide. It's only a two-dimensional space, so it's very easy for us to visualize. But of course, for almost all problems that we care about, our data points are not going to be two-dimensional. If you think about an image, the dimensionality of an image is going to be the number of pixels that you have in the image, right? So these are going to be thousands of dimensions, millions of dimensions, or even more. And then drawing these types of plots, like you see here is simply not feasible, right? So we can't always do this, but hopefully this gives you some intuition to understand kind of as we build up into more complex models. So now that we have an idea of the perceptron, let's see how we can actually take this single neuron and start to build it up into something more complicated, a full neural network, and build a model from that. So let's revisit again this previous diagram of the perceptron. If, again, just to reiterate one more time, this core piece of information that I want all of you to take away from this class is how a perceptron works and how it propagates information to its decision. There are three steps. First is the dot product, second is the bias, and third is the non-linearity. And you keep repeating this process for every single perceptron in your neural network. Let's simplify the diagram a little bit. I'll get rid of the weights, and you can assume that every line here now basically has an associated weight scalar that's associated with it. Every line also corresponds to the input that's coming in. It has a weight that's coming in also at the, on the line itself, and I've also removed the bias just for a sake of simplicity, but it's still there. So now the result is that Z, which let's call that the result of our dot product plus the bias, is going, and that's what we pass into our nonlinear function, that piece is going to be applied to that activation function. Now the final output here is simply going to be G, which is our activation function, of Z, right. Z is going to be basically, you can think of the state of this neuron, it's the result of that dot product plus bias. Now, if we want to define and build up a multi-layered output neural network, if we want two outputs to this function, for example, it's a very simple procedure. We just have now two neurons, two perceptrons. Each perceptron will control the output for its associated piece, right? So now we have two outputs. Each one is a normal perceptron. It takes all of the inputs, so they both take the same inputs, but amazingly, now with this mathematical understanding, we can start to build our first neural network entirely from scratch. So what does that look like? So we can start by firstly initializing these two components. The first component that we saw was the weight matrix, excuse me, the weight vector. It's a vector of weights in this case. And the second component is the bias vector that we're going to multiply with the dot product of all of our inputs by our weights, right. So the only remaining step now, after we've defined these parameters of our layer, is to now define, you know, how this forward propagation of information works. And that's exactly those three main components that I've been stressing to you. So we can create this call function to do exactly that, to define this forward propagation of information. And the story here is exactly the same as we've been seeing it, right? Matrix multiply our inputs with our weights, right? Add a bias and then apply a non-linearity and return the result. And that, literally, this code will run, this will define a full neural network layer that you can then take, like this, and of course actually luckily for all of you, all of that code, which wasn't much code, that's been abstracted away by these libraries like TensorFlow, you can simply call functions like this which will actually you know replicate exactly that piece of code. So you don't need to necessarily copy all of that code down, you just can just call it. And with that understanding you know we just saw how you could build a single layer but of course now you can actually start to think about how you can stack these layers as well. So since we now have this transformation essentially from our inputs to a hidden output, you can think of this as basically how we can define some way of transforming those inputs, right, into some new dimensional space, right, perhaps closer to the value that we want to predict. And that transformation is going to be eventually learned, to know how to transform those inputs into our desired outputs, and we'll get to that later. But for now, the piece that I want to really focus on is, if we have these more complex neural networks, I want to really distill down that this is nothing more complex than what we've already seen. If we focus on just one neuron in this diagram, take as here for example Z2, right, Z2 is this neuron that's highlighted in the middle layer. It's just the same perceptron that we've been seeing so far in this class. It was a, its output is obtained by taking a dot product, adding a bias, and then applying that non-linearity between all of its inputs. If we look at a different node, for example Z3, which is the one right below it, it's the exact same story again. It sees all of the same inputs, but it has a different set of weight matrix that it's going to apply to those inputs, so it'll have a different output. But the mathematical equations are exactly the same. So from now on, I'm just going to kind of simplify all of these lines and diagrams just to show these icons in the middle just to demonstrate that this means everything is going to be fully connected to everything and defined by those mathematical equations that we've been covering. But there's no extra complexity in these models from what you've already seen. Now if you want to stack these types of solutions on top of each other, these layers on top of each other, you can not only define one layer very easily, but you can actually create what are called sequential models. These sequential models, you can define one layer after another, and they define basically the forward propagation of information, not just from the neuron level, but now from the layer level. Every layer will be fully connected to the next layer, and the inputs of the secondary layer will be all of the outputs of the prior layer. Now of course if you want to create a very deep neural network, all the deep neural network is, is we just keep stacking these layers on top of each other. There's nothing else to this story. That's really as simple as it is. Once, so these layers are basically, all they are is just layers where the final output is computed, right, by going deeper and deeper into this progression of different layers, right, and you just keep stacking them until you get to the last layer, which is your output layer. It's your final prediction that you want to output, right. We can create a deep neural network to do all of this by stacking these layers and creating these more hierarchical models like we saw very early in the beginning of today's lecture. One where the final output is really computed by, you know, just going deeper and deeper into this system. Okay, so that's awesome. So we've now seen how we can go from a single neuron to a layer, to all the way to a deep neural network, right, building off of these foundational principles. Let's take a look at how exactly we can use these principles that we've just discussed to solve a very real problem that I think all of you are probably very concerned about this morning when you woke up. So that problem is how we can build a neural network to answer this question, which is, will I pass this class and if I will, will I not? So to answer this question, let's see if we can train a neural network to solve this problem. Okay, so to do this let's start with a very simple neural network. We'll train this model with two inputs, just two inputs. One input is going to be the number of lectures that you attend over the course of this one week, and the second input is going to be how many hours that you spend on your final project or your competition. Okay, so what we're going to do is firstly go out and collect a lot of data from all of the past years that we've taught this course, and we can plot all of this data because it's only two input space, we can plot this data on a two-dimensional feature space, right? We can actually look at all of the students before you that have passed the class and failed the class and see where they lived in this space for the amount of hours that they've spent, the number of lectures that they've attended and so on. Green points are the people who have passed, red are those who have failed. Now, and here's you, right? You're right here, four five is your coordinate space. You fall right there and you've attended four lectures, you've spent five hours on your final project. We want to build a neural network to answer the question of will you pass the class or will you fail the class? So let's do it. We have two inputs, one is four, one is five. These are two numbers, we can feed them through a neural network that we've just seen how we can build that, and we feed that into a single layered neural network. Three hidden units in this example, but we could make it larger if we wanted to be more expressive and more powerful. And we see here that the probability of you passing this class is 0.1. It's pretty abysmal. So why would this be the case, right? What did we do wrong? Because I don't think it's correct, right? When we looked at the space, it looked like actually you were a good candidate to pass the class. But why is the neural network saying that there's only a 10% likelihood that you should pass? Does anyone have any ideas? Exactly, exactly. So Exactly, exactly. So this neural network is just like it was just born, right? It has no information about the world or this class. It doesn't know what four and five mean or what the notion of passing or failing means, right? So exactly right. This neural network has not been trained. You can think of it kind of as a baby. It hasn't learned anything yet. So our job firstly is to train it and part of that understanding is we first need to tell the neural network when it makes mistakes, right. So mathematically we should now think about how we can answer this question, which is did my neural network make a mistake and if it made a mistake how can I tell it how big of a mistake it was so that the next time it sees this data point, can it do better, minimize that mistake. So in neural network language, those mistakes are called losses, right, and specifically you want to define what's called a loss function, which is going to take as input your prediction and the true prediction, right, and how far away your prediction is from the true prediction tells right? And how far away your prediction is from the true prediction tells you how big of a loss there is, right? So for example, let's say we want to build a neural network to do classification of, or sorry, actually even before that, I want to maybe give you some terminology. So there are multiple different ways of saying the same thing in neural networks and deep learning. So what I just described as the loss function is also commonly referred to as an objective function, empirical risk, a cost function. These are all exactly the same thing. They're all a way for us to train the neural network, to teach the neural network when it makes mistakes. And what we really ultimately want to do is over the course of an entire data set, not just one data point of mistakes, we want to say over the entire data set, we want to minimize all of the mistakes on average that this neural network makes. So if we look at the problem, like I said, of binary classification, will I pass this class or will I not? There's a yes or no answer, that means binary classification. Now we can use what's called a loss function of the softmax cross-entropy loss. And for those of you who aren't familiar, this notion of cross-entropy is actually developed here at MIT by Shawn Clannis- Shawn, excuse me, yes, Claude Shannon, who is a visionary. He did his master's here over 50 years ago. He introduced this notion of cross-entropy and that was, you know, pivotal in the ability for us to train these types of neural networks, even now into the future. So let's start by, instead of predicting a binary cross-entropy output, what if we wanted to predict a final grade of your class score, for example? That's no longer a binary output, yes or no. It's actually a continuous variable, right? It's the grade, let's say out of a hundred points. What is the value of your score in the class project, right? For this type of loss, we can use what's called a mean squared error loss. You can think of this literally as just subtracting your predicted grade from the true grade and minimizing that distance apart. So I think now we're ready to really put all of this information together and tackle this problem of training a neural network, right, to not just identify right, to not just identify how erroneous it is, how large its loss is, but more importantly minimize that loss as a function of seeing all of this training data that it observes. So we know that we want to find this neural network, like we mentioned before, that minimizes this empirical risk or this empirical loss averaged across our entire data set. Now this means that we want to find mathematically these W's, right, that minimize J of W. J of W is our loss function averaged over our entire data set and W is our weight. So we want to find the set of weights that on average is going to give us the minimum, the smallest loss as possible. Now remember that W here is just a list, basically it's just a group of all of the weights in our neural network. You may have hundreds of weights in a very, very small neural network, or in today's neural networks you may have billions or trillions of weights, and you want to find what is the value of every single one of these weights that's going to result in the smallest loss as possible. Now, how can you do this? Remember that our loss function, J of W, is just a function of our weights, right? So for any instantiation of our weights, we can compute a scalar value of, you know, how erroneous would our neural network be for this instantiation of our weights. So let's try and visualize, for example, in a very simple example of a two-dimensional space where we have only two weights. Extremely simple neural network here, very small, two-weight neural network, and we want to find what are the optimal weights that would train this neural network. We can plot basically the loss, how erroneous the neural network is for every single instantiation of these two weights, right. This is a huge space, it's an infinite space, but still we can try to, we can have a function that evaluates at every point in this space. Now what we ultimately want to do is, again, we want to find which set of W's will give us the smallest loss possible. That means basically the lowest point on this landscape that you can see here, where is the W's that bring us to that lowest point? The way that we do this is actually just by firstly starting at a random place. We have no idea where to start, so pick a random place to start in this space and let's start there. At this location, let's evaluate our neural network. We can compute the loss at this specific location and on top of that we can actually compute how the loss is changing. We can compute the gradient of the loss because our loss function is a continuous function, right, so we can actually compute derivatives of our function across the space of our weights and the gradient tells us the direction of the highest point, right? So from where we stand, the gradient tells us where we should go to increase our loss. Now, of course, we don't want to increase our loss, we want to decrease our loss, so we negate our gradient and we take a step in the opposite direction of the gradient. That brings us one step closer to the bottom of the landscape and we just keep repeating this process, right? Over and over again, we evaluate the neural network at this new location, compute its gradient and step in that new direction. We keep traversing this landscape until we converge to the minimum. We can really summarize this algorithm, which is known formally as gradient descent. So, gradient descent simply can be written like this. We initialize all of our weights. This can be two weights, like you saw in the previous example. It can be billions of weights, like in real neural networks. We compute this gradient of the partial derivative with of our loss with respect to the weights and then we can update our weights in the opposite direction of this gradient. So essentially we just take this small amount, small step you can think of it, which here is denoted as eta, and we refer to this small step, right. This is commonly referred to as what's known as the learning rate. It's like how much we want to trust that gradient and step in the direction of that gradient. We'll talk more about this later, but just to give you some sense of code, this algorithm is very well translatable to real code as well. For every line on the pseudocode you can see on the left, you can see corresponding real code on the right that is runnable and directly implementable by all of you in your labs. But now let's take a look specifically at this term here. This is the gradient. We touched very briefly on this in the visual example. This explains, like I said, how the loss is changing as a function of the weights, right? So as the weights move around, will my loss increase or decrease? And that will tell the neural network if it needs to move the weights in a certain direction or not. But I never actually told you how to compute this, right? And I think that's an extremely important part because if you don't know that, then you can't, well, you can't train your neural network, right? This is a critical part of training neural networks. And that process of computing this line, this gradient line, is known as backpropagation. So let's do a very quick intro to backpropagation and how it works. So again, let's start with the simplest neural network in existence. This neural network has one input, one output, and only one neuron, right? This is as simple as it gets. We want to compute the gradient of our loss with respect to our weight. In this case, let's compute it with respect to w2, the second weight. So, this derivative is going to tell us how much a small change in this weight will affect our loss. If a small change, if we change our weight a little bit in one direction, will it increase our loss or decrease our loss? So to compute that, we can write out this derivative. We can start with applying the chain rule backwards from the loss function through the output. Specifically, what we can do is we can actually just decompose this derivative into two components. The first component is the derivative of our loss with respect to our output, multiplied by the derivative of our output with respect to w2, right. This is just a standard instantiation of the chain rule with this original derivative that we had on the left-hand side. Let's suppose we wanted to compute the gradients of the weight before that, which in this case are not w1, but w, excuse me, not w2, but w1. Well, all we do is replace w2 with w1, and that chain rule still holds, right, that same equation holds, but now you can see on the red component, that last component of the chain rule, we have to once again recursively apply one more chain rule, because that's again another derivative that we can't directly evaluate. We can expand that once more with another instantiation of the chain rule, and now all of these components, we can directly propagate these gradients through the hidden units, right, in our neural network, all the way back to the weight that we're interested in in this example, right. So we first computed the derivative with respect to w2, then we can back propagate that and use that information also with W1. That's why we really call it back propagation, because this process occurs from the output all the way back to the input. Now, we repeat this process essentially many, many times over the course of training by propagating these gradients over and over again through the network all the way from the output to the inputs to determine for every single weight answering this question, which is how much does a small change in these weights affect our loss function, if it increases it or decreases it, and how we can use that to improve the loss ultimately, because that's our final goal in this class. So that's the backpropagation algorithm. That's the core of training neural networks. In theory, it's very simple. It's really just an instantiation of the chain rule. But let's touch on some insights that make training neural networks actually extremely complicated in practice, even though the algorithm of backpropagation is simple and, you know, many decades old. In practice, though, optimization of neural networks looks something like this. It looks nothing like that picture that I showed you before. There are ways that we can visualize very large deep neural networks, and you can think of the landscape of these models looking like something like this. This is an illustration from a paper that came out several years ago, where they tried to actually visualize the landscape of very, very deep neural networks. And that's what this landscape actually looks like. That's what you're trying to deal with and find the minimum in this space. And you can imagine the challenges that come with that. So, to cover the challenges, let's first think of and recall that update equation defined in gradient descent, right? So I didn't talk too much about this parameter, eta, but now let's spend a bit of time thinking about this. This is called the learning rate, like we saw before. It determines basically how big of a step we need to take in the direction of our gradient and every single iteration of back propagation. In practice, even setting the learning rate can be very challenging. You as the designer of the neural network have to set this value, this learning rate, and how do you pick this value, right? So that can actually be quite difficult. It has really large consequences when building a neural network. So for example, if we set the learning rate too low, then we learn very slowly. So let's assume we start on the right-hand side here at that initial guess. If our learning rate is not large enough, not only do we converge slowly, we actually don't even converge to the global minimum, right, because we kind of get stuck in a local minimum. Now what if we set our learning rate too high, right? What can actually happen is we overshoot and we can actually start to diverge from the solution. The gradients can actually explode, very bad things happen, and then the neural network doesn't train. So that's also not good. In reality, there's a very happy medium between setting it too small, setting it too large, where you set it just large enough to kind of overshoot some of these local minima, put you into a reasonable part of the search space, where then you can actually converge on the solutions that you care most about. But actually, how do you set these learning rates in practice, right? How do you pick what is the ideal learning rate? One option, and this is actually a very common option in practice, is to simply try out a bunch of learning rates and see what works the best, right? So try out, let's say, a whole grid of different learning rates and, you know, train all of these neural networks, see which one works the best. But I think we can do something a lot smarter, right? So what are some more intelligent ways that we could do this instead of exhaustively trying out a whole bunch of different learning rates? Can we design a learning rate algorithm that actually adapts to our neural network and adapts to its landscape so that it's a bit more intelligent than that previous idea. So this really ultimately means that the learning rate, the speed at which the algorithm is trusting the gradients that it sees, is going to depend on how large the gradient is in that location and how fast we're learning. How many other options, and sorry, and many other options that we might have as part of training in neural networks, right? So it's not only how quickly we're learning, you may judge it on many different factors in the learning landscape. In fact, we've all been, these different algorithms that I'm talking about, these adaptive learning rate algorithms, have been very widely studied in practice. There is a very thriving community in the deep learning research community that focuses on developing and designing new algorithms for learning rate adaptation and faster optimization of large neural networks like these, and during your labs, you'll actually get the opportunity to not only try out a lot of these different adaptive algorithms which you can see here, but also try to uncover what are kind of the patterns and benefits of one versus the other. And that's going to be something that I think you'll find very insightful as part of your labs. So another key component of your labs that you'll see is how you can actually put all of this information that we've covered today into a single picture that looks roughly something like this, which defines your model at the first, at the top here. That's where you define your model. We talked about this in the beginning part of the lecture. For every piece in your model you're now going to need to define this optimizer, which we've just talked about. This optimizer is defined together with a learning rate, how quickly you want to optimize your lost landscape. And over many loops, you're going to pass over all of the examples in your dataset and observe essentially how to improve your network, that's the gradient, and then actually improve the network in those directions. And keep doing that over and over and over again until eventually your neural network converges to some sort of solution. So I want to very quickly, briefly in the remaining time that we have, continue to talk about tips for training these neural networks in practice and focus on this very powerful idea of batching your data into what are called mini-batches of smaller pieces of data. To do this, let's revisit that gradient descent algorithm. So here, this gradient that we talked about before is actually extraordinarily computationally expensive to compute because it's computed as a summation across all of the pieces in your data set, right? And in most real life or real world problems, you know, it's simply not feasible to compute a gradient over your entire data set. Data sets are just too large these days. So, you know, there are some alternatives, right? What are the alternatives? Instead of computing the derivative or the gradients across your entire data set, the derivative or the gradients across your entire data set, what if you instead computed the gradient over just a single example in your data set, just one example. Well of course this estimate of your gradient is going to be exactly that, it's an estimate, it's going to be very noisy, it may roughly reflect the trends of your entire data set, but because it's a very, it's only one example in fact, of your entire data set, it may be very noisy, right. Well, the advantage of this though is that it's much faster to compute, obviously, the gradient over a single example because it's one example. So, computationally, this has huge advantages, but the downside is that it's extremely stochastic, right. That's the reason why this algorithm is not called gradient descent, it's called stochastic gradient descent now. Now, what's the middle ground, right? Instead of computing it with respect to one example in your data set, what if we computed what's called a mini batch of examples? A small batch of examples that we can compute the gradients over, and when we take these gradients, they're still computationally efficient to compute because it's a mini-batch, it's not too large, maybe we're talking on the order of tens or hundreds of examples in our data set, but more importantly, because we've expanded from a single example to maybe a hundred examples, the stochasticity is significantly reduced and the accuracy of our gradient is much improved. So normally we're thinking of batch sizes, many batch sizes, roughly on the order of a hundred data points, tens or hundreds of data points. This is much faster, obviously, to compute than gradient descent and much more accurate to compute compared to stochastic gradient descent, which is that single, single point example. So this increase in gradient accuracy allows us to essentially converge to our solution much quicker than it could have been possible in practice due to gradient descent limitations. It also means that we can increase our learning rate because we can trust each of those gradients much more efficiently, right? We're now averaging over a batch. It's going to be much more accurate than the stochastic version, so we can increase that learning rate and actually learn faster as well. This allows us to also massively parallelize this entire algorithm and computation. Right? We can split up batches onto separate workers and achieve even more significant speed-ups of this entire problem using GPUs. The last topic that I very, very briefly want to cover in today's lecture is this topic of overfitting, right. When we're optimizing a neural network with stochastic gradient descent, we have this challenge of what's called overfitting. Overfitting looks like this, roughly, right? So on the left-hand side, we want to build a neural network or let's say in general, we want to build a machine learning model that can accurately describe some patterns in our data, but remember, ultimately we don't want to describe the patterns in our training data. Ideally, we want to define the patterns in our test data. Of course, we don't observe test data, we only observe training data. So we have this challenge of extracting patterns from training data and hoping that they generalize to our test data. So set in one different way, we want to build models that can learn representations from our training data that can still generalize even when we show them brand new unseen pieces of test data. So assume that you want to build a line that can describe or find the patterns in these points that you can see on the slide, right. If you have a very simple neural network, which is just a single line, straight line, you can describe this data sub-optimally, right, because the data here is nonlinear. You're not going to accurately capture all of the nuances and subtleties in this data set. That's on the left-hand side. If you move to the right-hand side, you can see a much more complicated model, but here you're actually over-expressive. You're too expressive and you're capturing kind of the nuances, the spurious nuances in your training data that are actually not representative of your test data. Ideally you want to end up with the model in the middle which is basically the middle ground, right? It's not too complex and it's not too simple. It still gives you what you want to perform well and even when you give it brand new data. So to address this problem let's briefly talk about what's called regularization. Regularization is a technique that you can introduce to your training pipeline to discourage complex models from being learned. Now, as we've seen before, this is really critical because neural networks are extremely large models, they are extremely prone to overfitting, right? So regularization, having techniques for regularization has extreme implications towards the success of neural networks and having them generalized beyond training data far into our testing domain. The most popular technique for regularization in deep learning is called dropout and the idea of dropout is is actually very simple. Let's revisit it by drawing this picture of deep neural networks that we saw earlier in today's lecture. In dropout during training, we essentially randomly select some subset of the neurons in this neural network and we try to prune them out with some random probability. So, for example, we can select this subset of neural- of neurons, we can randomly select them with a probability of 50 percent, and with that probability, we can randomly select them with a probability of 50% and with that probability we randomly turn them off or on on different iterations of our training. So this is essentially forcing the neural network to learn you can think of an ensemble of different models. On every iteration it's going to be exposed to kind of a different model internally than the one it had on the last iteration. So it has to learn how to build internal pathways to process the same information and it can't rely on information that it learned on previous iterations, right. So it forces it to kind of capture some deeper meaning within the pathways of the neural network and this can be extremely powerful because number one it lowers the capacity of the neural network significantly, right, you're lowering it by roughly 50% in this example, but also because it makes them easier to train because the number of weights that have gradients in this case is also reduced so it's actually much faster to train them as well. Now like I mentioned, on every iteration we randomly drop out a different set of neurons, right, and that helps the data generalize better. And the second regularization technique, which is actually a very broad regularization technique far beyond neural networks, is simply called early stopping. Now, we know the definition of overfitting is simply when our model starts to represent basically the training data more than the testing data. That's really what overfitting comes down to at its core. If we set aside some of the training data to use separately that we don't train on it, we can use a kind of a testing data set, synthetic testing data set in some ways. We can monitor how our network is learning on this unseen portion of data. So for example, over the course of training, we can basically plot the performance of our network on both the training set as well as our held out test set. As the network is trained, we're going to see that first of all, these both decrease, but there's going to be a point where the loss plateaus and starts to increase. The training loss will actually start to increase. This is exactly the point where you start to overfit, right, because now you're starting to have, sorry, that was the test loss. The test loss actually starts to increase because now you're starting to overfit on your training data. This pattern basically continues for the rest of training and this is the point that I want you to focus on, right. This middle point is where we need to stop training because after this point, assuming that this test set is a valid representation of the true test set, this is the place where the accuracy of the model will only get worse, right. So this is where we would want to early stop our model and regularize the performance. And we can see that stopping anytime before this point is also not good. We're going to produce an underfit model where we could have had a better model on the test data, but it's this trade-off, right? You can't stop too late and you can't stop too early as well. So, I'll conclude this lecture by just summarizing these three key points that we've covered in today's lecture so far. So we first covered these fundamental building blocks of all neural networks, which is the single neuron, the perceptron. We've built these up into larger neural layers, and then from there, neural networks and deep neural networks. We've learned how we can train these, apply them to data sets, back propagate through them. And we've seen some trips, tips and tricks for optimizing these systems end-to-end. In the next lecture we'll hear from Ava on deep sequence modeling using RNNs and specifically this very exciting new type of model called the transformer architecture and attention mechanisms. So maybe let's resume the class in about five minutes after we have a chance to swap speakers and thank you so much for all of your attention.", 3492, "2023-03-10 00:00:00"], ["MIT 6.S191: Introduction to Deep Learning", "MIT 6.S191: Recurrent Neural Networks, Transformers, and Attention", "https://www.youtube.com/watch?v=ySEx_Bqxvvo", " Hello everyone, and I hope you enjoyed Alexander's first lecture. I'm Ava, and in this second lecture, lecture two, we're going to focus on this question of sequence modeling, how we can build neural networks that can handle and learn from sequential data. So in Alexander's first lecture, he introduced the essentials of neural networks, starting with perceptrons, building up to feed-forward models, and how you can actually train these models and start to think about deploying them forward. Now we're going to turn our attention to specific types of problems that involve sequential processing of data, and we'll realize why these types of problems require a different way of implementing and building neural networks from what we've seen so far. And I think some of the components in this lecture traditionally can be a bit confusing or daunting at first, but what I really, really want to do is to build this understanding up from the foundations, walking through step-by-step, developing intuition all the way to understanding the math and the operations behind how these networks operate. Okay, so let's get started. To begin, I first want to motivate what exactly we mean when we talk about sequential data or sequential modeling. So we're gonna begin with a really simple intuitive example. Let's say we have this picture of a ball, and your task is to predict where this ball is going to travel to next. Now, if you don't have any prior information about the trajectory of the ball, its motion, its history, any guess or prediction about its next position is going to be exactly that, a random guess. If however, in addition to the current location of the ball, I gave you some information about where it was moving in the past, now the problem becomes much easier. And I think, hopefully we can all agree agree that most likely, our most likely next prediction is that this ball is going to move forward to the right in the next frame. So this is a really, you know, reduced down bare-bones intuitive example, but the truth is that beyond this, sequential data is really all around us, right? As I'm speaking, the words coming out of my mouth form a sequence of sound waves that define audio, which we can split up to think about in this sequential manner. Similarly, text, language, can be split up into a sequence of characters or a sequence of words. And there are many, many, many more examples in which sequential processing, sequential data is present, right? From medical signals like EKGs to financial markets and projecting stock prices to biological sequences encoded in DNA to patterns in the climate to patterns of motion and many more. And so already, hopefully, you're getting a sense of what these types of questions and problems may look like and where they are relevant in the real world. When we consider applications of sequential modeling in the real world, we can think about a number of different kind of problem definitions that we can have in our arsenal and work with. In the first lecture, Alexander introduced the notions of classification and the notion of regression, where he talked about and we learned about feedforward models that can operate one-to-one in this fixed and static setting, right? Given a single input, predict a single output. The binary classification example of will you succeed or pass this class? And here there's no notion of sequence, there's no notion of time. Now, if we introduce this idea of a sequential component, we can handle inputs that may be defined temporally and potentially also produce a sequential or temporal output. So, as one example, we can consider text, language, and maybe we want to generate one prediction given a can consider text, language, and maybe we want to generate one prediction given a sequence of text, classifying whether a message is a positive sentiment or a negative sentiment. Conversely, we could have a single input, let's say an image, and our goal may be now to generate text or a sequential description of this image. Given this image of a baseball player throwing a ball, can we build a neural network that generates that as a language caption? Finally, we can also consider applications and problems where we have sequence in, sequence out. For example, if we want to translate between two languages, and indeed, this type of thinking and this type of architecture is what powers the task of machine translation in your phones, in Google Translate, and many other examples. So, hopefully, right, this has given you a picture of what sequential data looks like, what these types of problem definitions may look like. And from this, we're going to start and build up our understanding of what neural networks we can build and train for these types of problems. So, first we're going to begin with the notion of recurrence and build up from that to define recurrent neural networks, and in the last portion of the lecture, we'll talk about the underlying mechanisms, underlying the transformer architectures that are very, very, very powerful in terms of handling sequential data. But as I said at the beginning, right, the theme of this lecture is building up that understanding step-by-step, starting with the fundamentals and the intuition. So to do that, we're going to go back, revisit the perceptron, and move forward from there. Right, so as Alexander introduced, where we study the perceptron in lecture one, the perceptron is defined by this single neural operation where we have some set of inputs, let's say x1 through xm, and each of these numbers are multiplied by a corresponding weight, passed through a nonlinear activation function that then generates a predicted output y hat. Here we can have multiple inputs coming in to generate our output, but still these inputs are not thought of as points in a sequence or time steps in a sequence. Even if we scale this perceptron and start to stack multiple perceptrons together to define these feed-forward neural networks, we still don't have this notion of temporal processing or sequential information. Even though we are able to translate and convert multiple inputs, apply these weight operations, apply this non-linearity to then define multiple predicted outputs. So, taking a look at this diagram, right, on the left in blue you have inputs, on the right in purple you have these outputs, and the green defines the neural- the single neural network layer that's transforming these inputs to the outputs. Next step, I'm going to just simplify this diagram. I'm going to collapse down those stacked perceptrons together and depict this with this green block. Still it's the same operation going on, right? We have an input vector being transformed to predict this output vector. Now, what I've introduced here, which you may notice, is this new variable t, which I'm using to denote a single time step. We are considering an input at a single time step and using our neural network to generate a single output corresponding to that. How can we start to extend and build off this to now think about multiple time steps and how we could potentially process a sequence of information. Well, what if we took this diagram, all I've done is just rotated it 90 degrees, where we still have this input vector and being fed in producing an output vector, and what if we can make a copy of this network, right, and just do this operation multiple times to try to handle inputs that are fed in corresponding to different times? We have an individual time step starting with T0, and we can do the same thing, the same operation for the next time step. Again, treating that as an isolated instance, and keep doing this repeatedly. And what you'll notice hopefully is all these models are simply copies of each other, just with different inputs at each of these different time steps. And we can make this concrete, right, in terms of what this functional transformation is doing. The predicted output at a particular time step, y hat of t, is a function of the input at that time step, x of t, and that function is what is learned and defined by our neural network weights. Okay, so I've told you that our goal here is, right, trying to understand sequential data, do sequential modeling, but what can be the issue with what this diagram is showing and what I've shown you here? Well, yeah, go ahead. Exactly, that's exactly right. So the student's answer was that x1 or it could be related to x naught and you have this temporal dependence but these isolated replicas don't capture that at all. And that's exactly answers the question perfectly right. Here a predicted output at a later time step could depend precisely on inputs at previous time steps if this is truly a sequential problem with this temporal dependence. So how can we start to reason about this? How could we define a relation that links the network's computations at a particular time step to prior history and memory from previous time steps? Well, what if we did exactly that, right? What if we simply linked the computation and the information understood by the network to these other replicas via what we call a recurrence relation? What this means is that something about what the network is computing at a particular time is passed on to those later time steps. And we define that according to this variable, h, which we call this internal state, or you can think of it as a memory term, that's maintained by the neurons and the network, and it's this state that's being passed time step to time step as we read in and process this sequential information. What this means is that the network's output, its predictions, its computations, is not only a function of the input data X, but also we have this other variable, H, which captures this notion of state, captures this notion of memory that's being computed by the network and passed on over time. Specifically, right, to walk through this, our predicted output, y-hat of t, depends not only on the input at a time, but also this past memory, this past state. And it is this linkage of temporal dependence and recurrence that defines this idea of a recurrent neural unit. What I've shown is this connection that's being unrolled over time. and recurrence that defines this idea of a recurrent neural unit. What I've shown is this connection that's being unrolled over time, but we can also depict this relationship according to a loop. This computation to this internal state variable, h of t, is being iteratively updated over time and that's fed back into the neuron's computation in this recurrence relation. This is how we define these recurrent cells that comprise recurrent neural networks or RNNs and the key here is that we have this idea of this recurrence relation that captures the cyclic temporal dependency. And indeed, it's this idea that is really the intuitive foundation behind recurrent neural networks or RNNs. And so let's continue to build up our understanding from here and move forward into how we can actually define the RNN operations mathematically and in code. So all we're going to do is formalize this relationship a little bit more. The key idea here is that the RNN is maintaining the state, and it's updating the state at each of these time steps, as the sequence is processed. We define this by applying this recurrence relation, and what the recurrence relation captures is how we're actually updating that internal state h of t. Specifically, that state update is exactly like any other neural network operation that we've introduced so far, where again we're learning a function defined by a set of weights w. We're using that function to update the cell state h of t, and the additional component, the newness here, is that that function depends both on the input and the prior time step h of t minus 1. And what you'll note is that this function f sub w is defined by a set of weights, and it's the same set of weights, the same set of parameters that are used time step to time step as the recurrent neural network processes this temporal information, the sequential data. Okay. So the key idea here, hopefully is coming through, is that this RNN state update operation takes this state and updates it each time a sequence is processed. We can also translate this to how we can think about implementing RNNs in Python code or rather pseudocode, hopefully getting a better understanding and intuition behind how these networks work. So what we do is we just start by defining an RNN. For now this is abstracted away. And we start, we initialize its hidden state and we have some sentence, right, let's say this is our input of interest where we're interested in predicting maybe the next word that's occurring in this sentence. What we can do is loop through these individual words in the sentence that define our temporal input, and at each step as we're looping through, each word in that sentence is fed into the RNN model along with the previous hidden state. This is what generates a prediction for the next word and updates the RNN state in turn. Finally, our prediction for the final word in the sentence, the word that we're missing, is simply the RNN's output after all the prior words have been fed in through the model. So this is really breaking down how the RNN works, how it's processing the sequential information. What you've noticed is that the RNN computation includes both this update to the hidden state, as well as generating some predicted output at the end. That is our ultimate goal that we're interested in. So to walk through this, how we're actually generating the output prediction itself, what the RNN computes is given some input vector, it then performs this update to the hidden state, and this update to the hidden state is just a standard neural network operation, just like we saw in the first lecture, where it consists of taking a weight matrix, multiplying that by the previous hidden state, taking another weight matrix, multiplying that by the previous hidden state, taking another weight matrix, multiplying that by the input at a time step, and applying a non-linearity. And in this case, right, because we have these two input streams, the input data X of t and the previous state h, we have these two separate weight matrices that the network is learning over the course of its training. That comes together, we apply the non-linearity, and then we can generate an output at a given time step by just modifying the hidden state using a separate weight matrix to update this value and then generate a predicted output. And that's what there is to it. That's how the RNN in its single operation updates both the hidden state and also generates a predicted output. So now this gives you the internal working of how the RNN computation occurs at a particular time step. Let's next think about how this looks like over time and define the computational graph of the RNN as being unrolled or expanded across time. So, so far, the dominant way I've been showing the RNNs is according to this loop-like diagram on the left, feeding back in on itself. Another way we can visualize and think about RNNs is as kind of unrolling this recurrence over time, over the individual time steps in our sequence. What this means is that we can take the network at our first time step and continue to iteratively unroll it across the time steps going on forward all the way until we process all the time steps in our input. Now we can formalize this diagram a little bit more by defining the weight matrices that connect the inputs to the hidden state update inputs to the hidden state update and the weight matrices that are used to update the internal state across time and finally the weight matrices that define the update to generate a predicted output. Now recall that in all these cases, right, for all these three weight matrices at all these time steps we are simply reusing the same weight matrices, right? So it's one set of parameters, one set of weight matrices that just process this information sequentially. Now, you may be thinking, okay, so how do we actually start to be thinking about how to train the RNN, how to define the loss, given that we have this temporal processing in this temporal dependence. Well a prediction at an individual time step will simply amount to a computed loss at that particular time step. So now we can compare those predictions time step by time step to the true label and generate a loss value for those time steps. Finally, we can get our total loss by taking all these individual loss terms together and summing them, defining the total loss for a particular input to the RNN. If we can walk through an example of how we implement this RNN in TensorFlow starting from scratch, the RNN can be defined as a layer operation and a layer class that Alexander introduced in the first lecture. So we can define it according to an initialization of weight matrices, initialization of a hidden state, which commonly amounts to initializing these two to zero. Next, we can define how we can actually which commonly amounts to initializing these two to zero. Next, we can define how we can actually pass forward through the RNN network to process a given input x. And what you'll notice is in this forward operation, the computations are exactly like we just walked through. We first update the hidden state according to that equation we introduced earlier, and then generate a predicted output that is a transformed version of that hidden state. Finally, at each time step, we return it both the output and the updated hidden state, as this is what is necessary to be stored to continue this RNN operation over time. What is very convenient is that although you could define your RNN network and your RNN layer completely from scratch, is that TensorFlow abstracts this operation away for you. So you can simply define a simple RNN according to this call that you're seeing here, which makes all the computations very efficient and very easy. You'll actually get practice implementing and working with RNNs in today's software lab. So that gives us the understanding of RNNs. Going back to what I described as the problem setups or the problem definitions at the beginning of this lecture, I just want to remind you of the types of sequence modeling problems on which we can apply RNNs, right. We can think about taking a sequence of inputs, producing one predicted output at the end of the sequence. We can think about taking a static single input and trying to generate text according to that single input. And finally, we can think about taking a sequence of inputs, producing a prediction at every time step in that sequence, and then doing this sequence to sequence type of prediction and translation. Okay, so, yeah. So this will be the foundation for the software lab today, which will focus on this problem of many to many processing, and many to many sequential modeling, taking a sequence, going to a sequence. What is common and what is universal across all these types of problems and tasks that we may want to consider with RNNs is what I like to think about, what type of design criteria we need to build a robust and reliable network for processing these sequential modeling problems. What I mean by that is what are the characteristics? What are the design requirements that the RNN needs to fulfill in order to be able to handle sequential data effectively? The first is that sequences can be of different lengths. Right? They may be short, they may be long. We want our RNN model or our neural network model in general to be able to handle sequences of variable lengths. Secondly, and really importantly, is as we were discussing earlier, that the whole point of thinking about things through the lens of sequence is to try to track and learn dependencies in the data that are related over time. So our model really needs to be able to handle those different dependencies, which may occur at times that are very, very distant from each other. Next, sequence is all about order. There's some notion of how current inputs depend on prior inputs, and the specific order of observations we see, makes a big effect on what prediction we may want to generate at the end. Finally, in order to be able to process this information effectively, our network needs to be able to do what we call parameter sharing, meaning that given one set of weights, that set of weights should be able to apply to different time steps in the sequence and still result in a meaningful prediction. And so today we're going to focus on how recurrent neural networks meet these design criteria and how these design criteria motivate the need for even more powerful architectures that can outperform RNNs in sequence modeling. So to understand these criteria very concretely, we're going to consider a sequence modeling problem where, given some series of words, our task is just to predict the next word in that sentence. So let's say we have this sentence, this morning I took my cat for a walk. Our task is to predict the last word in the sentence given the prior words, this morning I took my cat for a blank. Our goal is to take our RNN, define it, and put it to test on this task. What is our first step to doing this? Well, the very, very first step before we even think about defining the RNN is how we can actually represent this information to the network in a way that it can process and understand. If we have a model that is processing this data, processing this text-based data and wanting to generate text as the output, our problem can arise in that the neural network itself is not equipped to handle language explicitly, right? Remember that neural networks are simply functional operators, they're just mathematical operations and so we can't expect it, right, it doesn't have an understanding from the start of what a word is or what language means. Which means that we need a way to represent language numerically so that it can be passed in to the network to process. So what we do is that we need to define a way to translate this text, this language information, into a numerical encoding, a vector, an array of numbers that can then be fed in to our neural network and generating a vector of numbers as its output. So now, right, this raises the question of how do we actually define this transformation? How can we transform language into this numerical encoding? The key solution and the key way that a lot of these networks work is this notion and concept of embedding. What that means is it's some transformation that takes indices or something that can be represented as an index into a numerical vector of a given size. So if we think about how this idea of embedding works for language data, let's consider a vocabulary of words that we can possibly have in our language. And our goal is to be able to map these individual words in our vocabulary to a numerical vector of fixed size. One way we could do this is by defining all the possible words that could occur in this vocabulary and then indexing them, assigning a index label to each of these distinct words. A corresponds to index one, cat responds to index two, so on and so forth. And this indexing maps these individual words to numbers, unique indices. What these indices can then define is what we call a embedding vector, which is a fixed length encoding where we've simply indicated a one value at the index for that word when we observe that word. And this is called a one-hot embedding, where we have this fixed length vector of the size of our vocabulary, and each instance of the vocabulary corresponds to a one-hot one at the corresponding index. This is a very sparse way to do this, and it's simply based on purely count the count index. There's no notion of semantic information, meaning that's captured in this vector-based encoding. Alternatively, what is very commonly done is to actually use a neural network to learn an encoding, to learn an embedding. The goal here is that we can learn a neural network that then captures some inherent meaning or inherent semantics in our input data, and maps related words or related inputs closer together in this embedding space, meaning that they'll have numerical vectors that are more similar to each other. This concept is really, really foundational to how these sequence modeling networks work and how neural networks work in general. Okay, so with that in hand, we can go back to our design criteria, thinking about the capabilities that we desire. First, we need to be able to handle variable length sequences. If we again want to predict the next word in the sequence, we can have short sequences, we can have long sequences, we can have even longer sentences, and our key task is that we want to be able to track dependencies across all these different lengths. And what we need, what we mean by dependencies is that there could be information very, very early on in a sequence, but that may not be relevant or come up late until very much later in the sequence. And we need to be able to track these dependencies and maintain this information in our network. Dependencies relate to order and sequences are defined by their order and we know that same words in a completely different order have completely different meanings, right? So our model needs to be able to handle these differences in order and the differences in length that could result in different predicted outputs. Okay, so hopefully that example, going through the example in text, motivates how we can think about transforming input data into a numerical encoding that can be passed into the RNN, and also what are the key criteria that we want to meet in handling these types of problems. So, so far we've painted the picture of RNNs, how they work, intuition, their mathematical operations, and what are the key criteria that they need to meet. The final piece to this is how we actually train and learn the weights in the RNN. And that's done through back propagation algorithm with a bit of a twist to just handle sequential information. If we go back and think about how we train feed-forward neural network models, the steps break down in thinking through starting with an input, where we first take this input and make a forward pass through the network, going from input to output. The key to backpropagation that Alexander introduced was this idea of taking the prediction and backpropagating gradients back through the network and using this operation to then define and update the loss with respect to each of the parameters in the network in order to gradually adjust the parameters, the weights of the network, in order to minimize the overall loss. Now with RNNs, as we walked through earlier, we had this temporal unrolling, which means that we have these individual losses across the individual steps in our sequence that sum together to comprise the overall loss. What this means is that when we do backpropagation we have to now instead of backpropagating errors through a single network, backpropagate the loss through each of these individual time steps. And after we backpropagate loss through each of the individual time steps, we then do that across all time steps, all the way from our current time, time t, back to the beginning of the sequence. And this is why this algorithm is called back propagation through time. Because as you can see, the data and the predictions and the resulting errors are fed back in time all the way from where we are currently to the very beginning of the input data sequence. So the back propagation through time is actually a very tricky algorithm to implement in practice, and the reason for this is if we take a close look, looking at how gradients flow across the RNN, what this algorithm involves is many, many repeated computations and multiplications of these weight matrices repeatedly against each other. In order to compute the gradient with respect to the very first time step, we have to make many of these multiplicative repeats of the weight matrix. Why might this be problematic? Well, if this weight matrix W is very, very big, what this can result in is what they call- what we call the exploding gradient problem, where our gradients that we're trying to use to optimize our network do exactly that. They blow up, they explode, and they get really big and makes it infeasible and not possible to train the network stably. What we do to mitigate this is a pretty simple solution called gradient clipping, which effectively scales back these very big gradients to try to constrain them in a more restricted way. Conversely, we can have the instance where the weight matrices are very, very small, and if these weight matrices are very, very small, we end up with a very, very small value at the end as a result of these repeated weight matrix computations and these repeated multiplications. And this is a very real problem in RNNs in particular, where we can lead into this funnel called a vanishing gradient, where now your gradient has just dropped down close to zero and again you can't train the network stably. Now, there are particular tools that we can use to implement to try to mitigate this vanishing gradient problem. We'll touch on each of these three solutions briefly. First being, how we can define the activation function in our network, and how we can change the network architecture itself to try to better handle this vanishing gradient problem. Before we do that, I want to take just one step back to give you a little more intuition about why vanishing gradients can be a real issue for recurrent neural networks. Point I've kept trying to reiterate is this notion of dependency in the sequential data, and what it means to track those dependencies. Well, if the dependencies are very constrained in a small space, not separated out that much by time, this repeated gradient computation and the repeated weight matrix multiplication is not so much of a problem. If we have a very short sequence where the words are very closely related to each other and it's pretty obvious what our next output is going to be, the RNN can use the immediately passed information to make a prediction. And so there are not going to be that many, that much of a requirement to learn effective weights if the related information is close to each other temporally. Conversely now, if we have a sentence where we have a more long-term dependency, what this means is that we need information from way further back in the sequence to make our prediction at the end, and that gap between what's relevant and where we are at currently becomes exceedingly large and therefore the vanishing gradient problem is increasingly exacerbated, meaning that we really need to- the RNN becomes unable to connect the dots and establish this long-term dependency, all because of this vanishing gradient issue. So the ways and modifications that we can make to our network to try to alleviate this problem, threefold. The first is that we can simply change the activation functions in each of our neural network layers to be such that they can effectively try to mitigate and safeguard from gradients in instances where, from shrinking the gradients in instances where the data is greater than zero. And this is in particular true for the ReLU activation function. And the reason is that in all instances where x is greater than zero, with the ReLU function, the derivative is one. And so that is not less than one, and therefore it helps in mitigating the vanishing gradient problem. Another trick is how we initialize the parameters in the network themselves to prevent them from shrinking to zero too rapidly, and there are mathematical ways that we can do this, namely by initializing our weights to identity matrices, and this effectively helps in practice to prevent the weight updates to shrink too rapidly to zero. However, the most robust solution to the vanishing gradient problem is by introducing a slightly more complicated version of the recurrent neural unit to be able to more effectively track and handle long-term dependencies in the data. And this is this idea of gating. And what the idea is, is by controlling selectively the flow of information into the neural unit to be able to filter out what's not important while maintaining what is important. And the key and the most popular type of recurrent unit that achieves this gated computation is called the LSTM, or long short-term memory network. Today, we're not going to go into detail on LSTMs, their mathematical details, their operations, and so on, but I just want to convey the key idea and intuitive idea about why these LSTMs are effective at tracking long-term dependencies. The core is that the LSTM is able to control the flow of information through these gates to be able to more effectively filter out the unimportant things and store the important things. What you can do is implement LSTMs in TensorFlow just as you would in RNN. But the core concept that I want you to take away when thinking about the LSTM is this idea of controlled information flow through gates. Very briefly, the way that LSTM operates is by maintaining a cell state, just like a standard RNN, and that cell state is independent from what is directly outputted. The way the cell state is updated is according to these gates that control the flow of information, forgetting and eliminating what is irrelevant, storing the information that is relevant, updating the cell state in turn, and then filtering this updated cell state to produce the predicted output, just like the standard RNN. And again, we can train the LSTM using the back propagation through time algorithm, but the mathematics of how the LSTM is defined allows for a completely uninterrupted flow of the gradients, which completely eliminates the, well, largely eliminates the vanishing gradient problem that I introduced earlier. Again, we're not, if you're interested in learning more about the mathematics and the details of LSTMs, please come and discuss with us after the lectures, but again just emphasizing the core concept and the intuition behind how the LSTM operates. Okay, so, so far where we've been at, we've covered a lot of ground. We've gone through the fundamental workings of RNNs, the architecture, the training, the type of problems that they've been applied to, And I'd like to close this part by considering some concrete examples of how you're going to use RNNs in your software lab. And that is going to be in the task of music generation, where you're going to work to build an RNN that can predict the next musical note in a sequence and use it to generate brand new musical sequences that have never been realized before. So to give you an example of just the quality and type of output that you can try to aim towards, a few years ago there was a work that trained an RNN on a corpus of classical music data. And famously, there is this composer, Schubert, who wrote a famous unfinished symphony that consisted of two movements. But he was unable to finish his symphony before he died. So he died, and then he left the third movement unfinished. So a few years ago, a group trained a RNN-based model to actually try to generate the third movement to Schubert's famous Unfinished Symphony, given the prior two movements. So I'm going to play the result right now. \u266a Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, oh, there are any classical music aficionados out there, hopefully you get an appreciation for kind of the quality that was generated in terms of the music quality. And this was already from a few years ago, and as we'll see in the next lectures and continuing with this theme of generative AI, the power of these algorithms has advanced tremendously since we first played this example, particularly in, you know, a whole range of domains which I'm excited to talk about, but not for now. Okay, so you'll tackle this problem head-on in today's lab, RNN music generation. Also, right, we can think about the simple example of input sequence to a single output with sentiment classification, where we can think about the simple example of input sequence to a single output with sentiment classification, where we can think about, for example, text like tweets and assigning positive or negative labels to these text examples based on the contents that is learned by the network. Okay. So this concludes the portion on RNNs and I think it's quite remarkable that using all the foundational concepts and operations that we've talked about so far, we've been able to try to build up networks that handle this complex problem of sequential modeling. But like any technology, right, an RNN is not without limitations. So what are some of those limitations and what are some potential issues that can arise with using RNNs or even LSTMs? The first is this idea of encoding and dependency in terms of the temporal separation of data that we're trying to process. While RNNs require is that the sequential information is fed in and processed time-step by time-step. What that imposes is what we call an encoding bottleneck, where we're trying to encode a lot of content, for example, a very large body of text, many different words, into a single output, that may be just at the very last time step. How do we ensure that all that information leading up to that time step was properly maintained and encoded and learned by the network? In practice, this is very, very challenging and a lot of information can be lost. Another limitation is that by doing this time step by time step processing, RNNs can be quite slow. There is not really an easy way to parallelize that computation. And finally, together these components of the encoding bottleneck, the requirement to process this data step-by-step, imposes the biggest problem, which is when we talk about long memory, the capacity of the RNN and the LSTM is really not that long. We can't really handle data of tens of thousands or hundreds of thousands or even beyond sequential information that effectively to learn the complete amount of information and patterns that are present within such a rich data source. So because of this, very recently there's been a lot of attention in how we can move beyond this notion of step-by-step recurrent processing to build even more powerful architectures for processing sequential data. To understand how we do, how we can start to do this, let's take a big step back, right, think about the high-level goal of sequence modeling that I introduced at the very beginning. Given some input, a sequence of data, we want to build a feature encoding and use our neural network to learn that and then transform that feature encoding into a predicted output. What we saw is that RNNs use this notion of recurrence to maintain order information, processing information time step by time step. But as I just mentioned, we have these key three bottlenecks to RNNs. What we really want to achieve is to go beyond these bottlenecks and RNNs. What we really want to achieve is to go beyond these bottlenecks and achieve even higher capabilities in terms of the power of these models. Rather than having an encoding bottleneck, ideally we want to process information continuously as a continuous stream of information. Rather than being slow, we want to be able to parallelize computations to speed up processing. And finally, of course, our main goal is to really try to establish long memory that can build nuanced and rich understanding of sequential data. The limitation of RNNs that's linked to all these problems and issues in our inability to achieve these capabilities is that they require this time step by time step processing. So what if we could move beyond that? What if we could eliminate this need for recurrence entirely and not have to process the data time step by time step? Well a first and naive approach would be to just squash all the data, all the time steps together to create a vector that's effectively concatenated. The time steps are eliminated. There's just one stream where we have now one vector input with the data from all time points, that's then fed into the model. It calculates some feature vector and then generates some output which hopefully makes sense. And because we've squashed all these time steps together, we could simply think about maybe building a feed-forward network that could do this computation. Well, with that we'd eliminate the need for recurrence, but we still have the issues that it's not scalable because the dense feed-forward network would have to be immensely large, defined for recurrence, but we still have the issues that it's not scalable because the dense feed-forward network would have to be immensely large, defined by many, many different connections, and critically, we've completely lost our in- order information by just squashing everything together blindly. There's no temporal dependence and we're then stuck in our ability to try to establish long-term memory. So, what if instead we could still think about bringing these time steps together, but be a bit more clever about how we try to extract information from this input data. The key idea is this idea of being able to identify and attend to what is important in a potentially sequential stream of information. And this is the notion of attention or self-attention, which is an extremely, extremely powerful concept in modern deep learning and AI. I cannot understate or, I don't know, understate, overstate. I cannot emphasize enough how powerful this concept is. Attention is the foundational mechanism of the transformer architecture, which many of you may have heard about. The notion of a transformer can often be very daunting because sometimes they're presented with these really complex diagrams or deployed in complex applications and you may think, okay, how do I even start to make sense of this? At its core though, attention, the key operation, is a very intuitive idea and we're going to, in the last portion of this lecture, break it down step-by-step to see why it's so powerful and how we can use it as part of a larger neural network like a transformer. Specifically we're going to be talking and focusing on this idea of self-attention, attending to the most important parts of an input example. So let's consider an image. I think it's most intuitive to consider an image first. This is a picture of Iron Man, and if our goal is to try to extract information from this image of what's important, what we could do maybe is using our eyes naively scan over this image pixel by pixel, right, just going across the image. However, our brains, maybe, maybe internally they're doing some type of computation like this, but you and I, we can simply look at this image and be able to attend to the important parts. We can see that it's Iron Man coming at you right in the image, and then we can focus in a little further and say, okay, what are the details about Iron Man that may be important? What is key, what you're doing, is your brain is identifying which parts are attending to, to attend to, and then extracting those features that deserve the highest attention. The first part of this problem is really the most interesting and challenging one, and it's very similar to the concept of search. Effectively, that's what search is doing. Taking some larger body of information and trying to extract and identify the important parts. So let's go there next. How does search work? You're thinking, you're in this class, how can I learn more about neural networks? Well in this day and age, one thing you may do besides coming here and joining us is going to the Internet, having all the videos out there, trying to find something that matches, doing a search operation. So you have a giant database like YouTube, you want to find a video, you enter in your query, deep learning, and what comes out are some possible outputs, right? For every video in the database, there is going to be some key information related to that video, let's say the title. Now to do the search, what the task is to find the overlaps between your query and each of these titles, right, the keys in the database. What we want to compute is a metric of similarity and relevance between the query and these keys. How similar are they to our desired query? We can do this step-by-step. Let's say this first option of a video about the elegant giant sea turtles, not that similar to our query about deep learning. Our second option, introduction to deep learning, the first introductory lecture on this class, yes, highly relevant. The third option, a video about the late and great Kobe Bryant, not that relevant. The key operation here is that there is this similarity computation, bringing the query and the key together. The final step is now that we've identified what key is relevant, extracting the relevant information, what we want to pay attention to, and that's the video itself. We call this the value, and because the search is implemented well, we've successfully identified the relevant video on deep learning that you are going to want to pay attention to. It's this idea, this intuition of giving a query, trying to find similarity, trying to extract the related values that form the basis of self-attention and how it works in neural networks like transformers. So to go concretely into this, let's go back now to our text, our language example. With the sentence, our goal is to identify and attend to features in this input that are relevant to the semantic meaning of the sentence. Now, first step, we have sequence, we have order, we've eliminated recurrence. We're feeding in all the time steps all at once. We still need a way to encode and capture this information about order and this positional dependence. How this is done is this idea of positional encoding, which captures some inherent order information present in the sequence. I'm just going to touch on this very briefly, but the idea is related to this idea of embeddings which I introduced earlier. What is done is a neural network layer is used to encode positional information that captures the relative relationships in terms of order within this text. That's the high-level concept. We're still being able to process these time steps all at once. There is no notion of time step, rather. The data is singular. But still, we learn this encoding that captures the positional order information. Now, our next step is to take this encoding and figure out what to attend to, exactly like that search operation that I introduced with the YouTube example. Extracting a query, extracting a key, extracting a value, and relating them to each other. So we use neural network layers to do exactly this. Given this positional encoding, what attention does is applies a neural network layer, transforming that, first generating the query. We do this again using a separate neural network layer, and this is a different set of weights, a different set of parameters that then transform that positional embedding in a different way, generating a second output, the key. And finally, this operation is repeated with a third layer, a third set of weights, generating the value. Now, with these three in hand, the key, the query, the key, and the value, we can compare them to each other to try to figure out where in that self input the network should attend to what is important. That's the key idea behind this similarity metric or what you can think of as an attention score. What we're doing is we're computing a similarity score between a query and the key. Remember that these query and key values are just arrays of numbers. We can define them as arrays of numbers, which you can think of as vectors in space. The query values are some vector. The key values are some other vector. And mathematically, the way that we can compare these two vectors to understand how similar they are is by taking the dot product and scaling it. Captures how similar these vectors are, whether or not they're pointing in the same direction, right. This is the similarity metric and if you are familiar with a little bit of linear algebra, this is also known as the cosine similarity. The operation functions exactly the same way for matrices. If we apply this dot product operation to our query in key matrices, we get this similarity metric out. Now, this is very, very key in defining our next step, computing the attention weighting in terms of what the network should actually attend to within this input. This operation gives us a score which defines how the components of the input data are related to each other. So given a sentence, right, when we compute this similarity score metric, we can then begin to think of weights that define the relationship between the components of the sequential data to each other. So for example, in this example with a text sentence, he tossed the tennis ball to serve. The goal with the score is that words in the sequence that are related to each other should have high attention weights, ball related to toss, related to tennis. This metric itself is our attention weighting. What we have done is passed that similarity score through a softmax function, which all it does is it constrains those values to be between zero and one. So you can think of these as relative scores of relative attention weights. Finally, now that we have this metric that captures this notion of similarity and these internal self-relationships, we can finally use this metric to extract features that are deserving of high attention. And that's the exact final step in the self-attention mechanism. In that, we take that attention weighting matrix, multiply it by the value, and get a transformed transformation of the initial data as our output, which in turn reflects the features that correspond to high attention. All right, let's take a breath. Let's recap what we have just covered so far. The goal with this idea of self-attention, the backbone of transformers, is to eliminate recurrence, attend to the most important features in the input data. In an architecture, how this is actually deployed is first we take our input data, we compute these positional encodings, the neural network layers are applied threefold to transform the positional encoding into each of the key query and value matrices. We can then compute the self-attention weight score according to the dot product operation that we went through prior and then self-attend to this information to extract features that deserve high attention. What is so powerful about this approach in taking this attention weight, putting it together with the value to extract high attention features, is that this operation, this scheme that I'm showing on the right, defines a single self-attention head. And multiple of these self-attention heads can be linked together to form larger network architectures, where you can think about these different heads trying to extract different information, different relevant parts of the input, to now put together a very, very rich encoding and representation of the data that we're working with. Intuitively, back to our Iron Man example, what this idea of multiple self-attention heads can amount to is that different salient features and salient information in the data is extracted. First, maybe you consider Iron Man, attention head one, and you may have additional attention heads that are picking out other relevant parts of the data which maybe we did not realize before. For example, the building or the spaceship in the background that's chasing Iron Man. And so this is a key building block of many, many, many, many powerful architectures that are out there today. I again cannot emphasize how enough, how powerful this mechanism is. And indeed, this backbone idea of self-attention that you just built up understanding of is the key operation of some of the most powerful neural networks and deep learning models out there today, ranging from the very powerful language models like GPT-3, which are capable of synthesizing natural language in a very human-like fashion, digesting large bodies of text information to understand relationships in text, to models that are being deployed for extremely impactful applications in biology and medicine, such as AlphaFold2, which uses this notion of self-attention to look at data of protein sequences and be able to predict the three-dimensional structure of a protein just given sequence information alone. And all the way even now to computer vision, which will be the topic of our next lecture tomorrow, where the same idea of attention that was initially developed in sequential data applications has now transformed the field of computer vision, and again, using this key concept of attending to the important features in an input to build these very rich representations of complex high-dimensional data. Okay, so that concludes lectures for today. I know we have covered a lot of territory in a pretty short amount of time, but that is what this boot camp program is all about. So hopefully today you've gotten a sense of the foundations of neural networks in the lecture with Alexander. We talked about RNNs, how they're well suited for sequential data, how we can train them using back propagation, how we can deploy them for different applications, and finally how we can move beyond recurrence to build this idea of self-attention for building increasingly powerful models for deep learning in sequence modeling. All right, hopefully you enjoyed. We have about 45 minutes left for the lab portion and open office hours in which we welcome you to ask us questions of us and the TAs and to start work on the labs. The information for the labs is up there. Thank you so much for your attention. and the TAs and to start work on the labs. The information for the labs is up there. Thank you so much for your attention.", 3770, "2023-03-17 00:00:00"], ["MIT 6.S191: Introduction to Deep Learning", "MIT 6.S191: Convolutional Neural Networks", "https://www.youtube.com/watch?v=NmLK_WQBxB4", " Hi everyone, and welcome back to Intro to Deep Learning. We had a really awesome kickoff day yesterday, so we're looking to keep that same momentum all throughout the week and starting with today. And today we're really excited to be talking about actually one of my favorite topics in this course, which is how we can build computers that can achieve the sense of sight and vision. Now I believe that sight and specifically, like I said, vision is one of the most important human senses that we all have. In fact, sighted people rely on vision quite a lot in our day-to-day lives, from everything from walking around, navigating the world, interacting and sensing other emotions in our colleagues and peers. And today we're going to learn about how we can use deep learning and machine learning to build powerful vision systems that can both see and predict what is where by only looking at raw visual inputs. And I like to think of that phrase as a very concise and sweet definition of what it really means to achieve vision. But at its core, vision is actually so much more than just understanding what is where. It also goes much deeper. Take this scene, for example. We can build computer vision systems that can identify, of course, all of the objects in this environment, starting first with the yellow taxi or the van parked on the side of the road. But we also need to understand each of these objects at a much deeper level, not just where they are, but actually predicting the future, predicting what may happen in the scene next. For example, that the yellow taxi is more likely to be moving and dynamic into the future because it's in the middle of the lane compared to the white van which is parked on the side of the road. Even though you're just looking at a single image, your brain can infer all of these very subtle cues, and it goes all the way to the pedestrians on the road and even these even more subtle cues in the traffic lights and the rest of the scene as well. Now, accounting for all of these details in the scene is an extraordinary challenge, but we as humans do this so seamlessly. Within a split second, I probably put that frame up on the slide and all of you within a split second could reason about many of those subtle details without me even pointing them out. But the question of today's class is how we can build machine learning and deep learning algorithms that can achieve that same type and subtle understanding of our world. And deep learning in particular is really leading this revolution of computer vision and achieving sight of computers. For example, allowing robots to pick up on these key visual cues in their environment, critical for really navigating the world together with us as humans. These algorithms that you're going to learn about today have become so mainstreamed, in fact, that they're fitting on all of your smartphones and your pockets, processing every single image that you take, enhancing those images, detecting faces, and so on and so forth. And we're seeing some exciting advances ranging all the way from biology and medicine, which we'll talk about a bit later today, to autonomous driving and accessibility as well. And like I said, deep learning has taken this field as a whole by storm in the over the past decade or so because of its ability, critically like we were talking about yesterday, its ability to learn directly from raw data and those raw image inputs in what it sees in its environment. And learn explicitly how to perform, like we talked about yesterday, what is called feature extraction of those images in the environment. And one example of that is through facial detection and recognition, which all of you are going to get practice with in today's and tomorrow's labs as part of the grand final competition of this class. Another really go-to example of computer vision is in autonomous driving and self-driving vehicles, where we can take an image as input, or maybe potentially a video as input, multiple images, and process all of that data so that we can train a car to learn how to steer the wheel, or command a throttle, or actuate a braking command. This entire control system, the steering, the throttle, the braking of a car, can be executed end-to-end by taking as input the images and the throttle, the braking of a car can be executed end-to-end by taking as input the images and the sensing modalities of the vehicle and learning how to predict those actuation commands. Now actually this end-to-end approach, having a single neural network do all of this, is actually radically different than the vast majority of autonomous vehicle companies. Like if you look at Waymo, for example, that's a radically different approach, but we'll talk about those approaches in today's class. And in fact, this is one of our vehicles that we've been building at MIT, in my lab in CSAIL, just a few floors above this room. And we'll, again, share some of the details on this incredible work. But of course, it doesn't stop here with autonomous driving. These algorithms, directly the same algorithms that you'll learn about in today's class, can be extended all the way to impact healthcare, medical decision making, and finally, even in these accessibility applications, where we're seeing computer vision algorithms helping the visually impaired. So for example, in this project, researchers have built deep learning enabled devices that could detect trails so that visually impaired runners could be provided audible feedback so that they too could navigate when they go out for runs. And like I said, we often take many of these tasks that we're going to talk about in today's lecture for granted because we do them so seamlessly in our day-to-day lives. But the question of today's class is going to be at its core how we can build a computer to do these same types of incredible things that all of us take for granted day-to-day. And specifically we'll start with this question of how does a computer really see and even more detailed than that is how does a computer process an image. If we think of you know sight as coming to computers How does a computer really see? And even more detailed than that is how does a computer process an image? If we think of sight as coming to computers through images, then how can a computer even start to process those images? Well, to a computer, images are just numbers, right? And suppose, for example, we have a picture here of Abraham Lincoln, okay? This picture is made up of what are called pixels. Every pixel is just a dot in this image and since this is a grayscale image, each of these pixels is just a single number. Now, we can represent our image now as this two-dimensional matrix of numbers and because, like I said, this is a grayscale image, every pixel is corresponding to just one number added to matrix location. Now assume, for example, we didn't have a grayscale image, we had a color image, that would be an RGB image, right? So now every pixel is going to be composed not just of one number, but of three numbers. So you can think of that as kind of a 3D matrix instead of a 2D matrix, where you almost have three two-dimensional matrix that are stacked on top of each other. So now with this basis of basically numerical representations of images, we can start to think about how we can, or what types of computer vision algorithms we can build that can take these systems as input and what they can perform, right? So the first thing that I want to talk to you about is what kind of tasks do we even want to train these systems to complete with images? And broadly speaking, there are two broad categories of tasks. We touched on this a little bit in yesterday's lecture, but just to be a bit more concrete in today's lecture, those two tasks are either classification or regression. Now in regression, your prediction value is going to take a continuous value, right? That could be any real number on the number line, but in classification, your prediction could take one of, let's say, k or n different classes, right? These are discrete different classes. So let's consider first the task of image classification. In this task, we want to predict an individual label for every single image, and this label that we predict is going to be one of n different possible labels that could be considered. So for example, let's say we have a bunch of images of US presidents, and we want to build a classification pipeline to tell us which president is in this particular image that you see on the screen. Now, the goal of our model in this case is going to be basically to output a probability score, a probability of this image containing one of these different precedents, right, and the maximum score is going to be ultimately the one that we infer to be the correct precedent in the image. So in order to correctly perform this task and correctly classify these images, our pipeline, our computer vision model, needs the ability to be able to tell us what is unique about this particular image of Abraham Lincoln, for example, versus a different picture of George Washington versus a different picture of Obama, for example. Now another way to think about this whole problem of image classification or image processing at its high level is in terms of features or think of these as almost patterns in your data or characteristics of a particular class. And classification then is simply done by detecting all of these different patterns in your data and identifying when certain patterns occur over other patterns. So for example, if the features of a particular class are present in an image, then you might infer that that image is of that class. So for example, if you want to detect cars, you might look for patterns in your data like wheels, license plates, or headlights, and if those things are present in your image, then you can say with fairly high confidence that your image is of a car versus one of these other categories. So if we're building a computer vision pipeline, we have two main steps really to consider. The first step is that we need to know what features or what patterns we're looking for in our data. And the second step is we need to then detect those patterns. Once we detect them, we can then infer which class we're in. Now one way to solve this is to leverage knowledge about our particular field, right? So if we know something about our field, for example, about human faces, we can use that knowledge to define our features, right? What makes up a face? We know faces are made up of eyes, noses, and ears, for example. We can define what each of those components look like in defining our features. But there's a big problem with this approach, and remember that images are just these three-dimensional arrays of numbers, right? They can have a lot of variation even within the same type of object. These variations can include really anything ranging from occlusions to variations in lighting, rotations, translations, intra-class variation. And the problem here is that our classification pipeline needs the ability to handle and be invariant to all of these different types of variations while still being sensitive to all of the interclass variations, the variations that occur between different classes. Now, even though our pipeline could use features that we as humans, you know, define, manually define based on some of our prior knowledge, the problem really breaks down in that these features become very non-robust when considering all of these vast amount of different variations that images take in the real world. So in practice, like I said, your algorithms need to be able to withstand all of those different types of variations. And then the natural question is that how can we build a computer vision algorithm to do that and still maintain that level of robustness? What we want is a way to extract features that can both detect those features, right, those patterns in the data, and do so in a hierarchical fashion, right? So going all the way from the ground up, from the pixel level, to something with semantic meaning, like for example the eyes or the noses in a human face. Now we learned in the last class that we can use neural networks exactly for this type of problem, right? Neural networks are capable of learning features directly from data and learn, most importantly, a hierarchical set of features, building on top of previous features that it's learned to build more and more complex set of features. Now, we're going to see exactly how neural networks can do this in the image domain as part of this lecture, but specifically neural networks will allow us to learn these visual features from visual data if we construct them cleverly. And the key point here is that actually the models and the architectures that we learned about in yesterday's lecture and so far in this course, we'll see how they're actually not suitable or extensible to today's problem domain of images and how we can build and construct neural networks a bit more cleverly to overcome those issues. So maybe let's start by revisiting what we talked about in lecture one, which was where we learned about fully connected networks. Now, these were networks that have multiple hidden layers, and each neuron in a given hidden layer is connected to every neuron in its prior layer, right, so it receives all of the previous layers inputs as a function of these fully connected layers. Now let's say that we want to directly, without any modifications, use a fully connected network like we learned about in lecture one with an image processing pipeline. So directly taking an image and feeding it to a fully connected network. Could we do something like that? Actually, in this case we could. The way we would have to do it is, remember that because our image is a two-dimensional array, the first thing that we would have to do is collapse that to a one-dimensional sequence of numbers, right, because a fully connected network is not taking in a two-dimensional array, it's taking in a one-dimensional sequence. So the first thing that we have to do is flatten that two-dimensional array to a vector of pixel values and feed that to our network. In this case, every neuron in our first layer is connected to all neurons in that input layer, right? So in that original image, flattened down, we feed all of those pixels to the first layer. And here you should already appreciate the very important notion that every single piece of spatial information that really defined our image, that makes an image an image, is totally lost already before we've even started this problem because we've flattened that two-dimensional image into a one-dimensional array. We've completely destroyed all notion of spatial information. And in addition, we really have a enormous number of parameters because this system is fully connected. Take for example a very very small image which is even a hundred by a hundred pixels. That's an incredibly small image in today's standards, but that's going to take 10,000 neurons just in the first layer which will be connected to let's say 10,000 neurons in the second layer. The number of parameters that you'll have just in that one layer alone is going to be 10,000 squared parameters. It's going to be highly inefficient, you can imagine, if you wanted to scale this network to even a reasonably sized image that we have to deal with today. So not feasible in practice, but instead we need to ask ourselves how we can build and maintain some of that spatial structure that's very unique about images here into our input and here into our model, most importantly. So to do this, let's represent our 2D image as its original form, as a two-dimensional array of numbers. One way that we can use spatial structure here, inherent to our input, is to connect what are called basically these patches of our input to neurons in the hidden layer. So for example, let's say that each neuron in the hidden layer that you can see here only is going to see or respond to a certain set or a certain patch of neurons in the previous layer. Right, so you could also think of this as almost a receptive field, or what the single neuron in your next layer can attend to in the previous layer. It's not the entire image, but rather a small receptive field from your previous image. Now notice here how the region of the input layer, right, which you can see on the left-hand side here, influences that single neuron on the right-hand side, and that's just one neuron in the next layer. But of course, you can imagine basically defining these connections across the whole input, right? Each time you have the single patch on your input that corresponds to a single neuron output on the other layer. And we can apply the same principle of connecting these patches across the entire image to single neurons in the subsequent layer, and we do this by essentially sliding that patch pixel by pixel across the input image, and we'll be responding with, you know, another image on our output layer. In this way, we essentially preserve all of that very key and rich spatial information inherent to our input, but remember that the ultimate task here is not only to just preserve that spatial information, we want to ultimately learn features, learn those patterns so that we can detect and classify these images. And we can do this by weighting, right, weighting the connections between the patches of our input and in order to detect, you know, what those certain features are. Let me give a practical example here. And so in practice, this operation that I'm describing, this patching and sliding operation I'm describing, is actually a mathematical operation formerly known as convolution. We'll first think about this as a high level, supposing that we have what's called a 4x4 pixel patch. Right, so you can see this 4x4 pixel patch represented in red as a red box on the left-hand side. And let's suppose, for example, since we have a 4x4 patch, this is going to consist of 16 different weights in this layer. We're going to apply this same 4 by 4, let's call this not a patch anymore, let's use the terminology filter. We'll apply this same 4 by 4 filter in the input and use the result of that operation to define the state of the neuron in the next layer, right? And now we're going to shift our filter by, let's say, two pixels to the right, and that's going to define the next neuron in the adjacent location in the future layer, right? And we keep doing this, and you can see that on the right-hand side, you're sliding over not only the input image, but you're also sliding over the output neurons in the secondary layer. And this is how we can start to think about convolution at a very, very high level. But you're also sliding over the output neurons in the secondary layer. And this is how we can start to think about convolution at a very, very high level. But you're probably wondering, right, not just how the convolution operation works, but I think the main thing here to really narrow down on is how convolution allows us to learn these features, these patterns in the data that we were talking about, because ultimately that's our final goal, that's our real goal for this class is to extract those patterns. So let's make this very concrete by walking through maybe a concrete example, right. So suppose, for example, we want to build a convolutional algorithm to detect or classify an X in an image, right? This is the letter X in an image, and we here, for simplicity, let's just say we have only black and white images, right? So every pixel in this image will be represented by either a 0 or a 1. For simplicity, there's no grayscale in this image, right? And actually here, so we're representing black as negative 1 and white as positive 1. So to classify, we simply cannot, you know, compare the left-hand side to the right-hand side, right, because these are both X's, but you can see that because the one on the right-hand side is slightly rotated to some degree, it's not going to directly align with the X on the left-hand side, even though it is an X. We want to detect X's in both of these images, so we need to think about how we can detect those features that define an X a bit more cleverly. So let's see how we can use convolutions to do that. So in this case, for example, instead we want our model to compare images of this X piece by piece or patch by patch, right, and the important patches that we look for are exactly these features that will define our X. So if our model can find these rough feature patches roughly in the same positions in our input, then we can determine or we can infer that these two images are of the same type or the same letter, right? It can get a lot better than simply measuring the similarity between these two images because we're operating at the patch level. So think of each patch almost like a miniature image, a small two-dimensional array of values. And we can use filters to pick up on when these small patches or small images occur. So in the case of X's, these filters may represent semantic things, for example, the diagonal lines or the crossings that capture all of the important characteristics of the X. So we'll probably capture these features in the arms and the center of our letter, right, in any image of an X, regardless of how that image is, you know, translated or rotated or so on. And note that even in these smaller matrices, right, these are filters of weights, right, these are also just numerical values of each pixel in these many patches is simply just a numerical value. They're also images in some effect, right? And all that's really left in this problem, and in this idea that we're discussing, is to define that operation that can take these miniature patches and try to pick up, you know, detect when those patches occur in your image, and when they maybe don't occur. And that brings us right back to this notion of convolution, right? So convolution is exactly that operation that will solve that problem. Convolution preserves all of that spatial information in our input by learning image features in those smaller squares of regions that preserve our input data. So just to give another concrete example, to perform this operation, we need to do an element-wise multiplication between the filter matrix, those miniature patches, as well as the patch of our input image, right? So you have basically, think of two patches. You have the weight matrix patch, the thing that you want to detect, which you can see on the top left hand here, and you also have the secondary patch, which is the thing that you are looking to compare it against in your input image. And the question is how how similar are these two patches that you observe between them? So for example, this results in a 3x3 matrix because you're doing an element-wise multiplication between two small 3x3 matrices, you're going to be left with another 3x3 matrix. In this case, all of the matrix, all of the elements of this resulting matrix, you can see here, are 1s, right, because in every location in the filter and every location in the image patch, we are perfectly matching. So when we do that element-wise multiplication, we get 1s everywhere. The last step is that we need to sum up the results of that matrix, or that element-wise multiplication, and the result is, let's say, 9 in this case, right? Everything was a 1, it's a 3 by 3 matrix, so the result is 9. Now, let's consider one more example, right? Now we have this image in green and we want to detect this filter in yellow. Suppose we want to compute the convolution of this 5x5 image with this 3x3 filter. To do this we need to cover basically the entirety of our image by sliding over this filter piece by piece and comparing the similarity or the entirety of our image by sliding over this filter piece by piece and comparing the similarity or the convolution of this filter across the entire image. And we do that again through the same mechanism. At every location we compute an element-wise multiplication of that patch with that location on the image, add up all of the resulting entries, and pass that to our next layer. So let's walk through it. First, let's start off in the upper left-hand corner. We place our filter over the upper left-hand corner of our image. We element-wise multiply, we add up all the results, and we get four. And that four is going to be placed into the next layer, right? This next layer, again, is another image, right? But it's determined as the result of our convolution operation. We slide over that filter to the next location. The next location provides the next value in our image and we keep repeating this process over and over and over again until we've covered our filter over the entire image and as a result we've also completely filled out the result of our output feature map. The output feature map is basically what you can think of is how closely aligned our filter is to every location in our input image. So now that we've kind of gone through the mechanism that defines this operation of convolution, let's see how different filters could be used to detect different types of patterns in our data. So for example, let's take this picture of a woman's face and the output of applying three different types of filters to this picture, right? So you can see the exact filter. This is- they're all three by three filters, so the exact filters you can see on the bottom right-hand corner of the corresponding face. And by applying these by three filters, so the exact filters you can see on the bottom right-hand corner of the corresponding face. And by applying these three different filters, you can see how we can achieve drastically different results. And simply by changing the weights that are present in these three by three matrices, you can see the variability of different types of features that we can detect. So for example, we can design filters that can sharpen an image, make the edges sharper in the image. We can design filters that will extract edges. We can do stronger edge detection by again modifying the weights in all of those filters. So I hope now that all of you can kind of appreciate the power of, you know, number one is these filtering operations and how we can define them, you know, mathematically in the form of these smaller patch-based operations and matrices that we can then slide over an image. And these concepts are so powerful because, number one, they preserve the spatial information of our original input while still performing this feature extraction. Now, you can think of of instead of defining those filters like we said on the previous slide, what if we tried to learn them? And remember again that those filters are kind of proxies for important patterns in our data. So our neural network could try to learn those elements of those small patch filters as weights in the neural network. And learning those would essentially equate to picking up and learning the patterns that define one class versus another class. And now that we've gotten this operation and this understanding under our belt, we can take this one step further, right? We can take this singular convolution operation and start to think about how we can build entire layers, convolutional layers out of this operation so that we can start to think about how we can build entire layers, convolutional layers out of this operation so that we can start to even imagine convolutional networks and neural networks. And first we'll take a look at what are called, well, what you ultimately create by creating convolutional layers and convolutional networks is what's called a CNN, a convolutional neural network. And that's going to be the core architecture of today's class. So let's consider a very simple CNN that was designed for image classification. The task here, again, is to learn the features directly from the raw data and use these learned features for classification towards some task of object detection that we want to perform. Now, there are three main operations to a CNN, and we'll go through them step-by-step here, but then go deeper into each of them in the remainder, remainder of this class. So the first step is convolutions, which we've already seen a lot of in today's class already. Convolutions are used to generate these feature maps. So they take as input both the previous image, as well as some filter that they want to detect and they output a feature map of how this filter is related to the original image. The second step is, like yesterday, applying a non-linearity to the result of these feature maps that injects some nonlinear activations to our neural networks, allows it to deal with nonlinear data. Third step is pooling, which is essentially a downsampling operation to allow our images or allow our networks to deal with larger and larger scale images by progressively downscaling their size so that our filters can progressively grow in receptive field. And finally, feeding all of these resulting features to some neural network to infer the class scores. Now, by the time that we get to this fully connected layer, remember that we've already extracted our features, and essentially you can think of this no longer being a two-dimensional image. We can now use the methods that we learned about in lecture one to directly take those learned features that the neural network has detected and infer based on those learned features and based on what if they were detected or if they were not what class we're in. So now let's basically just go through each of these operations one by one in a bit more detail and see how we could even build up this very basic architecture of a CNN. So first let's go back and consider one more time the convolution operation that's central, a central core to the CNN. And as before, each neuron in this hidden layer is going to be computed as a weighted sum of its inputs, applying a bias, and activating with a non-linearity. Should sound very similar to lecture one in yesterday's class, but except now, when we're going to do that first step, instead of just doing a dot product with our weights, we're going to apply a convolution with our weights, which is simply that element-wise multiplication and addition, right, and that sliding operation. Now, what's really special here and what I really want to stress is the local connectivity. Every single neuron in this hidden layer only sees a certain patch of inputs in its previous layer. So if I point at just this one neuron in the output layer, this neuron only sees the inputs at this red square. It doesn't see any of the other inputs in the rest of the image. And that's really important to be able to scale these models to very large-scale images. Now you can imagine that as you go deeper and deeper into your network, eventually, because the next layer you're going to attend to a larger patch, right? And that will include data from not only this red square, but effectively a much larger red square that you could imagine there. Now let's define this actual computation that's going on. For a neuron in a hidden layer, its inputs are those neurons that fell within its patch in the previous layer. We can apply this matrix of weights here denoted as a 4x4 filter that you can see on the left-hand side, and in this case we do an element-wise multiplication, we add the outputs, we apply a bias, and we add that non-linearity, right? That's the core steps that we take in really all of these neural networks that you're learning about in today's and this week's class, to be honest. Now remember that this element-wise multiplication and addition operation, that sliding operation, that's called convolution and that's the basis of these layers. So that defines how neurons in convolutional layers are connected, how they're mathematically formulated. But within a single convolutional layer, it's also really important to understand that a single layer could actually try to detect multiple sets of filters, right? Maybe you want to detect in one image multiple features, not just one feature, but you know, if you were detecting faces, you don't only want to detect eyes, you want to detect, you know, eyes, noses, mouths, ears, right? All of those things are critical patterns that define a face and can help you classify a face. So what we need to think of is actually convolution operations that can output a volume of different images, right? Every slice of this volume effectively denotes a different filter that can output a volume of different images, right? Every slice of this volume effectively denotes a different filter that can be identified in our original input. And each of those filters is going to basically correspond to a specific pattern or feature in our image as well. Think of the connections in these neurons in terms of their receptive field, once again. The locations within the input of that node that they were connected to in the previous layer. These parameters really define what I like to think of as the spatial arrangement of information that propagates throughout the network and throughout the convolutional layers in particular. Now, I think just to summarize what we've seen and how connections in these types of neural networks are defined. And let's say how the output of a convolutional network is a volume. We are well on our way to really understanding convolutional neural networks and defining them, right? That's what we just covered is really the main component of CNNs, right? That's the convolutional operation that defines these convolutional layers. The remaining steps are very critical as well, but I want to maybe pause for a second and make sure that everyone's on the same page with the convolutional operation and the definition of convolutional layers. Awesome, okay. with the convolutional operation and the definition of convolutional layers. Awesome. Okay, so the next step here is to take those resulting feature maps that our convolutional layers extract and apply a non-linearity to the output volume of the convolutional layer. So as we discussed in the first lecture, applying these non-linearities is really critical because it allows us to deal with nonlinear data, and because image data in particular is extremely nonlinear, that's a, you know, a critical component of what makes convolutional neural networks actually operational in practice. In particular, for convolutional neural networks, the activation function that is really, really common for these models is the ReLU activation function. We talked a little bit about this in lecture one and two yesterday. The ReLU activation function, you can see it on the right-hand side, think of this function as a pixel-by-pixel operation that replaces basically all negative values with zero. It keeps all positive values the same. It's the identity function when a value is positive, but when it's negative it basically squashes everything back up to zero. Think of this almost as a thresholding function, right? Thresholds is everything at zero. Anything less than zero comes back up to zero. So negative values here indicate basically a negative detection in convolution that you may want to just say was no detection, right, and you can think of that as kind of an intuitive mechanism for understanding why the ReLU activation function is so popular in convolutional neural networks. The other common, the other popular belief is that ReLU activation functions, well it's not a belief, they are extremely easy to compute and they're very easy and computationally efficient. Their gradients are very cleanly defined, they're constants except for a piecewise non-linearity. So that makes them very popular for these domains. Now the next key operation in a CNN is that of pooling. Now, pooling is an operation that is, at its core, it serves one purpose, and that is to reduce the dimensionality of the image progressively as you go deeper and deeper through your convolutional layers. Now, you can really start to reason about this is that when you decrease the dimensionality of your features, you're effectively increasing the dimensionality of your filters, right? Now because every filter that you slide over a smaller image is capturing a larger receptive field that occurred previously in that network. So a very common technique for pooling is what's called maximum pooling or max pooling for short. Max pooling is exactly, you know, what it sounds like. So it basically operates with these small patches again that slide over an image, but instead of doing this convolution operation, what these patches will do is simply take the maximum of that patch location. So I think of this as kind of activating the maximum value that comes from that location and propagating only the maximums. I encourage all of you actually to think of maybe brainstorm other ways that we could perform even better pooling operations than max pooling. There are many common ways, but you could think of some, for example, are mean pooling or average pooling, right? Maybe you don't want to just take the maximum, you could collapse basically the average of all of these pixels into your single value in the result. But these are the key operations of convolutional neural networks at their core, and now we're ready to really start to put them together and form and construct a CNN all the way from the ground up, and with CNNs we can layer these operations one after the other, right? Starting first with convolutions, nonlinearities, and then pool up, and with CNNs, we can layer these operations one after the other, right, starting first with convolutions, non-linearities, and then pooling, and repeating these over and over again to learn these hierarchies of features. And that's exactly how we obtain pictures like this, which we started yesterday's lecture with, and learning these hierarchical decompositions of features by progressively stacking and stacking these filters on top of each other. Each filter could then use all of the previous filters that it had learned. So a CNN built for image classification can be really broken down into two parts. First is the feature learning pipeline, which we learn the features that we want to detect, and then the second part is actually detecting those features and doing the classification. Now the convolutional and pooling layers output from the first part of that model. The goal of those convolutional and pooling layers is to output the high-level features that are extracted from our input, but the next step is to actually use those features and detect their presence in order to classify the image. So we can feed these outputted features into the fully connected layers that we learned about in lecture one because these are now just a one-dimensional array of features and we can use those to detect, you know, what class we're in. And we can do this by using a function called a softmax function. You can think of a softmax function as simply a normalizing function whose output represents that of a categorical probability distribution. So another way to think of this is basically if you have an array of numbers you want to collapse, and those numbers could take any real number form, you want to collapse that into some probability distribution. Probability distribution has several properties, namely that all of its values have to sum to one. It always has to be between 0 and 1 as well. So maintaining those two properties is what a softmax operation does. You can see its equation right here. It effectively just makes everything positive and then it normalizes the result across each other and that maintains those two properties that I just mentioned. Great. So let's put all of this together and actually see how we could program our first convolutional neural network end-to-end entirely from scratch. So let's start by firstly defining our feature extraction head, which starts with a convolutional layer and here 32 filters or 32 features. You can imagine that this first layer, the result of this first layer, is to learn not one filter, not one pattern in our image, but 32 patterns. Okay, so those 32 results are going to then be passed to a pooling layer and then passed on to the next set of convolutional operations. The next set of convolutional operations now will contain 64 features. We'll keep progressively growing and expanding our set of patterns that we're identifying in this image. Next, we can finally flatten those resulting features that we've identified and feed all of this through our dense layers, our fully connected layers that we learned about in lecture one. These will allow us to predict those final, let's say, ten classes. If we have ten different final possible classes in our image, this layer will account for that and allow us to output using softmax the probability distribution across those ten classes. So, so far we've talked about, right, how we can, let's say, use CNNs to perform image classification tasks, but in reality, one thing I really want to stress in today's class, especially towards the end, is that this same architecture and same building blocks that we've talked about so far are extensible and they extend to so many different applications and model types that we can imagine. So for example, when we considered the CNN for classification, we saw that it really had two parts, right? The first part being feature extraction, learning what features to look for, and the second part being the classification, the detection of those features. Now what makes a convolutional neural network really, really powerful is exactly the observation that the feature learning part, this first part of the neural network, is extremely flexible. You can take that first part of the neural network, chop off what comes after it, and put a bunch of different heads into the part that comes after it. The goal of the first part is to extract those features. What you do with the features is entirely up to you, but you can still leverage the flexibility and the power of the first part to learn all of those core features. So for example, that portion will look for all of the different image classification domains, that future portion, after you've extracted the features. Or we could also introduce new architectures that take those features and maybe perform tasks like segmentation or image captioning like we saw in yesterday's lecture. So in the case of classification, for example, just to tie up the classification story, there's a significant impact in domains like health care, medical decision-making, where deep learning models are being applied to the analysis of medical scans across a whole host of different medical imagery. Now, classification tells us basically a discrete prediction of what our image contains, but we can actually go much deeper into this problem as well. So, for example, imagine that we're not trying to only identify that this image is an image of a taxi, which you can see here, but also more importantly, maybe we want our neural network to tell us not only that this is a taxi, but identify and draw a specific bounding box over this location of the taxi. So this is kind of a two-phase problem. Number one is that we need to draw a box, and number two is we need to classify what was in that box, right? So it's both a regression problem, where is the box, right? That's a continuous problem, as well as a classification problem is what is in that box. Now that's a much, much harder problem than what we've covered so far in the lecture today because potentially there are many objects in our scene, not just one object, right. So we have to account for this fact that maybe our scene could contain arbitrarily many objects. Now, our network needs to be flexible to that degree, right? It needs to be able to infer a dynamic number of objects in the scene, and if the scene is only of a taxi, then it should only output, you know, that one bounding box. But on the other hand, if the image has many objects, right, potentially even of different classes, we need a model that can draw a bounding box for each of these different examples, as well as associate their predicted classification labels to each one independently. Now, this is actually quite complicated in practice because those boxes can be anywhere in the image, right? There's no constraints on where the boxes can be, and they can also be of different sizes. They can be anywhere in the image. There's no constraints on where the boxes can be. And they can also be of different sizes. They can be also of different ratios. Some can be tall, some can be wide. Let's consider a very naive way of doing this first. Let's take our image and start by placing a random box somewhere on that image. For example, we just pick a random location, a random size, we'll place a box right there. This box, like I said, has a random location, a random size, we'll place a box right there. This box, like I said, has a random location, random size. Then we can take that box and only feed that random box through our convolutional neural network, which is trained to do classification, just classification. And this neural network can detect, well, number one, is there a class of object in that box or not? And if so, what class is it? And then what we could do is we could just keep repeating this process over and over again for all of these random boxes in our image. You know, many, many instances of random boxes. We keep sampling a new box, feed it through our convolutional neural network, and ask this question, what was in the box? If there was something in there, then what is it? Right, and we keep moving on until we kind of have exhausted all of the boxes in the image. But the problem here is that there are just way too many potential inputs that we would have to deal with. This would be totally impractical to run in a real-time system, for example, with today's compute. It results in way too many scales, especially for the types of resolutions of images that we deal with today. So instead of picking random boxes, let's try and use a very simple heuristic, right, to identify maybe some places with lots of variability in the image where there is high likelihood of having an object might be present, right? These might have meaningful insights or meaningful objects that could be available in our image. And we can use those to basically just feed in those high attention locations to our convolutional neural network. And then we can basically speed up that first part of the pipeline a lot because now we're not just picking random boxes. Maybe we use some simple heuristic to identify where interesting parts of the image might be. But still, this is actually very slow in practice. We have to feed in each region independently to the model. And plus, it's very brittle because ultimately, the part of the model that is looking at where potential objects might be is detached from the part that's doing the detection of those objects. Ideally, we want one part that's doing the detection of those objects. Ideally we want one model that is able to both, you know, figure out where to attend to and do that classification afterwards. So there have been many variants that have been proposed in this field of object detection, but I want to just for the purpose of today's class introduce you to one of the most popular ones. Now this is a point, or this is a model called R-CNN, or Faster R-CNN. Which actually attempts to learn not only how to classify these boxes, but learns how to propose where those boxes might be in the first place so that you could learn how to feed or where to feed into the downstream neural network. Now, this means that we can feed in the image to what are called these region proposal networks. The goal of these networks is to propose certain regions in the image that you should attend to and then feed just those regions into the downstream CNNs. So the goal here is to directly try to learn or extract all of those key regions and process them through the later part of the model. Each of these regions are processed with their own independent feature extractors and then a classifier can be used to aggregate them all and perform feature detection as well as object detection. Now the beautiful thing about this is that this requires only a single pass through the network, so it's extraordinarily fast. It can easily run in real time and is very commonly used in many industry applications as well. Even it can even run on your smartphone. So in classification, we just saw how we can predict, you know, not only a single image per, or sorry, a single object per image. We saw an object detection potentially inferring multiple objects with bounding boxes in your image. There's also one more type of task which I want to point out, which is called segmentation. Segmentation is the task of classification, but now done at every single pixel. This takes the idea of object detection, which bounding boxes, to the extreme. Now, instead of drawing boxes, we're not even going to consider boxes. We're going to learn how to classify every single pixel in this image in isolation, right? So it's a huge number of classifications that we're going to do and we'll do this, well first let me show this example. So on the left hand side what this looks like is you're feeding in an original RGB image. The goal of the right hand side is to learn for every pixel in the left hand side what was the class of that pixel, right? So this is kind of in contrast to just determining, you know, boxes over our image. Now we're looking at every pixel in isolation, and you can see, for example, you know, this pixels of the cow are clearly differentiated from the pixels of the sky or the pixels of the grass, right? And that's a key critical component of semantic segmentation networks. The output here is created by, again, using these convolutional operations, followed by pooling operations, which learn an encoder, which you can think of on the left-hand side. These are learning the features from our RGB image, learning how to put them into a space so that it can reconstruct into a new space of semantic labels. So you can imagine kind of a downscaling and then progressive upscaling into the semantic space, but when you do that upscaling, it's important, of course, you can't be pooling down that information. You need to kind of invert all of those operations. So instead of doing convolutions with pooling, you can now do convolutions with basically reverse pooling or expansions, right? You can grow your feature sets at every labels. And here's an example on the bottom of just a code piece that actually defines these layers. You can plug these layers, combine them with convolutional layers, and you can build these fully convolutional networks that can accomplish this type of task. Now, of course, this can be applied in many other applications in healthcare as well, especially for segmenting out, let's say, cancerous regions or even identifying parts of the blood which are infected with malaria, for example. And one final example here of self-driving cars, let's say that we want to build a neural network for autonomous navigation, specifically building a model, let's say, that can take as input an image, as well as, let's say, some very coarse maps of where it thinks it is. Think of this as basically a screenshot of the Google Maps, essentially, to the neural network, right? It's the GPS location of the map. And it wants to directly infer not a classification or a semantic classification of the map, and it wants to directly infer not a classification or a semantic classification of the scene, but now directly infer the actuation, how to drive and steer this car into the future, right? Now this is a full probability distribution over the entire space of control commands, right? It's a very large continuous probability space, and the question is how can we build a neural network to learn this function? And the key point that I'm stressing with all of these different types of architectures here is that all of these architectures use the exact same encoder. We haven't changed anything when going from classification to detection to semantic segmentation and now to here. All of them are using the same underlying building blocks of convolutions, nonlinearities, and pooling. The only difference is that after we perform those feature extractions, how do we take those features and learn our ultimate tasks? So for example, in the case of probabilistic control commands, we would want to take those learned features and understand how to predict, you know, the parameters of a full continuous probability distribution, like you can see on the right-hand side, as well as the deterministic control of our desired destination. And again, like we talked about at the very beginning of this class, this model, which goes directly from images all the way to steering wheel angles, essentially, of the car, is a single model. It's learned entirely end to end. We never told the car, for example, what a lane marker is, or the rules of the road. It was able to observe a lot of human driving data, extract these patterns, these features from what makes a good human driver different from a bad human driver, and learn how to imitate those same types of actions that are occurring so that without any human intervention or human rules that we impose on these systems, they can simply watch all of this data and learn how to drive entirely from scratch. So a human, for example, can actually enter the car, input a desired destination, and this end-to-end CNN will actually actuate the control commands to bring them to their destination. Now, I'll conclude today's lecture with just saying that the applications of CNNs, we've touched on a few of them today, but the applications of CNNs are enormous, right, far beyond these examples that I provided today. They all tie back to this core concept of feature extraction and detection, and after you do that feature extraction, you can really crop off the rest of your network and apply it to many different heads for many different tasks and applications that you might care about. We've touched on a few today, but there are really so, so many in different domains. And with that I'll conclude and very shortly we'll just be talking about generative modeling, which is a really central part of today's and this week's lectures series. And after that, later on, we'll have the software lab, which I'm excited for all of you to to start participating in. And yeah, we'll have the software lab, which I'm excited for all of you to start participating in. And yeah, we can take a short five minute break and continue the lectures from there. Thank you.", 3315, "2023-03-24 00:00:00"], ["MIT 6.S191: Introduction to Deep Learning", "MIT 6.S191: Deep Generative Modeling", "https://www.youtube.com/watch?v=3G5hWM6jqPk", " I'm really, really excited about this lecture because as Alexander introduced yesterday, right now we're in this tremendous age of generative AI, and today we're going to learn the foundations of deep generative modeling, where we're going to talk about building systems that can not only look for patterns in data, but can actually go a step beyond this to generate brand new data instances based on those learned patterns. This is an incredibly complex and powerful idea, and as I mentioned, it's a particular subset of deep learning that has actually really exploded in the past couple of years, and as I mentioned, it's a particular subset of deep learning that has actually really exploded in the past couple of years, and this year in particular. So to start, and to demonstrate how powerful these algorithms are, let me show you these three different faces. I want you to take a minute, think. Think about which face you think is real. Raise your hand if you think it's face A. Okay, I see a couple of people. Face B. Many more people. Face C. About second place. Well, the truth is that all of you are wrong. All three of these faces are fake. These people do not exist. These images were synthesized by deep generative models trained on data of human faces and asked to produce new instances. Now I think that this demonstration kind of demonstrates the power of these ideas and the power of this notion of generative modeling. So let's get a little more concrete about how we can formalize this. So far in this course, we've been looking at what we call problems of supervised learning, meaning that we're given data and associated with that data is a set of labels. Our goal is to learn a function that maps that data to the labels. Now we're in a course on deep learning so we've been concerned with functional mappings that are defined by deep neural networks, but really that function could be anything. Neural networks are powerful, but we could use other techniques as well. In contrast, there's another class of problems in machine learning that we refer to as unsupervised learning, where we take data, but now we're given only data, no labels, and our goal is to try to build some method that can understand the hidden underlying structure of that data. What this allows us to do is it gives us new insights into the foundational representation of the data, and as we'll see later, actually enables us to generate new data instances. Now this class of problems, this definition of unsupervised learning, captures the types of models that we're going to talk about today in the focus on generative modeling, which is an example of unsupervised learning and is united by this goal of the problem where we're given only samples from a training set, and we want to learn a model that represents the distribution of the data that the model is seeing. Generative modeling takes two general forms. First, density estimation, and second, sample generation. In density estimation, the task is, given some data examples, our goal is to train a model that learns a underlying probability distribution that describes where the data came from. With sample generation, the idea is similar, but the focus is more on actually generating new instances. Our goal with sample generation is to, again, learn this model of this underlying probability distribution, but then use that model to sample from it and generate new instances that are similar to the data that we've seen, approximately following along, ideally, that same real data distribution. Now in both these cases of density estimation and sample generation, the underlying question is the same. Our learning task is to try to build a model that learns this probability distribution that is as close as possible to the true data distribution. Okay. So with this definition and this concept of generative modeling, what are some ways that we can actually deploy generative modeling forward in the real world for high impact applications? Well, part of the reason that generative models are so powerful is that they have this ability to uncover the underlying features in a data set and encode it in an efficient way. So for example, if we're considering the problem of facial detection and we're given a data set with many, many different faces. Starting out without inspecting this data, we may not know what the distribution of faces in this data set is with respect to features we may be caring about. For example, the pose of the head, clothing, glasses, skin tone, hair, etc. And it can be the case that our training data may be very, very biased towards particular features without us even realizing this. Using generative models, we can actually identify the distributions of these underlying features in a completely automatic way, without any labeling, in order to understand what features may be over-represented in the data, what features may be under-represented in the data. And this is the focus of today and tomorrow's software labs, which are going to be part of the software lab competition, developing generative models that can do this task and using it to uncover and diagnose biases that can exist within facial detection models. Another really powerful example is in the case of outlier detection, identifying rare events. So let's consider the example of self-driving autonomous cars. With an autonomous car, let's say it's driving out in the real world, we really, really want to make sure that that car can be able to handle all the possible scenarios and all the possible cases it may encounter, including edge cases like a deer coming in front of the car or some unexpected rare events, not just, you know, the typical straight freeway driving that it may see the majority of the time. With generative models, we can use this idea of density estimation to be able to identify rare and anomalous events within the training data, and as they're occurring as the model sees them for the first time. So hopefully this paints a picture of what generative modeling the underlying concept is, and a couple of different ways in which we can actually deploy these ideas for powerful and impactful real-world applications. In today's lecture, we're going to focus on a broad class of generative models that we call latent variable models, and specifically distill down into two subtypes of latent variable models. First things first, I've introduced this term latent variable, but I haven't told you or described to you what that actually is. I think a great example, and one of my favorite examples throughout this entire course that gets at this idea of the latent variable is this little story from Plato's Republic, which is known as the myth of the cave. In this myth, there is a group of prisoners and as part of their punishment, they're constrained to face a wall. Now, the only things the prisoners can observe are shadows of objects that are passing in front of a fire that's behind them, and they're observing the casting of the shadows on the wall of this cave. To the prisoners, those shadows are the only things they see, their observations. They can measure them, they can give them names, because to them, that's their reality. But they're unable to directly see the underlying objects, the true factors themselves that are casting those shadows. Those objects here are like latent variables in machine learning. They're not directly observable, but they're the true underlying features or explanatory factors that create the observed differences and variables that we can see and observe. And this gets at the goal of generative modeling, which is to find ways that we can actually learn these hidden features, these underlying latent variables, even when we're only given observations of the observed data. So, let's start by discussing a very simple generative model that tries to do this through the idea of encoding the data input. The models we're going to talk about are called autoencoders, and to take a look at how an autoencoder works, we'll go through step by step, starting with the first step of taking some raw input data and passing it through a series of neural network layers. Now, the output of this, of this first step is what we refer to as a low-dimensional latent space. It's an encoded representation of those underlying features, and that's our goal in trying to train this model and predict those features. The reason a model like this is called an encoder or an autoencoder is that it's mapping the data, X, into this vector of latent variables, Z. Now, let's ask ourselves a question, let's pause for a moment. Why may we care about having this latent variable vector Z be in a low dimensional space? Anyone have any ideas? Anyone have any ideas? All right, maybe there are some ideas. Yes? The suggestion was that it's more efficient. Yes, that gets at it, the heart of the question. The idea of having that low dimensional latent space is that it's a very efficient compact encoding of the rich high-dimensional data that we may start with. As you pointed out, right, what this means is that we're able to compress data into this small feature representation, a vector, that captures this compactness and richness without requiring so much memory or so much storage. So, how do we actually train the network to learn this latent variable vector? Since we don't have training data, we can't explicitly observe these latent variables Z, we need to do something more clever. What the auto encoder does is it builds a way to decode this latent variable vector back up to the original data space, trying to reconstruct the original image from that compressed efficient latent encoding. And once again, we can use a series of neural network layers such as convolutional layers, fully connected layers, but now to map back from that lower dimensional space back upwards to the input space. This generates a reconstructed output which we can denote as x-hat, since it's an imperfect reconstruction of our original input data. To train this network, all we have to do is compare the outputted reconstruction and the original input data and say, how do we make these as similar as possible? We can minimize the distance between that input and our reconstructed output. So for example, for an image we can compare the pixel-wise difference between the input data and the reconstructed output, just subtracting the images from one another and squaring that difference to capture the pixel-wise divergence between the input and the reconstruction. What I hope you'll notice and appreciate is in that definition of the loss, it doesn't require any labels. The only components of that loss are the original input data X and the reconstructed output X hat. So I've simplified now this diagram by abstracting away those individual neural network layers in both the encoder and decoder components of this. And again, this idea of not requiring any labels gets back to the idea of unsupervised learning, since what we've done is we've been able to learn a encoded quantity, our latent variables, that we cannot observe without any explicit labels. All we started from was the raw data itself. It turns out that as, as the question and answer got at, that dimensionality of the latent space has a huge impact on the quality of the generated reconstructions and how compressed that information bottleneck is. Autoencoding is a form of compression and so the lower the dimensionality of the latent space, the less good our reconstructions are going to be, but the higher the dimensionality, the more- the less efficient that encoding is going to be. So to summarize this first part, this idea of an autoencoder is using this bottlenecked, compressed, hidden latent layer to try to bring the network down to learn a compact, efficient representation of the data. We don't require any labels, this is completely unsupervised, and so in this way we're able to automatically encode information within the data itself to learn this latent space, auto-encoding information, auto-encoding data. Now this is a pretty simple model, and it turns out that in practice this idea of self-encoding or auto-encoding has a bit of a twist on it to allow us to actually generate new examples that are not only reconstructions of the input data itself. And this leads us to the concept of variational auto-encers, or VAEs. With the traditional autoencoder that we just saw, if we pay closer attention to the latent layer, right, which is shown in that orange salmon color, that latent layer is just a normal layer in the neural network. It's completely deterministic. What that means is once we've trained the network, once the weights are set, any time we pass a given input in and go back through the latent layer, decode back out, we're going to get the same exact reconstruction. The weights aren't changing, it's deterministic. In contrast, variational autoencoders, VAEs, introduce a element of randomness, a probabilistic twist on this idea of autoencoding. What this will allow us to do is to actually generate new images similar to the- or new data instances that are similar to the input data, but not forced to be strict reconstructions. In practice, with the variational autoencoder, we've replaced that single deterministic layer with a random sampling operation. Now, instead of learning just the latent variables directly themselves, for each latent variable we define a mean and a standard deviation that captures a probability distribution over that latent variable, we define a mean and a standard deviation that captures a probability distribution over that latent variable. What we've done is we've gone from a single vector of latent variable Z to a vector of means mu and a vector of standard deviations sigma that parametrize the probability distributions around those latent variables. What this will allow us to do is now sample, using this element of randomness, this element of probability, to then obtain a probabilistic representation of the latent space itself. As you hopefully can tell, right, this is very, very, very similar to the autoencoder itself, but we've just added this probabilistic twist where we can sample in that intermediate space to get these samples of latent variables. Okay, now to get a little more into the depth of how this is actually learned, how this is actually trained, With defining the VAE, we've eliminated this deterministic nature to now have these encoders and decoders that are probabilistic. The encoder is computing a probability distribution of the latent variable Z given input data X, while the decoder is doing the inverse, trying to learn a probability distribution back in the input data space given the latent variables z. And we define separate sets of weights, phi and theta, to define the network weights for the encoder and decoder components of the VAE. Alright, so when we get now to how we actually optimize and learn the network weights in VAE, the first step is to define a loss function, right? That's the core element to training a neural network. Our loss is going to be a function of the data and a function of the data and a function of the neural network weights, just like before, but we have these two components, these two terms that define our VAE loss. First, we see the reconstruction loss, just like before, where the goal is to capture the difference between our input data and the reconstructed output, and now, for the VAE, we've introduced a second term to the loss, what we call the regularization term. Often you'll maybe even see this referred to as a VAE loss, and we'll go into- we'll go into describing what this regularization term means and what it's doing. To do that and to understand, remember and keep in mind that in all neural network operations, our goal is to try to optimize the network weights with respect to the data, with respect to minimizing this objective loss. And so here we're concerned with the network weights phi and theta that define the weights of the encoder and the decoder. We consider these two terms. First, the reconstruction loss. Again, the reconstruction loss is very, very similar, same as before. You can think of it as the error or the likelihood that effectively captures the difference between your input and your outputs. And again, we can trade in this in an unsupervised way, not requiring any labels, to force the latent space and the network to learn how to effectively reconstruct the input data. The second term, the regularization term, is now where things get a bit more interesting. So let's go on into this in a little bit more detail. Because we have this probability distribution, and we're trying to compute this encoding and then decode back up, as part of regularizing, we want to take that inference over the latent distribution and constrain it to behave nicely, if you will. The way we do that is we place what we call a prior on the latent distribution, and what this is is some initial hypothesis or guess about what that latent variable space may look like. This helps us and helps the network to enforce a latent space that roughly tries to follow this prior distribution. And this prior is denoted as P of Z, right? That term D, that's effectively the regularization term. It's capturing a distance between our encoding of the latent variables and our prior hypothesis about what the structure of that latent space should look like. So over the course of training, we're trying to enforce that each of those latent variables adopts a probability distribution that's similar to that prior. A common choice when training VAEs and developing these models is to enforce the latent variables to be roughly standard, normal Gaussian distributions, meaning that they are centered around mean zero and they have a standard deviation of 1. What this allows us to do is to encourage the encoder to put the latent variables roughly around a centered space, distributing the encoding smoothly so that we don't get too much divergence away from that smooth space, which can occur if the network tries to cheat and try to simply memorize the data. By placing the Gaussian standard normal prior on the latent space, we can define a concrete mathematical term that captures the distance, the divergence, between our encoded latent variables and this prior. And this is called the KL divergence. When our prior is a standard normal, the KL divergence takes the form of the equation that I'm showing up on the screen, but what I want you to really get away, come away with is that the concept of trying to smooth things out and to capture this divergence and this difference between the prior and the latent encoding is all this KL term is trying to capture. So it's a bit of math and I acknowledge that, but what I want to next go into is really what is the intuition behind this regularization operation. Why do we do this, and why does the normal prior in particular work effectively for VAEs? So let's consider what properties we want our latent space to adopt and for this regularization to achieve. The first is this goal of continuity. We don't- and what we mean by continuity is that if there are points in the latent space that are close together, ideally after decoding we should recover two reconstructions that are similar in content that make sense that they're close together. The second key property is this idea of completeness. We don't want there to be gaps in the latent space. We want to be able to decode and sample from the latent space in a way that is smooth and a way that is connected. To get more concrete, what, let's ask, what could be the consequences of not regularizing our latent space at all. Well, if we don't regularize, we can end up with instances where there are points that are close in the latent space but don't end up with similar decodings or similar reconstructions. Similarly, we could have points that don't lead to meaningful reconstructions at all. They're somehow encoded, but we can't decode effectively. Regularization allows us to realize points that end up close in the latent space and also are similarly reconstructed and meaningfully reconstructed. Okay, so continuing with this example, the example that I showed there and I didn't get into details was showing these shapes, these shapes of different colors, and that we're trying to be encoded in some lower dimensional space. With regularization, we are able to achieve this by trying to minimize that regularization term. It's not sufficient to just employ the reconstruction loss alone to achieve this continuity and this completeness. Because of the fact that without regularization, just encoding and reconstructing does not guarantee the properties of continuity and completeness. We overcome this, these issues of having potentially pointed distributions, having discontinuities, having disparate means that could end up in the latent space without the effect of regularization. We overcome this by now regularizing the mean and the variance of the encoded latent distributions according to that normal prior. What this allows is for the learned distributions of those latent variables to effectively overlap in the latent space, because everything is regularized to have, according to this prior of mean zero, standard deviation one, and that centers the means, regularizes the variances for each of those independent latent variable distributions. Together, the effect of this regularization in net is that we can achieve continuity and completeness in the latent space. Points and distances that are close should correspond to similar reconstructions that we get out. So hopefully this gets at some of the intuition behind the idea of the VAE, behind the idea of the regularization and trying to enforce the structured normal prior on the latent space. With this in hand, with the two components of our loss function, reconstructing the inputs, regularizing learning to try to achieve continuity and completeness, we can now think about how we define a forward pass through the network, going from an input example and being able to decode and sample from the latent variables to look at new examples. Our last critical step is how the actual backpropagation training algorithm is defined and how we achieve this. The key, as I introduced with VAEs, is this notion of randomness of sampling that we have introduced by defining these probability distributions over each of the latent variables. The problem this gives us is that we cannot back propagate directly through anything that has an element of sampling, anything that has an element of randomness. Backpropagation requires completely deterministic nodes, deterministic layers, to be able to successfully apply gradient descent and the backpropagation algorithm. The breakthrough idea that enabled VAEs to be trained completely end-to-end was this idea of re-parameterization within that sampling layer, and I'll give you the key idea about how this operation works. It's actually really quite clever. So, as I said, when we have a notion of randomness of probability, we can't sample directly through that layer. Instead, with reparameterization, what we do is we redefine how a latent variable vector is sampled as a sum of a fixed deterministic mean mu, a fixed vector of standard deviation sigma, and now the trick is that we divert all the randomness, all the sampling, to a random constant epsilon that's drawn from a normal distribution. So mean itself is fixed, standard deviation is fixed, all the randomness and the sampling occurs according to that epsilon constant. We can then scale the mean and standard deviation by that random constant to re-achieve the sampling operation within the latent variables themselves. What this actually looks like, and an illustration that breaks down this concept of reparameterization and divergence is as follows. So looking here, right, what I've shown is these completely deterministic steps in blue, and the sampling random steps in orange. Originally, if our latent variables are what effectively are capturing the randomness, the sampling themselves, we have this problem in that we can't back propagate, we can't train directly through anything that has stochasticity, that has randomness. What reparameterization allows us to do is it shifts this diagram, where now we've completely diverted that sampling operation off to the side to this constant epsilon, which is drawn from a normal prior, and now when we look back at our latent variable, it is deterministic with respect to that sampling operation. What this means is that we can back propagate to update our network weights completely end-to-end without having to worry about direct randomness, direct stochasticity within those latent variables Z. This trick is really, really powerful because it enabled the ability to train these VAEs completely end-to-end in a- in- through a back propagation algorithm. Alright, so at this point we've gone through the core architecture of VAEs, we've introduced these two terms of the loss, we've seen how we can train it end-to-end. Now let's consider what these latent variables are actually capturing and what they represent. When we impose this distributional prior, what it allows us to do is to sample effectively from the latent space and actually slowly perturb the value of single latent variables, keeping the other ones fixed, and what you can observe and what you can see here is that by doing that perturbation, that tuning of the value of the latent variables, we can run the decoder of the VAE every time, reconstruct the output every time we do that tuning, and what you'll see, hopefully with this example with the face, is that an individual latent variable is capturing something semantically informative, something meaningful, and we see that by this perturbation, by this tuning. In this example, the face, as you hopefully can appreciate, is shifting, the pose is shifting, and all this is driven by is the perturbation of a single latent variable, tuning the value of that latent variable and seeing how that affects the decoded reconstruction. The network is actually able to learn these different encoded features, these different latent variables, such that by perturbing the values of them individually, we can interpret and make sense of what those latent variables mean and what they represent. To make this more concrete, right, we can consider even multiple latent variables simultaneously, compare one against the other, and ideally we want those latent features to be as independent as possible in order to get at the most compact and richest representation and compact encoding. So here again in this example of faces, we're walking along two axes, head pose on the x-axis, and what appears to be kind of a notion of a smile on the y-axis. And you can see that with these reconstructions, we can actually perturb these features to be able to perturb the end effect in the reconstructed space. And so ultimately with a VAE, our goal is to try to enforce as much information to be captured in that encoding as possible. We want these latent features to be independent and ideally disentangled. It turns out that there is a very clever and simple way to try to encourage this independence and this disentanglement. While this may look a little complicated with with the math and and a bit scary, I will break this down with the idea of how a very simple concept enforces this independent latent encoding and this disentanglement. All this term is showing is those two components of the loss. The reconstruction term, the regularization term. That's what I want you to focus on. The idea of latent space disentanglement really arose with this concept of beta VAEs. What beta VAEs do is they introduce this parameter, beta, and what it is, it's a weighting constant. The weighting constant controls how powerful that regularization term is in the overall loss of the VAE, and it turns out that by increasing the value of beta, you can try to encourage greater disentanglement, more efficient encoding to enforce these latent variables to be uncorrelated with each other. Now, if you're interested in mathematically why beta VAEs enforce this disentanglement, there are many papers in the literature and proofs and discussions as to why this occurs, and we can point you in those directions. But to get a sense of what this actually affects downstream, when we look at face reconstruction as a task of interest, with the standard VAE, no beta term or rather a beta of one, you can hopefully appreciate that the features of the rotation of the head, the pose and the rotation of the head, is also actually ends up being correlated with smile and the facial- the mouth expression and the rotation of the head is also actually ends up being correlated with smile and the facial, the mouth expression and the mouth position, in that as the head pose is changing, the apparent smile or the position of the mouth is also changing. But with beta VAEs, empirically we can observe that with imposing these beta values much, much, much greater than one, we can try to enforce greater disentanglement, where now we can consider only a single latent variable, head pose, and the smile, the position of the mouth in these images, is more constant compared to the standard VAE. Alright, so this is really all the core math, the core operations, the core architecture of VAEs that we're going to cover in today's lecture and in this class in general. To close this section and as a final note, I want to remind you back to the motivating example that I introduced at the beginning of this lecture, facial detection, where now hopefully you've understood this concept of latent variable learning and encoding and how this may be useful for a task like facial detection, where we may want to learn those distributions of the underlying features in the data. And indeed you're going to get hands-on practice in the software labs to build variational autoencoders that can automatically uncover features underlying facial detection data sets and use this to actually understand underlying and hidden biases that may exist with those data and with those models. And it doesn't just stop there. Tomorrow we'll have a very, very exciting guest lecture on robust and trustworthy deep learning, which will take this concept a step further to realize how we can use this idea of generative models and latent variable learning to not only uncover and diagnose biases, but actually solve and mitigate some of those harmful effects of those biases in neural networks for facial detection and other applications. Alright, so to summarize quickly the key points of VAEs, we've gone through how they are able to compress data into this compact encoded representation. From this representation, we can generate reconstructions of the input in a completely unsupervised fashion. We can train them end-to-end using the reparameterization trick. We can understand the semantic interpretation of individual latent variables by perturbing their values, and finally we can sample from the latent space to generate new examples by passing back up through the decoder. So VAEs are looking at this idea of latent variable encoding and density estimation as their core problem. What if now we only focus on the quality of the generated samples and that's the task that we care more about. For that, we're going to transition to a new type of generative model called a generative adversarial network or GAN, where with GANs, our goal is really that we care more about how well we generate new instances that are similar to the existing data, meaning that we want to try to sample from a potentially very complex distribution that the model is trying to approximate. It can be extremely, extremely difficult to learn that distribution directly because it's complex, it's high-dimensional, and we want to be able to get around that complexity. What GANs do is they say, okay, what if we start from something super, super simple, as simple as it can get, completely random noise. Could we build a neural network architecture that can learn to generate synthetic examples from complete random noise? And this is the underlying concept of GANs, where the goal is to train this generator network that learns a transformation from noise to the training data distribution, with the goal of making the generated examples as close to the real deal as possible. With GANs, the breakthrough idea here was to interface these two neural networks together, one being a generator and one being a discriminator, and these two components, the generator and discriminator, are at war, at competition with each other. Specifically, the goal of the generator network is to look at random noise and try to produce an imitation of the data that's as close to real as possible. The discriminator then takes the output of the generator, as well as some real data examples, and tries to learn a classification decision distinguishing real from fake. And effectively, in the GAN, these two components are going back and forth, competing each other, trying to force the discriminator to better learn this distinction between real and fake, while the generator is trying to fool and outperform the ability of the discriminator to make that classification. So that's the overlying concept, but what I'm really excited about is the next example, which is one of my absolute favorite illustrations and walkthroughs in this class, and it gets at the intuition behind GANs, how they work, and the underlying concept. Okay, we're going to look at a 1D example, points on a line, right? That's the data that we're working with. In a GAN, the generator starts from random noise, produces some fake data, they're going to fall somewhere on this one-dimensional line. Now, the next step is the discriminator then sees these points, and it also sees some real data. The goal of the discriminator then sees these points, and it also sees some real data. The goal of the discriminator is to be trained to output a probability that a instance it sees is real or fake, and initially, in the beginning, before training, it's not trained, right? So its predictions may not be very good, but over the course of training you're going to train it, and it hopefully will start increasing the probability for those examples that are real, and decreasing the probability for those examples that are fake. Overall goal is to predict what is real, until eventually the discriminator reaches this point where it has a perfect separation, perfect classification of real versus fake. Okay, so at this point the discriminator thinks, okay I've done my job. Now we go back to the generator and it sees the examples of where the real data lie, and it can be forced to start moving its generated fake data closer and closer, increasingly closer, to the real data. We can then go back to the discriminator, which receives these newly synthesized examples from the generator, and repeats that same process of estimating the probability that any given point is real, and learning to increase the probability of the true real examples, decrease the probability of the fake points, adjusting, adjusting over the course of its training. And finally, we can go back and repeat to the generator again. One last time, the generator starts moving those fake points closer, closer, and closer to the real data, such that the fake data is almost following the distribution of the real data. At this point, it becomes very, very hard for the discriminator to distinguish between what is real and what is fake, while the generator will continue to try to create fake data points to fool the discriminator. This is really the key concept, the underlying intuition behind how the components of the GAN are essentially competing with each other, going back and forth between the generator and the discriminator. And in fact, this is the- this intuitive concept is how the GAN is trained in practice, where the generator first tries to synthesize new examples, synthetic examples, to fool the discriminator, and the goal of the discriminator is to take both the fake examples and the real data to try to identify the synthesized instances. In training, what this means is that the objective, the loss for the generator and discriminator, have to be at odds with each other. They're adversarial, and that is what gives rise to the component of adversarial in generative adversarial network. These adversarial objectives are then put together to then define what it means to arrive at a stable global optimum, where the generator is capable of producing the true data distribution that would completely fool the discriminator. Concretely, this can be defined mathematically in terms of a loss objective, and again, though I'm showing math, we can distill this down and go through what each of these terms reflect in terms of that core intuitive idea and conceptual idea that hopefully that 1D example conveyed. So we'll first consider the perspective of the discriminator D. Its goal is to maximize probability that its decisions, in its decisions that real data are classified real, fake data classified as fake. So here, the first term, g of z is the generator's output, and d of g of z is the discriminator's estimate of that generated output as being fake. d of x, x is the real data, and so D of X is the estimate of the probability that a real instance is fake. 1 minus D of X is the estimate that that real instance is real. So here, in both these cases, the discriminator is producing a decision about fake data, real data, and together it wants to try to maximize the probability that it's getting answers correct, right? Now, with the generator, we have those same exact terms, but keep in mind the generator is never able to affect anything that the discriminator's decision is actually doing besides generating new data examples. So for the generator, its objective is simply to minimize the probability that the generated data is identified as fake. Together, we want to then put this together to define what it means for the generator to synthesize fake images that hopefully fool the discriminator. All in all, right, this term, besides the math, besides the particularities of this definition, what I want you to get away from this section on GANs is that we have this dual competing objective where the generator is trying to synthesize these synthetic examples that ideally fool the best discriminator possible, and in doing so, the goal is to build up a network via this adversarial training, this adversarial competition, to use the generator to create new data that best mimics the true data distribution and is completely synthetic new instances. What this amounts to in practice is that after the training process, you can look exclusively at the generator component and use it to then create new data instances. All this is done by starting from random noise and trying to learn a model that goes from random noise to the real data distribution, and effectively what GANs are doing is learning a function that transforms that distribution of random noise to some target. What this mapping does is it allows us to take a particular observation of noise in that noise space and map it to some output take a particular observation of noise in that noise space and map it to some output, a particular output in our target data space. And in turn, if we consider some other random sample of noise, if we feed it through the generator of the GAN, it's going to produce a completely new instance, falling somewhere else on that true data distribution manifold. And indeed, what we can actually do is interpolate and traverse between trajectories in the noise space that then map to traversals and interpolations in the target data space. And this is really, really cool because now you can think about an initial point and a target point, and all the steps that are going to take you to synthesize and go between those images in that target data distribution. So hopefully this gets- gives a sense of this concept of generative modeling for the purpose of creating new data instances, and that notion of interpolation and data transformation leads very nicely to some of the recent advances and applications of GANs, where one particularly commonly employed idea is to try to iteratively grow the GAN to get more and more detailed image generations, progressively adding layers over the course of training to then refine the examples generated by the generator. And this is the approach that was used to generate those synthetic, those images of those synthetic faces that I showed at the beginning of this lecture. This idea of using a GAN that is refined iteratively to produce higher resolution images. Another way we can extend this concept is to extend the GAN architecture to consider particular tasks and impose further structure on the network itself. One particular idea is to say, okay, what if we have a particular label or some factor that we want to condition the generation on? We call this C, and it's supplied to both the generator and the discriminator. What this will allow us to achieve is paired translation between different types of data. So for example, we can have images of a street view and we can have images of the segmentation of that street view, and we can build a GAN that can directly translate between the street view and the segmentation. Let's make this more concrete by considering some particular examples. So what I just described was going from a segmentation label to a street scene. We can also translate between a satellite view, aerial satellite image, to what is the roadmap equivalent of that aerial satellite image. Or particular annotation or labels of the image of a building to the actual visual realization and visual facade of that building. We can translate between different lighting conditions, day to night, black and white to color, outlines to a colored photo. All these cases, and I think in particular the most interesting and impactful to me, is this translation between street view and aerial view. And this is used to consider, for example, if you have data from Google Maps, how you can go between a street view of the map to the aerial image of that. Finally, again, extending the same concept of translation between one domain to another, another idea is that of completely unpaired translation, and this uses a particular GAN architecture called CycleGAN. And so in this video that I'm showing here, the model takes as input a bunch of images in one domain, and it doesn't necessarily have to have a corresponding image in another target domain, but it is trained to try to generate examples in that target domain that roughly correspond to the source domain, transferring the style of the source onto the target and vice versa. So this example is showing the translation of images in horse domain to zebra domain. The concept here is this cyclic dependency, right? You have two GANs that are connected together via this cyclic loss, transforming between one domain and another. And really, like all the examples that we've seen so far in this lecture, the intuition is this idea of distribution transformation. Normally with a GAN, you're going from noise to some target. With the cycle GAN, you're trying to go from some source distribution, some data manifold X, to a target distribution, another data manifold Y. And this is really, really not only cool, but also powerful in thinking about how we can translate across these different distributions flexibly. And in fact, this is a- allows us to do transformations not only to images, but to speech and audio as well. So in the case of speech and audio, turns out that you can take sound waves, represent it compactly in a spectrogram image, and use a cycle GAN to then translate and transform speech from one person's voice in one domain to another person's voice in another domain, right? These are two independent data distributions that we define. Maybe you're getting a sense of where I'm hinting at, maybe not, but in fact this was exactly how we developed the model to synthesize the audio behind Obama's voice that we saw in yesterday's introductory lecture. What we did was we trained a CycleGAN to take data in Alexander's voice and transform it into data in the manifold of Obama's voice. So we can visualize how that spectrogram waveform looks like for Alexander's voice versus Obama's voice that was completely synthesized using this CycleGAN approach. Hi everybody, and welcome to MIT Fit Ask 191, the official introductory course on human response theory at MIT. Hi everybody. I replayed it. OK. But basically, what we did was Alexander spoke that exact phrase that was played yesterday. And we had the train cycle GAN model. And we can deploy it then on that exact audio to transform it from the domain of Alexander's voice to Obama's voice, generating the synthetic audio that was played for that video clip. All right. Okay, before I accidentally play it again, I jump now to the summary slide. So today in this lecture, we've learned deep generative models, specifically talking mostly about latent variable models, autoencoders, variational autoencoders, where our goal is to learn this low-dimensional latent encoding of the data, as well as generative adversarial networks, where we have these competing generator and discriminator components that are trying to synthesize synthetic examples. We've talked about these core foundational generative methods, but it turns out, as I alluded to in the beginning of the lecture, that in this past year in particular, we've seen truly, truly tremendous advances in generative modeling, many of which have not been from those two methods, those two foundational methods that we described, but rather a new approach called diffusion modeling. Diffusion models are driving- are the driving tools behind the tremendous advances in generative AI that we've seen in this past year in particular. These GANs, they're learning these transformations, these encodings, but they're largely restricted to generating examples that fall similar to the data space that they've seen before. Diffusion models have this ability to now hallucinate and envision and imagine completely new objects and instances which we as humans may not have seen or even thought about, right? Parts of the design space that are not covered by the training data. And so an example here is this AI-generated art, which art if you will, right, which was created by a diffusion model. And I think not only does this get at some of the limits and capabilities of these powerful models, but also questions about what does it mean to create new instances, what are the limits and bounds of these models, and how do they- how can we think about their advances with respect to human capabilities and human intelligence? And so I'm really excited that on Thursday in lecture 7 on new frontiers in deep learning, we're going to take a really deep dive into diffusion models, talk about their fundamentals, talk about not only applications to images, but other fields as well, in which we're seeing these models really start to make transformative advances, because they are indeed at the very cutting edge and very much the new frontier of generative AI today. Alright, so with that tease and hopefully set the stage for lecture 7 on Thursday, and conclude and remind you all that we have now about an hour for open office hour time for you to work on your software labs. Come to us, ask any questions you may have, as well as the TAs who will be here as well. Thank you so much.", 3592, "2023-03-31 00:00:00"], ["MIT 6.S191: Introduction to Deep Learning", "MIT 6.S191: Robust and Trustworthy Deep Learning", "https://www.youtube.com/watch?v=kIiO4VSrivU", " I'm really excited, especially for this lecture, which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course, Themis AI. And as you'll see today, Themis AI is a startup, actually locally based here in Cambridge. Our mission is to design, advance, and deploy the future of AI and trustworthy AI specifically. I'm especially excited about today's lecture because I co-founded Themis right here at MIT. Right here in this very building, in fact. This all stemmed from really the incredible scientific innovation and advances that we created right here, just a few floors higher than where you're sitting today. And because of our background in really cutting-edge scientific innovation stemming from MIT, Themis is very rooted deeply in science and, like I said, innovation. We really aim to advance the future of deep learning and AI. And much of our technology has already grown from published research that we've published at top-tier peer-review conferences in the AI venues around the world. And our work has been covered by high-profile international media outlets. This scientific innovation, with this scientific innovation, Themis, we are tackling some of the biggest challenges in safety critical AI that exist today and really that stems from the fact that we want to take all of these amazing advances that you're learning as part of this course and actually achieve them in reality as part of our daily lives. And we're working together with leading global industry partners across many different disciplines, ranging from robotics, autonomy, healthcare, and more, to develop a line of products that will guarantee safe and trustworthy AI. And we drive this really deeply with our technical engineering and machine learning team and our focus is very much on the engineering very flexible and very modular platforms to scale algorithms towards robust and trustworthy AI. This really enables this deployment towards grand challenges that our society faces with AI today, specifically the ability for AI solutions today are not very trustworthy at all, even if they may be very high performance on some of the tasks that we study as part of this course. So it's an incredibly exciting time for Themis in specific right now. We're VC backed, we're located, our offices are right here in Cambridge, so we're local, and we have just closed a round of funding. So we're actively hiring the best and the brightest engineers, like all of you, to realize the future of safe and trustworthy AI. And we hope that really today's lecture inspires you to join us on this mission to build the future of AI. And with that, it's my great pleasure to introduce Sadhana. Sadhana is a machine learning scientist at Themis. She's also the lead TA of this course, Intro to Deep Learning at MIT. Her research at Themis focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy AI. And today she'll be teaching us more about specifically the bias and the uncertainty realms of AI algorithms, which are really two key or critical components towards achieving this mission or this vision of safe and trustworthy deployment of AI all around us. So thank you and please give a big warm round of applause for Sadhana. you and please give a big warm round of applause for Sadhana. Thank you so much Alexander for the introduction. Hi everyone, I'm Sadhana. I'm a machine learning scientist here at Themis AI and the lead TA of the course this year and today I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of Themmis. So over the past decade, we've seen some tremendous growth in artificial intelligence across safety critical domains. In the spheres of autonomy and robotics, we now have models that can make critical decisions about things like self-driving at a second's notice, and these are paving the way for fully autonomous vehicles and robots. And that's not where this stops. In the spheres of medicine and healthcare, robots are now equipped to conduct life-saving surgery. We have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable. And we have models that can automatically diagnose diseases without intervention from any healthcare professionals at all. These advances are revolutionary and they have the potential to change life as we know it today. But there's another question that we need to ask, which is, where are these models in real life? A lot of these technologies were innovated five, 10 years ago, but you and I don't see them in our daily lives. So what's the gap here between innovation and deployment? The reason why you and I can't go buy self-driving cars or robots don't typically assist in operating rooms is this. These are some headlines about the failures of AI from the last few years alone. In addition to these incredible advances, we've also seen catastrophic failures in every single one of the safety critical domains I just mentioned. These problems range from crashing autonomous vehicles to health care algorithms that don't actually work for everyone, even though they're deployed out in the real world so everyone can use them. Now, at a first glance, this seems really demoralizing. If these are all of the things wrong with artificial intelligence, how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives in terms of safety critical deployment? But at Themis this is exactly the type of problem that we solve. We want to bring these advances to the real world and the way we do this is by innovating in the spheres of safe and trustworthy artificial intelligence in order to bring the things that were developed in research labs around the world to customers like you and me. And we do this by our core ideology is that we believe that all of the problems on this slide are underlaid by two key notions. The first is bias. Bias is what happens when machine learning models do better on some demographics than others. This results in things like facial detection systems not being able to detect certain faces with high accuracy, Siri not being able to recognize voices with accents, or algorithms that are trained on imbalanced data sets. So what the algorithm believes is a good solution doesn't actually work for everyone in the real world. And the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty. This is when models don't know when they can or can't be trusted. And this results in scenarios such as self-driving cars continuing to operate in environments when they're not 100% confident instead of giving control to users, or robots moving around in environments that they've never been in before and have high unfamiliarity with. And a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty. So today in this lecture we're going to focus on investigating the root causes of all of these problems, these two big challenges to robust deep learning. We'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone. And we'll start by talking about bias. Bias is a word that we've all heard outside the context of deep learning, but in the context of machine learning it can be quantified and mathematically defined. Today we'll talk about how to do this and methods for mitigation of this bias algorithmically and how Themis is innovating in these areas in order to bring new algorithms in this space to industries around the world. Afterwards we'll talk about uncertainty, which is can we teach a model when it does or doesn't know the answer to its given task, and we'll talk about the ramifications for this for real-world AI. So what exactly does bias mean, and where is it present in the artificial intelligence life cycle? The most intuitive form of bias comes from data. We have two main types of bias here. The first is sampling bias, which is when we oversample from some regions of our input data distribution and undersample from others. A good example of this is a lot of clinical data sets where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts. In addition, we also have selection bias at the data portion of the AI life cycle. Think about Apple's series voice recognition algorithm. This model is trained largely on flawless American English, but it's deployed across the real world to be able to recognize voices with accents from all over the world. The distribution of the model's training data doesn't match the distribution of The distribution of the models training data doesn't match the distribution of this type of, um, of language in the real world because American English is highly over-represented as opposed to other demographics. But that's not where uncertain- that's not where bias in data stops. These biases can be propagated towards models training cycles themselves, which is what we'll focus on in the second half of this lecture. Then once the model is actually deployed, which means it's actually put out into the real world and customers or users can actually get the predictions from it, we may see further biases perpetuated that we haven't seen before. The first of these is distribution shifts. Let's say I have a model that I trained on the past 20 years of data, and then I deploy it into the real world. In 2023, this model will probably do fine because the data input distribution is quite similar to data in the training distribution. But what would happen to this model in 2033? It probably would not work as well because the distribution that the data is coming from would shift significantly across this decade. And if we don't continue to update our models with this in- um, input stream of data, we're going to have obsolete and incorrect predictions. And finally, after deployment, there is the evaluation aspect. So think back to the Apple-Siri example that we've been talking about. Um, if the evaluation aspect. So think back to the Apple Siri example that we've been talking about. If the evaluation metric or the evaluation dataset that Siri was evaluated on, was also mostly comprised of American English, then to anybody, this model will look like it does extremely well. It can recognize American English voices with extremely high accuracy, and therefore is deployed into the real world. But what about its accuracy on subgroups, on accented voices, on people for whom English is not their first language? If we don't also test on subgroups in our evaluation metrics, we're going to face evaluation bias. So now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle. Commercial facial detection systems are everywhere. You actually played around with some of them in lab two, when you trained your VAE on a facial detection dataset. In addition to the lock screens on your cell phones, facial detection systems are also present in the automatic filters that your phone cameras apply whenever you try to take a picture, and they're also used in criminal investigations. These are three commercial facial detection systems that were deployed and we'll analyze the biases that might have been present in all of them in the next few minutes. So the first thing you may notice is that there is a huge accuracy gap between two different demographics in this, um, plot. This accuracy gap can get up to 34 percent. Keep in mind that this, uh, facial detection is a binary classification task. Everything is either a face or it's not a face. This means that a randomly initialized model would be expected to have an accuracy of 50% because it's going to randomly assign whether or not something is a face or not. Some of these facial detection classifiers do only barely better than random on these underrepresented data- on these underrepresented samples in this population. So how did this happen? Why is there such a blatant gap in accuracy between these different demographic groups, and how did these models ever get deployed in the first place? What types of biases were present in these models? So a lot of facial detection systems exhibit very clear selection bias. This model was likely trained mostly on lighter skin faces, and therefore learned those much more effectively than it learned to classify darker skin faces. But that's not the only bias that was present. The second bias that's often very present in facial detection systems is evaluation bias. Because originally, this dataset that you see on the screen is not the dataset that these models were evaluated on. They were evaluated on one big bulk dataset without any classification into subgroups at all. Therefore, you can imagine if the dataset was also comprised mostly of lighter skinned faces, these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence, and we can deploy them into the real world. In fact, the biases in these models were only uncovered once an independent study actually constructed a dataset that is specifically designed to uncover these sorts of biases by balancing across race and gender. However, there are other ways that data- datasets can be biased that we haven't yet talked about. So, so far we've assumed, um, a pretty key assumption in our dataset, which is that the number of faces in our data is the exact same as the number of non-faces in our data. But you can imagine, especially if you're looking at things like security feeds, this might not always be the case. You might be faced with many more negative samples than positive samples in your dataset. In the most- so what's the problem here? In the most extreme case, we may assign the label non-face to every item in the dataset, because the model sees items that are labeled as faces so infrequently that it isn't able to learn an accurate class boundary between the two classes. So how can we mitigate this? This is a really big problem and it's very common across a lot of different types of machine learning tasks and datasets. The first way that we can try to mitigate class imbalance is using sample reweighting, which is when instead of uniformly sampling from our dataset at a rate, we instead sample at a rate that is inversely proportional to the incidence of a class in our dataset. So in the previous example, if the number of faces was much lower than the number of non-faces in our dataset, we would sample the faces with a higher probability than the negatives so that the model sees both classes equally. The second example- the second way we can mitigate class imbalance is through loss re-weighting, which is when instead of having every single mistake that the model makes contribute equally to our total loss function, we re-weight the samples such that samples from underrepresented classes contribute more to the loss function. So instead of the model assigning every single input face as a negative, it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function than the loss of the negatives. And the final way that we can mitigate class imbalance is through batch selection, which is when we choose randomly from classes so that every single batch has an equal number of data points per class. So is everything solved? Like clearly there are other forms of bias that exist even when the classes are completely balanced because the thing that we haven't thought about yet is latent features. So if you remember from lab two and the last lecture, latent features is the actual representation- is the actual representation of this image according to the model. And so far, we've mitigated the problem of when we know that we have underrepresented classes, but we haven't mitigated the problem of when we have a lot of variability within the same class. Let's say we have an equal number of faces and negative examples in our data set. What happens if the majority of the faces are from a certain demographic or they have a certain set of features? Can we still apply the techniques that we just learned about? The answer is that we cannot do this, and the problem is that the bias present right now is in our latent features. All of these images are labeled with the exact same label. So according to the model, a, as the model, all we know is that they're all faces. So we have no information about any of these features, only from the label. Therefore, we can't apply any of the previous approaches that we used to mitigate class imbalance because our classes are balanced, but we have feature imbalance now. However, we can adapt the previous methods to account for bias in latent features which we'll do in just a few slides. So let's unpack this a little bit further. We have our potentially biased dataset and we're trying to build and deploy a model that classifies the faces in a traditional training pipeline. This is what that pipeline would look like. We would train our classifier and we would deploy it into the real world, but this training pipeline doesn't de-bias our inputs in any way. So one thing we could do is label our biased features and then apply resampling. So let's say in reality that this data set was biased on hair color. Most of the data set is made up of people with blonde hair, with faces with black hair and red hair underrepresented. If we knew this information, we could label the hair color of every single person in this data set, and we could apply either sample reweighting or loss reweighting just as we did previously. But does anyone want to tell me what the problem is here? You have to go through each samples, it takes a lot of time. Yeah. So there are a couple of problems here and that's definitely one of them. The first is, how do we know that hair color is a biased feature in this dataset? Unless we visually inspect every single sample in this dataset, we're not going to know what the biased features are. And the second thing is exactly what you said, which is once we have our biased features, going through and annotating every image with this feature is an extremely labor-intensive task that is infeasible in the real world. So now the question is, what if we had a way to automatically learn latent features and use this learned feature representation to de-bias a model. So what we want is a way to learn the features of this dataset and then automatically determine the samples with the highest feature bias and the samples with the lowest feature bias. We've already learned a method of doing this. In the generative modeling lecture, you all learned about variational autoencoders, which are models that learn the latent features of a dataset. As a recap, variational autoencoders work by probabilistically sampling from a learned latent space, and then they decode this new latent vector into back into the original input space, measure the reconstruction loss between the inputs and the outputs, and continue to update their representation of the latent space. And the reason why we care so much about this latent space is that we want samples that are similar to each other in the input to decode to latent vectors that are very close to each other in this latent space. And samples that are far from each other, or samples that are dissimilar to each other in the input, should decode to, should encode to latent vectors that are far from each other or samples that are dissimilar to each other in the input should decode to- should encode to latent vectors that are far from each other in the latent space. So now we'll walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under-sample and over-sample from regions in our data set. Before I start, I want to point out that this debiasing model is actually the foundation of Themis' work. This work comes out of a paper that we published a few years ago, that has been demonstrated to debias commercial facial detection algorithms, and it was so impactful that we decided to make it available and work with companies and industries, and that's how Themis was started. So let's first start by training a VAE on this data set. The Z shown here in this diagram ends up being our latent space and the latent space automatically captures features that were important for classification. So here's an example latent feature that this model captured. This is the facial position of an input face, and something that's really crucial here is that we never told the model to encode the feature vector of the facial position of a given face. It learned this automatically because this feature is important for the model to develop a good representation of what a face actually is. So now that we have our latent structure, we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on- that's based on the features of every item in this dataset. Essentially, what this means is that we can calculate the probability that a certain combination of features appears in our dataset based on the latent space that we just learned, and then we can oversample denser- or sparser areas of this dataset and undersample from denser areas of this dataset. So let's say our distribution looks something like this. This is an oversimplification, but for visualization purposes, and the denser portions of this dataset, we would expect to have, um, a homogenous skin color and pose and hair color and very good lighting. And then in the sparser portions of this dataset, we would expect to see diverse skin color, pose, and illumination. So now that we have this distribution and we know what areas of our distribution are dense and which areas are sparse, we want to under-sample areas from the under-sample samples that fall in the denser areas of this distribution and over-sample data points that fall in the sparser areas of this distribution. So, for example, we would probably undersample points with the very common skin colors, hair colors, and good lighting that is extremely present in this dataset, and oversample the diverse images that we saw on the last slide. This allows us to train in a fair and unbiased manner. To dig in a little bit more into the math behind how this resampling works, this approach basically approximates the latent space via a joint histogram over the individual latent variables. So we have a histogram for every latent variable z sub i and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily. Then we multiply the probabilities together across all of the latent distributions, and then after that we can develop an understanding of the joint distribution of all of the samples in our latent space. Based on this, we can define the adjusted probability for sampling for a particular data point as follows. The probability of selecting a sample data point X will be based on the latent space of X such that it is the inverse of the joint approximated distribution. We have a parameter alpha here which is a debiasing parameter and as alpha increases this probability will tend to the uniform distribution and as alpha decreases we tend to debias more strongly. And this gives us the final weight of the sample in our data set that we can calculate on the fly and use it to adaptively resample while training. And so once we apply this debiasing, we have pretty remarkable results. This is the original graph that shows the accuracy gap between the darker males and the lighter males in this dataset. Once we apply the debiasing algorithm, where as alpha gets smaller, we're debiasing more and more, as we just talked about, the- this accuracy gap decreases significantly. And that's because we tend to oversample samples with darker skin color, and therefore the model learns them better and tends to do better on them. Keep this algorithm in mind because you're going to need it for the Lab 3 competition, which I'll talk more about towards the end of this lecture. So, so far, we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias. However, bias is actually far more widespread in machine learning. Consider the example of autonomous driving. Many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility. And this is because the data for these cars, for these algorithms, is actually just collected by cars driving down roads. However, in some specific cases you're going to face adverse weather, bad visibility, near collision scenarios, and these are actually the samples that are the most important for the model to learn, because they're the hardest samples and they're the samples where the model is most likely to fail. But in a traditional autonomous driving pipeline, these samples are often extremely low, have extremely low representation. So this is an example where using the unsupervised latent de-biasing that we just talked about, we would be able to up-sample these important data points and under-sample the data points of driving down straight and sunny roads. Similarly, consider the example of large language models. An extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine, you're going to get roles such as artist or things in the humanities. But if you input similar things but of the male counterpart, you put things like male into the search engine, you'll end up with roles for scientists and engineers. So this type of bias also occurs regardless of the task at hand for a and engineers. So this type of bias also occurs regardless of the task at hand for a specific model. And finally, let's talk about health care recommendation algorithms. These recommendation algorithms tend to amplify racial biases. A paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care. And that's because of inherent bias in the data set of this model. And so in all of these examples, we can use the above algorithmic bias mitigation method to try and solve these problems and more. So we just went through how to mitigate some forms of bias in artificial intelligence and where these solutions may be applied. And we talked about a foundational algorithm that Themis uses that you all will also be developing today. And for the next part of the lecture, we'll focus on uncertainty or when a model does not know the answer. We'll talk about why uncertainty is important and how we can estimate it and also the applications of uncertainty estimation. So to start with, what is uncertainty and why is it necessary to compute? Let's look at the following example. This is a binary classifier that is trained on images of cats and dogs. For every single input, it will output a probability distribution over these two classes. Now, let's say I give this model an image of a horse. It's never seen a horse before. The horse is clearly neither a cat nor a dog. However, the model has no choice but to output a probability distribution because that's how this model is structured. However, what if in addition to this prediction we also achieved a confidence estimate? In this case, the model would should be able to say, I've never seen anything like this before and I have very low confidence in this prediction, so you as the user should not trust my prediction on this model. And that's the core idea behind uncertainty estimation. So in the real world, uncertainty estimation is useful in the real world, uncertainty estimation is useful for scenarios like this. This is an example of a Tesla car driving behind a horse-drawn buggy, which are very common in some parts of the United States. It has no idea what this horse-drawn buggy is. It first thinks it's a truck, and then a car, and then a person, and it continues to output predictions, even though it is very clear that the model does not know what this image is. And now you might be asking, okay, so what's the big deal? It didn't recognize the horse-drawn buggy, but it seems to drive successfully anyway. However, the exact same problem that resulted in that video has also resulted in numerous autonomous car crashes. So let's go through why something like this might have happened. There are multiple different types of uncertainty in neural networks, which may cause incidents like the ones that we just saw. We'll go through a simple example that illustrates the two main types of uncertainty that we'll focus on in this lecture. So let's say I'm trying to estimate the curve y equals x cubed as part of a regression task. The input here, x, is some real number and we want it to output f of x, which should be ideally x cubed. So right away you might notice that there are some issues in this data set. Assume the red points in this image are your training samples. So the boxed area of this image shows data points in our data set where we have really high noise. These points do not follow the curve y equals x cubed, in fact they don't really seem to follow any distribution at all, and the model won't be able to compute outputs for the points in this region accurately because very similar inputs have extremely different outputs, which is the definition of data uncertainty. We also have regions in this dataset where we have no data. So if we query the model for a prediction in this part of, in this region of the dataset, we should not really expect to see an accurate result, because the model's never seen anything like this before. This is what is called model uncertainty. When the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to output a correct prediction. So what would happen if I added the following blue training points to the areas of the dataset with high model uncertainty. Do you think the model uncertainty would decrease? Raise your hand. Okay. Does anyone think it would not change? Okay. So yeah, most of you were correct. Model uncertainty can typically be reduced by adding in data into any region, but specifically regions with high model uncertainty. And now, what happens if we add these blue data points into this data set? Would anyone expect the data uncertainty to decrease? You can raise your hand. That's correct. So data uncertainty is irreducible. In the real world, the blue points and the noisy red points on this image correspond to things like robot sensors. Let's say I have a robot that's trained to- that has a sensor that is making measurements of depth. If the sensor has noise in it, there's no way that I can add any more data into the system to reduce that noise unless I replaced my sensor entirely. So now let's assign some names to the types of uncertainty that we just talked about. The blue area or the area of high data uncertainty is known as aleatoric uncertainty. It is irreducible as we just mentioned, and it can be directly learned from data, which we'll talk about in a little bit. The green areas of the green boxes that we talked about, which were model uncertainty, are known as epistemic uncertainty. This cannot be learned directly from the data. However, it can be reduced by adding more data into our systems, into these regions. Okay, so first let's go through aleatoric uncertainty. So the goal of estimating aleatoric uncertainty is to learn a set of variances that correspond to the input. Keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance. We're training the model to do this task and so what that means is typically when we train a model we give it an input X and we expect an output Y hat which is the prediction of the model. Now we also predict an additional sigma squared so we add another layer to our model, with the same output size that predicts a variance for every output. So the reason why we do this is that we expect that areas in our dataset with high data uncertainty are going to have higher variance. The crucial thing to remember here is that this variance is not constant. It depends on the value of X. We typically tend to think of variance as a single number that parametrizes an entire distribution. However, in this case, we may have areas of our input distribution with really high variance, and we may have areas with very low variance. So, our variance cannot be independent of the input, and it depends on our input X. So now that we have this model, we have an extra layer attached to it in addition to predicting y-hat, we also predict a sigma squared. How do we train this model? Our current loss function does not take into account variance at any point. This is your typical mean squared error loss function that is used to train regression models. And there's no way training from this loss function that we can learn whether or not the variance that we're estimating is accurate. So in addition to adding another layer to estimate aleatoric uncertainty correctly, we also have to change our loss function. So the mean squared error actually learns a multivariate Gaussian with a mean yi and constant variance and we want to generalize this loss function to when we don't have constant variance. And the way we do this is by changing the loss function to the negative log likelihood. We can think about this for now as a generalization of the mean squared error loss to non-constant variances. So now that we have a sigma squared term in our loss function, we can determine how accurately the sigma and the y that we're predicting parameterize the distribution that is our input. So now that we know how to estimate aleatoric uncertainty, let's look at a real-world example. For this task, we'll focus on semantic segmentation, which is when we label every pixel of an image with its corresponding class. We do this for scene understanding and because it is more fine-grained than a typical object detection algorithm. So the inputs of this data- to this dataset are known as- it's from a dataset called Cityscapes and the inputs are RGB images of scenes. The labels are pixel-wise annotations of this entire image of which label every pixel belongs to. And the outputs try to mimic the labels, they're also predicted pixel-wise masks. So why would we expect that this dataset has high natural aleatoric uncertainty? Which parts of this dataset do you think would have aleatoric uncertainty? Because labeling every single pixel of an image is such a labor-intensive task and it's also very hard to do accurately, we would expect that the boundaries between, between objects in this image have high aleatoric uncertainty. That's exactly what we see. If you train a model to predict aleatoric uncertainty on this dataset, corners and boundaries have the highest aleatoric uncertainty, because even if your pixels are like one row off or one column off, that introduces noise into the model. The model can still learn in the face of this noise, but it does exist and it can't be reduced. So now that we know about data uncertainty or aleatoric uncertainty, let's move on to learning about epistemic uncertainty. As a recap, epistemic uncertainty can best be described as uncertainty in the model itself, and it is reducible by adding data to the model. So with epistemic uncertainty, essentially what we're trying to ask is, is the model unconfident about a prediction? So a really simple and very smart way to do this is, let's say I train the same network multiple times with random initializations, and I ask it to predict the exact- I, I call it on the same input. So let's say I give model one the exact same input, and the blue X is the output of this model. And then I do the same thing again with model 2, and then again with model 3, and again with model 4. These models all have the exact same hyperparameters, the exact same architecture, and they're trained in the same way. The only difference between them is that their weights are all randomly initialized, so where they start from is different. And The reason why we can use this to determine epistemic uncertainty is because we would expect that with familiar inputs in our network, our network should all converge to around the same answer, and we should see very little variance in the logits or the outputs that we're predicting. However, if a model has never seen a specific input before, or that input is very hard to learn, all of these models should predict slightly different answers, and the variance of them should be higher than if they were predicting a similar input. So creating an ensemble of networks is quite simil- is quite simple. You, um, start out with defining the number of ensembles you want, you create them all the exact same way, and then you fit them all on the same training data and, uh, training data. And then afterwards, when at inference time, we call ever- all of the models, every model in the ensemble on our specific input, and then we can treat, um, our new prediction as the average of all of the ensembles. This results in a usually more robust and accurate prediction and we can treat the uncertainty as the variance of all of these predictions. Again, remember that if we saw familiar inputs or inputs with low epistemic uncertainty, we should expect to have very little variance and if we had a very unfamiliar input or something that was out of distribution or something the model hasn't seen before, we should have very high epistemic uncertainty or variance. So, what's the problem with this? Can anyone raise their hand and tell me what a problem with training an ensemble of networks is? So, training an ensemble of networks is really compute expensive. Even if your model is not very large, training five copies of it or 10 copies of it, tends to- it takes up compute and time, and that's just not really feasible when we're training, um, if- on specific tasks. However, the key insight for ensembles is that by introducing some method of randomness or stochasticity into our networks, we're able to estimate epistemic uncertainty. So another way that we've seen about introducing stochasticity into networks is by using dropout layers. We've seen dropout layers as a method of reducing overfitting, because we randomly drop out different, um, nodes in our, in our layer, and then we continue to propagate information through them, and it prevents models from memorizing data. However, in the, um, case of epistemic uncertainty, we can add dropout layers after every single layer in our model, and in addition, we can keep these dropout layers enabled at test time. Usually we don't keep dropout layers enabled at test time because we don't want to lose any information about the network's process or any weights when we're at inference time. However, when we're estimating epistemic uncertainty we do want to keep dropout enabled at test time because that's how we can introduce randomness at inference time as well. So what we do here is we have one model, it's the same model the entire way through, we add dropout layers with a specific probability and then we run multiple forward passes and at every forward pass, different layers get dropped, different nodes in a layer get dropped out. So we have that measure of randomness and stochasticity. So again, in order to co- implement this, what we have is a model with the exact- the one model, and then when we're running our forward passes, we can simply run T forward passes, where T is usually a number like 20. We keep dropout enabled at test time, and then we use the mean of these samples as the new prediction and the variance of these samples as a measure of epistemic uncertainty. So both of the methods we talked about just now involves sampling, and sampling is expensive. Ensembling is very expensive, but even if you have a pretty large model, having or introducing dropout layers and calling 20 forward passes might also be something that's pretty infeasible. And at Themis, we're dedicated to developing innovative methods of estimating epistemic uncertainty that don't rely on things like sampling, so that they're more generalizable and they're usable by more industries and people. So a method that we've developed to estimate or a method that we've studied to estimate epistemic uncertainty is by using generative modeling. So we've talked about VAEs a couple times now but let's say I trained a VAE on the exact same data set we were talking about earlier which is only dogs and cats. The latent space of this model would be comprised of features that relate to dogs and cats, and if I give it a prototypical dog, it should be able to generate a pretty good representation of this dog, and it should have pretty low reconstruction loss. Now, if I gave the same example of the horse to this VAE, the latent vector that this horse would be decoded to, would be incomprehensible to the decoder of this network. The decoder wouldn't be able to know how to project the latent vector back into the original input space. Therefore, we should expect to see a much worse reconstruction here, and we should see that the reconstruction loss is much higher than if we gave the model a familiar input or something that it was used to seeing. So now let's move on to what I think is the most exciting method of estimating epistemic uncertainty that we'll talk about today. So in both of the examples before, sampling is compute intensive, but generative modeling can also be compute intensive. Let's say you don't actually need a variational autoencoder for your task, then you're training an entire decoder for no reason and other than to estimate the epistemic uncertainty. So what if we had a method that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty. That's exactly what a method that we've developed here at Themis does. So we view learning as an evidence-based process. So if you remember from earlier when we were training the ensemble and calling multiple ensembles on the same input, we received multiple predictions and we calculated that variance. Now, the way we frame evidential learning is what if we assume that those data points, those predictions, were actually drawn from a distribution themselves? If we could estimate the parameters of this higher-order evidential distribution, we would be able to learn this variance or this measure of epistemic uncertainty automatically without doing any sampling or generative modeling, and that's exactly what evidential uncertainty does. So now that we have many methods in our toolbox for epist- for estimating epistemic uncertainty, let's go back to our real-world example. Let's say the- again, the input is the same as before. It's a RGB image of some scene in a city, and the output again is a pixel level mask of what every pixel in this image belongs to, which class it belongs to. Which parts of the dataset would you expect to have high epistemic uncertainty? In this example, take a look at the output of the model itself. The model does mostly well on semantic segmentation. However, it gets the sidewalk wrong. It assigns some of the sidewalk to the road, and other parts of the sidewalk are labeled incorrectly. Using epistemic uncertainty, we can see why this is. The areas of the sidewalk that are discolored, have high levels of epistemic uncertainty. Maybe this is because the model has never seen an example of the sidewalk that are discolored have high levels of epistemic uncertainty. Maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before, or maybe it hasn't been trained on examples with sidewalks generally. Either way, epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty. an area of high uncertainty. So today we've gone through two major challenges for robust deep learning. We've talked about bias, which is what happens when models are skewed by sensitive feature inputs, and uncertainty, which is when we can measure a level of confidence of a certain model. Now we'll talk about how Themis uses these concepts to build products that transform models to make them more risk-aware, and how we're changing the AI landscape in terms of safe and trustworthy AI. So at Themis, we believe that uncertainty and bias mitigation unlock a host of new solutions to solving these problems with safe and responsible AI. We can use bias and uncertainty to mitigate risk in every part of the AI lifecycle. Let's start with labeling data. Today, we talked about aleatoric uncertainty, which is a method to detect mislabeled samples, to highlight label noise, and to generally maybe tell labelers to relabel images or samples that they've gotten that may be wrong. In the second part of this cycle, we have analyzing the data. Before a model is even trained on any data, we can analyze the bias that is present in this dataset, and tell the creators whether or not they should add more samples, which demographics, which areas of the dataset are underrepresented in the current dataset, before we even train a model on them. Then let's go to training the model. Once we're actually training a model, if it's already been trained on a biased dataset, we can de-bias it adaptively during training, using the methods that we talked about today. Afterwards, we can also verify or certify deployed machine learning models, making sure that models that are actually out there are as safe and unbiased as they claim they are. The way we can do this is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do the worst on. The model has the most trouble learning or data set samples that are the most underrepresented in a model's data set. If we can test the model on these samples specifically, the hardest samples for the model, and it does well, then we know that the model has probably been trained in a fair and unbiased manner that mitigates uncertainty. And lastly, we can think about- we are developing a product at Themis called AI Guardian, and that's essentially a layer between the artificial intelligence algorithm and the user. And the way this works is, this is the type of algorithm that if you were driving an autonomous vehicle would say, hey, the model doesn't actually know what is happening in the world around it right now. As the user, you should take control of this autonomous vehicle. And we can apply this to spheres outside autonomy as well. So you'll notice that I skipped one part of the cycle. I skipped the part about building the model, and that's because today we're going to focus a little bit on Themis AI's product called CAPSA, which is a model agnostic framework for risk estimation. So CAPSA is an an open source library, you all will actually use it in your lab today, that transforms models so that they are risk-aware. So this is a typical training pipeline, you've seen this many times in the course by now. We have our data, we have the model, and it's fed into the training algorithm, and we get a trained model at the end that outputs a prediction for every input. But with CAPSA, what we can do is by adding a single line into any training workflow, we can turn this model into a risk-aware variant that essentially calculates biases, uncertainty, and label noise for you. Because today, as you've heard by now, there are so many methods of estimating uncertainty and bias, and sometimes certain methods are better than others. It's really hard to determine what kind of uncertainty you're trying to estimate and how to do so. So CAPSA takes care of this for you by inserting one line into your training workflow, you can achieve a risk-aware model that you can then further analyze. So this is the one line that I've been talking about. After you build your model, you can just create a wrapper or you can call a wrapper that Capsa has an extensive library of, and then in addition to achieving prediction or receiving predictions from your model, you can also receive whatever bias or uncertainty metric that you're trying to estimate. And the way Capsa works is it does this by wrapping models. For every uncertainty metric that we want to estimate, we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities. In the case of aleatoric uncertainty, this could be adding a new layer. In the case of a variational autoencoder, this could be creating and training the decoder and calculating the reconstruction loss on the fly. And this is an example of CAPSA working on one of the datasets that we talked about today, which was the cubic dataset with an added noise in it, and also another simple classification task. And the reason why I wanted to show this image is to show that using CAPSA we can achieve all of these uncertainty estimates with very little additional added work. So using all of the products that I just talked about today and using CAPSA, Themis is unlocking the key to deploy deep learning models safely across fields. We can now answer a lot of the questions that the headlines were raising earlier, which is when should a human take control of an autonomous vehicle? What types of data are underrepresented in commercial autonomous driving pipelines? We now have educated answers to these questions due to products that Themis is developing. And in spheres such as medicine and healthcare, we can now answer questions such as, when is a model uncertain about a life-threatening diagnosis? When should this diagnosis be passed to a medical professional before this information is conveyed to a patient? Or, what types of patients might drug discovery algorithms be biased against? And today, the application that you guys will focus on is on facial detection. You'll use CAPSA in today's lab to thoroughly analyze a common facial detection data set that we've perturbed in some ways for you so that you can discover them on your own. And we highly encourage you to compete in the competition, which the details are described in the lab. But basically, it's about analyzing this data set, creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline. And so at Themis, our goal is to design, advance, and deploy trustworthy AI across industries and around the world. We're passionate about scientific innovation, we release open-source tools like the ones you'll use today, and our products transform AI workflows and make artificial intelligence safer for everyone. We partner with industries around the globe and we're hiring for the upcoming summer and for full-time roles, so if you're interested please send an email to careers at themisai.io or apply by submitting your resume to the deep learning resume drop and we'll see those resumes and get back to you. Thank you. by submitting your resume to the deep learning resume drop and we'll see those resumes and get back to you. Thank you.", 3229, "2023-04-07 00:00:00"], ["MIT 6.S191: Introduction to Deep Learning", "MIT 6.S191: Reinforcement Learning", "https://www.youtube.com/watch?v=AhyznRSDjw8", " Hi everyone, welcome back. Today, I think that these two lectures today are really exciting because they start to move beyond a lot of what we've talked about in the class so far, which is focusing a lot on really static datasets. And specifically in this lecture right now, I'm going to start to talk about how we can learn about this very long standing field of how we can specifically marry two topics. The first topic being reinforcement learning, which has existed for many, many decades, together with a lot of the very recent advances in deep learning, which you've already started learning about as part of this course. Now this marriage of these two fields is actually really fascinating to me, particularly because, like I said, it moves away from this whole paradigm of, or really this whole paradigm that we've been exposed to in the class thus far. And that paradigm is really how we can build a deep learning model using some data set, but that data set is typically fixed in our world. We collect, we go out and collect that data set, we deploy it on our machine learning or deep learning algorithm, and then we can evaluate on a brand new data set. But that is very different than the way things work in the real world. In the real world, you have your deep learning model actually deployed together with the data, together out into reality, exploring, interacting with its environment, and trying out a whole bunch of different actions and different things in that environment in order to be able to learn how to best perform any particular task that it may need to accomplish. And typically, we want to be able to do this without explicit human supervision, right? This is the key motivation of reinforcement learning. You're going to try and learn through reinforcement, making mistakes in your world and then collecting data on those mistakes to learn how to improve. Now, this is obviously a huge field in or a huge topic in the field of robotics and autonomy. You can think of self-driving cars and robot manipulation, but also, very recently, we've started seeing incredible advances of deep reinforcement learning, specifically also on the side of gameplay and strategy making as well. So, one really cool thing is that now you can even imagine this combination of robotics together with gameplay, right? Now training robots to play against us in the real world. And I'll just play this very short video on StarCraft and DeepMind. Perfect information and is played in real time. It also requires long-term planning and the ability to choose what action to take from millions and millions of possibilities. I'm hoping for a 5-0, not to lose any games, but I think the realistic goal would be 4-1 in my favor. I think he looks more confident than TLO. TLO was quite nervous before. The room was much more tense this time. I think he looks more confident than TLO. TLO was quite nervous before. The room was much more tense this time. I really didn't know what to expect. He's been playing StarCraft pretty much since he's five. I wasn't expecting AI to be that good. Everything that he did was proper. It was calculated and it was done well. I thought I'm learning something. Yes! It's much better than I expected it to be. I would consider myself a good player, right? But I lost every single one of five games. We're way ahead of one. Right. So let's take maybe a start and take a step back, first of all, and think about how reinforcement learning fits into this whole paradigm of all of the different topics that you've been exposed to in this class so far. So as a whole, I think that we've really covered two different types of learning in this course to date, right? Up until now, we've really started focusing in the beginning part of the lectures, firstly, on what we called supervised learning, right? Supervised learning is in this domain where we're given data in the form of X's, our inputs, and our labels Y, right? And our goal here is to learn a function or a neural network that can learn to predict y given our inputs x. So for example, if you consider this example of an apple, observing a bunch of images of apples we want to detect in the future if we see a new image of an apple to detect that this is indeed an apple. Now the second class of learning approaches that we've discovered yesterday in yesterday's lecture was that of unsupervised learning. In these algorithms you have only access to the data. There's no notion of labels. This is what we learned about yesterday. In these types of algorithms you're not trying to predict a label, but you're trying to uncover some of the underlying structure, what we were calling basically these latent variables, these hidden features in your data. So for example in this Apple example, right, using unsupervised learning, the analogous example would basically be to build a model that could understand and cluster certain parts of these images together. And maybe it doesn't have to understand that necessarily this is an image of an apple, but it needs to understand that you know this image of the red apple is similar. It has the same latent features and same semantic meaning as this black and white outline sketch of the apple. Now in today's lecture we're going to talk about yet another type of learning algorithms, right. In reinforcement learning we're going to be only given data in the form of what are called state-action pairs. Now states are observations. This is what the agent, let's call it, the neural network is going to observe. It's what it sees. The actions are the behaviors that this agent takes in those particular states. So the goal of reinforcement learning is to build an agent that can learn how to maximize what are called rewards, right? This is the third component that is specific to reinforcement learning, and you want to maximize all of those rewards over many, many time steps in the future. So again, in this Apple example, we might now see that the agent doesn't necessarily learn that, okay, this is an apple or it looks like these other apples. Now it has to learn to, let's say, eat the apple, take an action, eat that apple, because it has learned that eating that apple makes it live longer or survive because it doesn't starve. So in today, right, like I said, we're going to be focusing exclusively on this third type of learning paradigm, which is reinforcement learning. And before we go any further, I just want to start by building up some very key terminology and like basically background for all of you, so that we're all on the same page when we start discussing some of the more complex components of today's lecture. So let's start by building up some of this terminology. The first main piece of terminology is that of an agent. An agent is a being, basically, that can take actions. For example, you can think of an agent as a machine that is, let's say, an autonomous drone that is making a delivery. Or, for example, in's say, an autonomous drone that is making a delivery, or for example, in a game. It could be Super Mario that's navigating inside of your video game. The algorithm itself, it's important to remember that the algorithm is the agent, right? We're trying to build an agent that can do these tasks, and the algorithm is that agent. So in life, for example, all of you are agents in life. The environment is the other kind of contrary approach, or the contrary perspective to the agent. The environment is simply the world where that agent lives and where it operates, right? Where it exists and it moves around in. The agent can send commands to that environment in the form of what are called actions. It can take actions to that environment in the form of what are called actions, right? It can take actions in that environment, and let's call for notation purposes, let's say the possible set of all actions that it could take is, let's say, a set of capital A, right? Now, it should be noted that agents at any point in time could choose amongst this, let's say, list of possible actions. But of course in some situations, your action space does not necessarily need to be a finite space, right? Maybe you could take actions in a continuous space. For example, when you're driving a car, you're taking actions on a continuous angle space of what angle you want to steer that car. It's not necessarily just going right or left or straight. you may steer at any continuous degree. Observations is essentially how the environment responds back to the agent, right? The environment can tell the agent, you know, what it should be seeing based on those actions that it just took, and it responds in the form of what is called a state. A state is simply a concrete and immediate situation that the agent finds itself in at that particular moment. Now it's important to remember that unlike other types of learning that we've covered in this course, reinforcement learning is a bit unique because it has one more component here in addition to these other components, which is called the reward. Now the reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment. So for example in a video game when Mario grabs a coin, for example, he wins points, right? So from a given state an agent can send out any form of actions to take some decisions and those actions may or may not result in rewards being collected and accumulated over time. Now it's also very important to remember that not all actions result in immediate rewards. You may take some actions that will result in a reward in a delayed fashion, maybe in a few time steps down the future, or maybe in life, maybe years. You may take an action today that results in a reward many, some time from now. But essentially, all of these try to effectively evaluate some way of measuring the success of a particular action that an agent takes. So for example, when we look at the total reward that an agent accumulates over the course of its lifetime, we can simply sum up all of the rewards that an agent gets after a certain time t, right? So this capital R of t is the sum of all rewards from that point on into the future, into infinity. And that can be expanded to look exactly like this. It's reward at time t plus the reward time t plus one plus t plus two and so on and so forth. Often it's actually very useful for all of us to consider not only the sum of all of these rewards but instead what's called the discounted sum. So you can see here I've added this gamma factor in front of all of the rewards and that discounting factor is essentially multiplied by every future reward that the agent sees and it's discovered by the agent. And the reason that we want to do this is actually this dampening factor is designed to make future rewards essentially worth less than rewards that we might see at this instant, right, at this moment right now. Now you can think of this as basically enforcing some kind of short-term greediness in the algorithm, right. So for example, if I offered you a reward of five dollars today or a reward of five dollars in 10 years from now, I think all of you would prefer that $5 today simply because we have that same discounting factor applied to this processing, right? We have that factor that that $5 is not worth as much to us if it's given to us 10 years in the future and that's exactly how this is captured here as well mathematically. This discounting factor is like multiply, like I said, multiplied at every single future award exponentially. And it's important to understand that also typically this discounting factor is, you know, between zero and one. There are some exceptional cases where maybe you want some strange behaviors and have a discounting factor greater than one, but in general that's not something we're going to be talking about today. Now finally, it's very important in reinforcement learning, this special function called the Q function, which ties in a lot of these different components that I've just shared with you all together. Now let's look at what this Q function is, right. So we already covered this R of T function, right. R of T is the discounted sum of rewards from time T all the way into the future, time infinity. But remember that this R of T, right, it's discounted, number one, and number two, we're going to try and build a Q function that captures the maximum or the best action that we could take that will maximize this reward. So let me say that one more time in a different way. The Q function takes as input two different things. The first is the state that you're currently in, and the second is a possible action that you could execute in this particular state. So here, S of T is that state at time t, a of t is that action that you may want to take at time t, and the Q function of these two pieces is going to denote or capture what the expected total return would be of that agent if it took that action in that particular state. Now, one thing that I think maybe we should all be asking ourselves now is, this seems like a really powerful function. If you had access to this type of a function, this Q function, I think you could actually perform a lot of tasks right off the bat. So if you wanted to, for example, understand what actions to take in a particular state, and let's suppose I gave you this magical Q function. Does anyone have any ideas of how you could transform that Q function to directly infer what action should be taken? Yep. Given a state, you can look at your possible action space and pick the one that gives you the highest Q value? Exactly. So that's exactly right. So just to repeat that one more time, the Q function tells us for any possible action, right, what is the expected reward for that action to be taken. So if we wanted to take a specific action given in a specific state, ultimately we need to, you know, figure out which action is the best action. The way we do that from a Q function is simply to pick the action that will maximize our future reward. And we can simply try out, number one, if we have a discrete action space, we can simply try out all possible actions, compute their Q value for every single possible action based on the state that we currently find ourselves in, and then we pick the action that is going to result in the highest Q-value. If we have a continuous action space, maybe we do something a bit more intelligent, maybe following the gradients along this Q-value curve and maximizing it as part of an optimization procedure. But generally, in this lecture what I want to focus on is actually how we can obtain this Q function to start with, right? I kind of skipped a lot of steps in that last slide where I just said, let's suppose I give you this magical Q function, how can you determine what action to take? But in reality, we're not given that Q function. We have to learn that Q function using deep learning, and that's what today's lecture is going to be talking about primarily, is first of all, how can we construct and learn that Q function from data, and then of course the final step is use that Q function to you know take some actions in the real world. And broadly speaking there are two classes of reinforcement learning algorithms that we're going to briefly touch on as part of today's lecture. The first class is what's going to be called value learning and that's exactly this process that we've just talked about. Value learning tries to estimate our Q function, right, so to find that Q function, Q given our state and our action, and then use that Q function to, you know, optimize for the best action to take given a particular state that we find ourselves in. The second class of algorithms, which we'll touch on right at the end of today's lecture, is kind of a different framing of the same approach. But instead of first optimizing the Q function and finding the Q value and then using that Q function to optimize our actions, what if we just tried to directly optimize our policy, which is what action to take based on a particular state that we find ourselves in. If we do that, if we can obtain this function, right, then we can directly sample from that policy distribution to obtain the optimal action. We'll talk more details about that later in the lecture, but first let's cover this first class of approaches, which is Q-learning approaches, and we'll build up that intuition and that knowledge onto the second part of policy learning. So maybe let's start by just digging a bit deeper into the Q-function specifically, just to start to understand how we could estimate this in the beginning. So first let me introduce this game, maybe some of you recognize it. This is the game called Atari Breakout. The game here is essentially one where the agent is able to move left or right, this paddle on the bottom, left or right, and the objective is to move it in a way that this ball that's coming down towards the bottom of the screen can be, you know, bounced off of your paddle, reflected back up, and essentially you want to break out, right, reflect that ball back up to the top of the screen towards the rainbow portion and keep breaking off. Every time you hit a pixel on the top of the screen you break off that pixel. The objective of the game is to basically eliminate all of those rainbow pixels. So you want to keep hitting that ball against the top of the screen until you remove all the pixels. Now the Q function tells us, you know, the expected total return or the total reward that we can expect based on a given state and action pair that we may find ourselves in this game. Now, the first point I want to make here is that sometimes, even for us as humans, to understand what the Q value should be is sometimes quite unintuitive. So here's one example. Let's say we find these two state-action pairs. Here is A and B, two different options that we can be presented with in this game. A, the ball is coming straight down towards us, that's our state. Our action is to do nothing and simply reflect that ball back up, vertically up. The second situation, the state is basically that the ball is coming slightly at an angle, we're not quite underneath it yet, and we need to move towards it and actually hit that ball in a way that, you know, will make it and not miss it hopefully, right, so hopefully that ball doesn't pass below us then the game would be over. Can you imagine, you know, which of these two options might have a higher Q value for the network? Which one would result in a greater reward for the neural network or for the agent? So how many people believe A would result in a higher return? Okay. How about B? Okay. How about someone who picked B? Can you tell me why B? It requires agency. You're actually doing something. Okay. Yeah. How about more? For A, you only have like the maximum you can take off is like one because after you reflect, your automatic comes back down, but then B, you can bounce around and there's more than one at least. Exactly, and actually there's a very interesting thing. So when I first saw this, actually it's, it was very unintuitive for me why A is actually working much worse than B, but in general with this very conservative action of B, you're kind of exactly like you said the two answers were implying, is that A is a very conservative action. You're kind of only going up and down. It will achieve a good reward. It will solve the game, right? In fact, it solves the game exactly like this right here. You can see in general this action is going to be quite conservative. It's just bouncing up, hitting one point at a time from the top and breaking off very slowly the board that you can see here. But in general you see the part of the board that it's being broken off is towards the center of the board, right, not much on the edges of the board. If you look at B now, with B you're kind of having agency, like one of the answers said. You're coming towards the ball, and what that implies is that you're sometimes going to actually hit the corner of your paddle and have a very extreme angle on your paddle and hit the sides of the board as well. And it turns out that the algorithm, the agent, can actually learn that hitting the side of the board can have some kind of unexpected consequences that look like this. So here you see it trying to enact that policy. It's targeting the sides of the board, but once it reaches a breakout on the side of the board, it found this hack in the solution where now it's breaking off a ton of points. So that was a kind of a trick that this neural network learned, which was a way that it even moves away from the ball as it's coming down, just so it could move back towards it, just to hit it on the corner and execute on those those corner parts of the board and break out a lot of pieces for free almost. So now that we can see that sometimes obtaining the Q function can be a little bit unintuitive, but the key point here is that if we have the Q function we can directly use it to determine you know what is the best action that we can take in any given state that we find ourselves in. So now the question naturally is, how can we train a neural network that can indeed learn this Q function? So the type of the neural network here, naturally, because we have a function that takes as input two things, let's imagine our neural network will also take as input these two objects as well. One object is going to be the state of the board. You can think of this as simply the pixels that are on the screen describing that board. So it's an image of the board at a particular time. Maybe you want to even provide two or three images to give it some sense of temporal information and some past history as well, but all of that information can be combined together and provided to the network in the form of a state. And in addition to that, you may also want to provide it some actions as well, right? So in this case, the actions that a neural network or an agent could take in this game is to move to the right, to the left, to stay still, right? And those could be three different actions that could be provided and parameterized to the input of a neural network. The goal here is to, you know, estimate the single number output that measures what is the expected value or the expected Q value of this neural network at this particular state-action pair. Now, oftentimes what you'll see is that if you wanted to evaluate, let's suppose, a very large action space, it's going to be very inefficient to try the approach on the left with a very large action space because what it would mean is that you'd have to run your neural network forward many different times, one time for every single element of your action space. So what if instead you only provided an input of your state, and as output you gave it, let's say, all n different Q values, one Q value for every single possible action. That way you only need to run your neural network once for the given state that you're in, and then that neural network will tell you for all possible actions what's the maximum. You simply then look at that output and pick the action that has the chi-sq value. Now, what would happen, right, so actually the question I want to pose here is really, you know, we want to train one of these two networks. Let's stick with the network on the right for simplicity, just since it's a much more efficient version of the network on the left. And the question is, you know, how do we actually train that network on the right? And specifically, I want all of you to think about really the best case scenario just to start with, how an agent would perform ideally in a particular situation, or what would happen, right, if an agent took all of the ideal actions at any given state. This would mean that essentially the target return, the value that we're trying to predict, the target, is going to always be maximized. And this can serve as essentially the ground truth to the agent. Now, for example, to do this, we want to formulate a loss function that's going to essentially represent our expected return if we're able to take all of the best actions. So for example, if we select an initial reward plus selecting some action in our action space that maximizes our expected return, then for the next future state we need to apply that discounting factor and recursively apply the same equation. And that simply turns into our target, right? Now we can ask basically what does our neural network predict, right? So that's our target and we recall from previous lectures if we have a target value, in this case our Q value is a continuous variable, we have also a predicted variable that is going to come as part of the output of every single one of these potential actions that could be taken. We can define what's called a Q loss, which is essentially just a very simple mean squared error loss between these two continuous variables. We minimize their distance over many, many different iterations of applying our neural network in this environment, observing actions, and observing not only the actions, but most importantly, after the action is committed or executed, we can see exactly the ground truth expected return, right? So we have the ground truth labels to train and supervise this model directly from the actions that were executed as part of random selection, for example. Now let me just stop right there and maybe summarize the whole process one more time in maybe a bit different terminology, just to give everyone kind of a different perspective on this same problem. So our deep neural network that we're trying to train looks like this, right? It takes as input a state, it's trying to output n different numbers. Those n different numbers correspond to the Q value associated to n different actions. One Q value per action. Here, the actions in Atari, Breakout, for example, should be three actions. We can either go left, we can go right, or we can do nothing. We can stay where we are. Right, so the next step from this we saw, if we have this Q value output, what we can do with it is we can make an action, or we can even, let me be more formal about it, we can develop what's called a policy function. A policy function is a function that, given a state, it determines what is the best action. So that's different than the Q function, right? The Q function tells us, given a state, what is the best or what is the value, the return of every action that we could take. The policy function tells us one step more than that. Given a state, what is the best action? So it's a very end-to-end way of thinking about the agent's decision-making process. Based on what I see right now, what is the action that I should take? And we can determine that policy function directly from the Q function itself simply by maximizing and optimizing all of the different Q values for all of the different actions that we see here. So for example, here we can see that given this state, the Q function has the result of these three different values, has a Q value of 20 if it goes to the left, has a Q value of 3 if it stays in the same place, and it has a Q value of 0, it's going to basically die after this iteration if it moves to the right, because you can see that the ball is coming to the left of it, if it moves to the right the game is over, right? So it needs to move to the left in order to do that, in order to continue the game and the Q value reflects that. The optimal action here is simply going to be the maximum of these three Q values, in this case it's going to be 20 and then the action is going to be the corresponding action that comes from that 20 which is moving left. Now we can send this action back to the environment in the form of the game to execute the next step, right, and as the agent moves through this environment it's going to be responded with not only by new pixels that come from the game, but more importantly, some reward signal. Now it's very important to remember that the reward signals in Pong, or sorry, in Atari Breakout, are very sparse, right? You get a reward not necessarily based on the action that you take at this exact moment. It usually takes a few time steps for that ball to travel back up to the top of the screen. So usually your rewards will be quite delayed, maybe at least by several time steps, sometimes even more if you're bouncing off of the corners of the screen. Now, one very popular or very famous approach that showed this was presented by Google DeepM, Google DeepMind, several years ago, where they showed that you could train a Q-value network, and you can see the input on the left-hand side is simply the raw pixels coming from the screen, all the way to the actions of a controller on the right-hand side. And you could train this one network for a variety of different tasks all across the Atari breakout ecosystem of games. And for each of these tasks, the really fascinating thing that they showed was for this very simple algorithm, which really relies on random choice of selection of actions and then, you know, learning from, you know, actions that don't do very well that you discourage them and trying to do actions that did perform well more frequently. Very simple algorithm but what they found was even with that type of algorithm they were able to surpass human level performance on over half of the game. There were some games that you can see here were still below human level performance but as we'll see this was really like a such an exciting advance because of the simplicity of the algorithm and how clean the formulation of the training was. You only needed a very little amount of prior knowledge to impose onto this neural network for it to be able to learn how to play these games. You never had to teach it any of the rules of the game, right? You only had to let it explore its environment, play the game many, many times against itself, and learn directly from that data. Now, there are several very important downsides of Q-learning and hopefully these are going to motivate the second part of today's lecture, which we'll talk about. But the first one that I want to really convey to everyone here is that Q-learning is naturally applicable to discrete action spaces, right, because you can think of this output space that we're providing as kind of like one number per action that could be taken. Now, if we have a continuous action space, we have to think about clever ways to work around that. In fact, there are now, more recently, there are some solutions to achieve Q-learning in continuous action spaces, but for the most part, Q-learning is very well suited for discrete action spaces, and we'll talk about ways of overcoming that with other approaches a bit later. And the second component here is that the policy that we're learning, right, the Q function is giving rise to that policy, which is the thing that we're actually using to determine what action to take given any state. That policy is determined by, you know, deterministically optimizing that Q function. We simply look at the results from the Q function and apply our, or we look at the results of the Q function and we pick the action that has the best or the highest Q value. That is very dangerous in many cases because of the fact that it's always going to pick the best value for a given state. There's no stochasticity in that pipeline so you can very frequently get caught in situations where you keep repeating the same actions and you don't learn to explore potentially different options that you may be thinking of. So to address these very important challenges, that's hopefully going to motivate now the next part of today's lecture, which is going to be focused on policy learning, which is a different class of reinforcement learning algorithms that are different than Q-learning algorithms. And like I said, those are called policy gradient algorithms. In policy gradient algorithms, the main difference is that instead of trying to infer the policy from the Q function, we're just going to build a neural network that will directly learn that policy function from the data, right? So it kind of skips one step and we'll see how we can train those networks. So before we get there, let me just revisit one more time the Q function illustration that we are looking at. Q function, we are trying to build a neural network, outputs these Q values, one value per action, and we determine the policy by looking over this state of Q values, picking the value that has the highest, and looking at its corresponding action. Now with policy networks, the idea that has the highest and looking at its corresponding action. Now, with policy networks, the idea that we want to keep here is that instead of predicting the Q values themselves, let's directly try to optimize this policy function. Here we're calling the policy function pi of s, right? So pi is the policy, s is our state, so it's a function that takes as input only the state and it's going to directly output the action. So the outputs here give us the desired action that we should take in any given state that we find ourselves in. And that represents not only the best action that we should take, but let's denote this as basically the probability that selecting that action would result in a very desirable outcome for our network. So not necessarily the value of that action, but rather the probability that selecting that action would be the highest value. So you don't care exactly about what is the numerical value that selecting this action takes, or gives rise to rather, but rather what is the numerical value that selecting this action takes, or gives rise to rather, but rather what is the likelihood that selecting this action will give you the best performing value that you could expect. Exact value itself doesn't matter. You only care about if selecting this action is going to give you with high likelihood the best one. So we can see that if these predicted probabilities here, in this example of Atari, going left has the probability of being the highest value action with 90% staying in the center. That's a probability of 10%. Going right is 0%. So ideally what our neural network should do in this case is 90% of the time in this situation go to the left, 10% of the time it could still try staying at where it is but never it should go to the right. Now note that this now is a probability distribution. This is very different than a Q function. A Q function has actually no structure, right? The Q values themselves can take any real number, right? But here, the policy network has a very formulated output. All of the numbers here in the output have to sum to one because this is a probability distribution, right? And that gives it a very rigorous version of how we can train this model that makes it a bit easier to train than Q functions as well. So one other very important advantage of having an output that is a probability distribution is actually going to tie back to this other issue of Q functions and Q neural networks that we saw before, and that is the fact that Q-functions are naturally suited towards discrete action spaces. Now when we're looking at this policy network, we're outputting a distribution, right, and remember those distributions can also take continuous forms. In fact, we've seen this in the last two lectures, right. In the generative lecture, we saw how VAEs could be used to predict Gaussian distributions over their latent space. In the last lecture, we also saw how we could learn to predict uncertainties, which are continuous probability distributions using data. And just like that, we could also use this same formulation to move beyond discrete action spaces like you can see here, which are one possible action, a probability associated to one possible action in a discrete set of possible actions. Now we may have a space which is not what action should I take, go left, right, or stay in the center, but rather how quickly should I move and in what direction should I move, right? That is a continuous variable as opposed to a discrete variable, and you could say that now the answer should look like this, right? Moving very fast to the right versus very slow to the, or excuse me, very fast to the left versus very slow to the left has this continuous spectrum that we may want to model. Now, when we plot this entire distribution of taking an action, giving a state, you can see basically a very simple illustration of that right here. This distribution has most of its mass over- or sorry, it has all of its mass over the entire real number line, first of all. It has most of its mass right in the optimal action space that we want to take. So if we want to determine the best action to take, we would simply take the mode of this distribution, right, the highest point. That would be the speed at which we should move and the direction that we should move in. If we wanted to also, you know, try out different things and explore our space, we could sample from this distribution and still obtain some stochasticity. Now let's look at an example of how we can actually model these continuous distributions, and actually we've already seen some examples of this in the previous two lectures like I mentioned, but let's take a look specifically in the context of reinforcement learning and policy gradient learning. So instead of predicting this probability of taking an action giving all possible states, which in this case there is now an infinite number of because we're in the continuous domain, we can't simply predict a single probability for every possible action because there is an infinite number of them. So instead, what if we parameterized our action space by a distribution, right? So let's take for example the Gaussian distribution. To parameterize a Gaussian distribution, we only need two outputs, right? We need a mean and a variance. Given a mean and a variance, we can actually have a probability mass and we can compute a probability over any possible action that we may want to take just from those two numbers. So for example, in this image here, we may want to output a Gaussian that looks like this, right? Its mean is centered at, let's see, negative 0.8, indicating that we should move basically left with a speed of 0.8 meters per second, for example. And again, we can see that because this is a probability distribution, because of the format of policy networks, we're enforcing that this is a probability distribution. That means that the integral now of this of these outputs, by definition of it being a Gaussian, must also integrate to one. Okay, great. So now let's maybe take a look at how policy gradient networks can be trained and, you know, step through that process as well as we look at a very concrete example. And maybe let's start by just revisiting this reinforcement learning loop that we started this class with. Now, let's start by just revisiting this reinforcement learning loop that we started this class with. Now, let's specifically consider the example of training an autonomous vehicle, since I think that this is a particularly very intuitive example that we can walk through. The agent here is the vehicle. The state could be obtained through many sensors that could be mounted on the vehicle itself. So for example, autonomous vehicles are typically equipped with sensors like cameras, lidars, radars, etc. All of these are giving observational inputs to the vehicle. The action that we could take is a steering wheel angle. This is not a discrete variable, this is a continuous variable. It's actually an angle that could take any real number. And finally, the reward in this very simplistic example is the distance that we travel before we crash. OK, so now let's take a look at how we could train a policy gradient neural network to solve this task of self-driving cars as a concrete example. So we start by initializing our agent. Remember that we have no training data. So we have to think about actually reinforcement learning as almost like a data acquisition plus learning pipeline combined together. So the first part of that data acquisition pipeline is first to initialize our agent, to go out and collect some data. So we start our vehicle, our agent to go out and collect some data. So we start our vehicle, our agent, and in the beginning of course it knows nothing about driving. It's never been exposed to any of these rules of the environment or the observation before, so it runs its policy, which right now is untrained entirely, until it terminates, right, until it goes outside of some bounds that we define. We measure basically the reward as the distance that it traveled before it terminated, and we record all of the states, all of the actions, and the final reward that it obtained until that termination, right? This becomes our mini data set that we'll use for the first round of training. Let's take those data sets and now we'll do one step of training. The first step of training that we'll do is to take, excuse me, to take the later half of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards. Now, because the vehicle, we know the vehicle terminated, we can assume that all of the actions that occurred in the later half of this trajectory were probably not very good actions because they came very close to termination. Right, so let's decrease the probability of all of those things happening again in the future. And we'll take all of the things that happened in the beginning half of our training episode and we will increase their probabilities. Now again, there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory and a bad action in the later half, but it's simply because actions that are in the later half were closer to a failure and closer determination that we can assume, for example, that these were probably suboptimal actions. But it's very possible that these are noisy rewards as well because it's such a sparse signal. It's very possible that you had some good actions at the end and you were actually trying to recover your car but you were just too late. Now, repeat this process again. Re-initialize the agent one more time and run it until completion. Now the agent goes a bit farther, right, because you've decreased the probabilities at the ends, increased the probabilities at the future, and you keep repeating this over and over again until you notice that the agent learns to perform better and better every time until it finally converges and at end, the agent is able to basically follow lanes, usually swerving a bit side to side while it does that, without crashing. And this is actually really fascinating because this is a self-driving car that we never taught anything about what a lane marker means or what are the rules of the road, anything about that, right? This was a car that learned entirely just by going out, crashing a lot, and you know, trying to figure out what to do to not keep doing that in the future, right? And the remaining question is actually how we can update, you know, that policy as part of this algorithm that I'm showing you on the right and the left hand side left-hand side, how can we basically formulate that same algorithm, and specifically the update equation, steps four and five, right here. These are the two really important steps of how we can use those two steps to train our policy and decrease the probability of bad events while promoting these likelihoods of all these good events. So let's look at the loss function, first of all. The loss function for a policy gradient neural network looks like this, and then we'll start by dissecting it to understand why this works the way it does. So here we can see that the loss consists of two terms. The first term is this term in green, which is called the log likelihood of two terms. The first term is this term in green, which is called the log likelihood of selecting a particular action. The second term is something that all of you are very familiar with already. This is simply the return at a specific time, right? So that's the expected return on rewards that you would get after this time point. Now let's assume that we got a lot of reward for a particular action that had a high log probability or a high probability. If we got a lot of reward for a particular action that had high probability, that means that we want to increase that probability even further. So we do it even more, or even more likelihood, we sample that action again into the future. On the other hand, if we selected, or let's say if we obtained a reward that was very low for an action that had high likelihood, we want the inverse effect. We never want to sample that action again in the future because it resulted in a low reward. And you'll notice that this loss function right here, by including this negative, we're going to minimize the likelihood of achieving any action that had low rewards in this trajectory. Now in our simplified example on the car example, all the things that had low rewards were exactly those actions that came closest to the termination part of the vehicle. All the things that had high rewards were the things that came in the beginning. That's just the assumption that we make when defining our reward structure. Now we can plug this into the loss of gradient descent algorithm to train our neural network when we see this policy gradient algorithm, which you can see highlighted here. This gradient is exactly of the policy part of the neural network. That's the probability of selecting an action given a specific state. If you remember before when we defined what does it mean to be a policy function, that's exactly what it means. Given a particular state that you find yourself in, what is the probability of selecting a particular action with the highest likelihood? And that's, you know, exactly where this method gets its name from this policy gradient piece here that you can see here. Now I want to take maybe just a very brief second towards the end of the class here just to talk about, you know, some of the challenges and keeping in line with the first lecture today, some of the challenges of deploying these types of algorithms in the context of the real world. What do you think, when you look at this training algorithm that you can see here, what do you think are the shortcomings of this training algorithm and which step, I guess, specifically, if we wanted to deploy this approach into reality? Yeah, exactly. So it's step two. If you wanted to do this in reality, that essentially means that you want to go out, collect your car, crashing it a bunch of times just to learn how to not crash it, right? And that's, you know, that's simply not feasible, right? Number one. It's also, you know, very dangerous, number two. So, there are ways around this, right? The number one way around this is that people try to train these types of models in simulation. Simulation is very safe because we're not going to actually be damaging anything real. It's still very inefficient because we have to run these algorithms a bunch of times, crash them a bunch of times just to learn how not to crash, but at least now, at least from a safety point of view, it's much safer. But you know the problem is that modern simulation engines for reinforcement learning and generally, very broadly speaking, modern simulators for vision specifically do not at all capture reality very accurately. In fact, there's a very famous notion called the sim-to-real gap, which is a gap that exists when you train algorithms in simulation and they don't extend to a lot of the phenomena that we see and the patterns that we see in reality. And one really cool result that I want to just highlight here is that when we're training reinforcement learning algorithms, we ultimately want them to be, you know, not operating in simulation. We want them to be in reality. And as part of our lab here at MIT, we've been developing this very, very cool, brand new photorealistic simulation engine that goes beyond basically the paradigm of how simulators work today, which is basically defining a model of their environment and trying to synthesize that model. Essentially, these simulators are like glorified game engines, right? They all look very game- like when you look at them, but one thing that we've done is taken a data-driven approach. Using real data of the real world, can we build up synthetic environments that are super photorealistic and look like this, right? So this is a cool result that we created here at MIT, developing this photorealistic simulation engine. This is actually an autonomous agent, not a real car, driving through our virtual simulator in a bunch of different types of different scenarios. So this simulator is called Vista. It allows us to basically use real data that we do collect in the real world, but then re-simulate those same real roads. So for example, let's say you take your car, you drive out on Mass Ave, you collect data of Mass Ave, you can now drop a virtual agent into that same simulated environment, observing new viewpoints of what that scene might have looked like from different types of perturbations or types of angles that it might be exposed to. And that allows us to train these agents now entirely using reinforcement learning, no human labels, but importantly, allow them to be transferred into reality because there's no sim-to-real gap anymore. So in fact, we did exactly this. We placed agents into our simulator, we trained them using the exact algorithms that you learned about in today's lecture, these policy gradient algorithms, and all of the training was done entirely in simulation. Then we took these policies and we deployed them on board our full-scale autonomous vehicle. This is now in the real world, no longer in simulation, and on the left-hand side you can see basically this car driving through this environment, completely autonomous in the real world. No transfer learning is done here. There is no augmentation of data from real-world data. This is entirely trained using simulation, and this represented actually the first time ever that reinforcement learning was used to train a policy end-to-end for an autonomous vehicle that could be deployed in reality. So that was something really cool that we created here at MIT. But now that we covered all of these foundations of reinforcement learning and policy learning, I want to touch on some other maybe very exciting applications that we're seeing. And one very popular application that a lot of people will tell you about and talk about is the game of Go. So here, reinforcement learning agents could be actually tried to put against the test, against grand master level Go players, and at the time achieved incredibly impressive results. So for those of you who are not familiar with the game of Go, the game of Go is played on a 19 by 19 board. The rough objective of Go is to claim basically more board pieces than your opponent, right, and through the grid of sorry, through the grid that you can see here, this 19 by 19 grid, and while the game itself, the logical rules, are actually quite simple, the number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the universe, right? So this game, even though the rules are very simple in their logical definitions, is an extraordinarily complex game for an artificial algorithm to try and master. So the objective here was to build a reinforcement learning algorithm to master the game of Go, not only beating these gold standard softwares, but also what was at the time like an amazing result was to beat the grand master level players. So the number one player in the world of Go was a human champion, obviously. So Google DeepMind rose to this challenge. They created a couple years ago, developing this solution, which is very much based in the exact same algorithms that you learned about in today's lecture, combining both the value part of this network with residual layers, which we'll cover in the next lecture tomorrow. And using reinforcement learning pipeline, they were able to defeat the grand champion human players. And the idea at its core was actually very simple. The first step is that you train a neural network to basically watch human level experts. So this is not using reinforcement learning, this is using supervised learning, using the techniques that we covered in lectures one, two, and three. And from this first step, the goal is to build like a policy that would imitate some of the rough patterns that a human type of player or a human grandmaster would take based on a given board state, the type of actions that they might execute. But then given this pre-trained model essentially, you could use it to bootstrap a reinforcement learning algorithm that would play against itself in order to learn how to improve even beyond the human levels. So it would take its human understandings, try to imitate the humans first of all, but then from that imitation, they would pin these two neural networks against themselves, play a game against themselves, and the winners would be receiving a reward. The losers would try to negate all of the actions that they may have acquired from their human counterparts counterparts and try to actually learn new types of rules and new types of actions basically that might be very beneficial to achieving superhuman performance. And one of the very important auxiliary tricks that brought this idea to be possible was the usage of this second network, this auxiliary network which took as input the state of the board and tried to predict, you know, what are all of the different possible board states that might emerge from this particular state and what would their values be, what would their potential returns and their outcomes be. So this network was an auxiliary network that was almost hallucinating, right? Different board states that it could take from this particular state and using those predicted values to guide its planning of, you know, what action should it take into the future. And finally, very much more recently, they extended this algorithm and showed that they could not even use the human grandmasters in the beginning to imitate from in the beginning and bootstrap these algorithms? What if they just started entirely from scratch and just had two neural networks, never trained before, they start pinning themselves against each other and you could actually see that you could, without any human supervision at all, have a neural network learn to not only outperform the solution that, or outperform the humans, but also outperform the solution that was created, which was bootstrapped by humans as well. So with that, I'll summarize very quickly what we've learned today and conclude for the day. So we've talked a lot about, really, the foundational algorithms underlying reinforcement learning. We saw two different types of reinforcement learning approaches of how we can optimize these solutions. First being Q-learning, where we're trying to actually estimate, given a state, what is the value that we might expect for any possible action. And the second way was to take a much more end-to-end approach and say, given a state that we see ourselves in, what is the likelihood that I should take any given action to maximize the potential that I have in this particular state. And I hope that all of this was very exciting to you. Today we have a very exciting lab and kickoff for the competition and the deadline for these competitions will be, well, it was originally set to be Thursday, which is tomorrow at 11 p.m. Thank you.", 3453, "2023-04-14 00:00:00"]], "ncols": 6, "nrows": 6}